{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Xiao-Yong\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from functools import reduce\n",
    "\n",
    "class Param:\n",
    "    def __init__(self, beta = 6.0, lat = [64, 64], tau = 2.0, nstep = 50, ntraj = 256, nrun = 4, nprint = 256, seed = 11*13, randinit = False, nth = int(os.environ.get('OMP_NUM_THREADS', '2')), nth_interop = 2):\n",
    "        self.beta = beta\n",
    "        self.lat = lat\n",
    "        self.nd = len(lat)\n",
    "        self.volume = reduce(lambda x,y:x*y, lat)\n",
    "        self.tau = tau\n",
    "        self.nstep = nstep\n",
    "        self.dt = self.tau / self.nstep\n",
    "        self.ntraj = ntraj\n",
    "        self.nrun = nrun\n",
    "        self.nprint = nprint\n",
    "        self.seed = seed\n",
    "        self.randinit = randinit\n",
    "        self.nth = nth\n",
    "        self.nth_interop = nth_interop\n",
    "    def initializer(self):\n",
    "        if self.randinit:\n",
    "            return torch.empty((param.nd,) + param.lat).uniform_(-math.pi, math.pi)\n",
    "        else:\n",
    "            return torch.zeros((param.nd,) + param.lat)\n",
    "    def summary(self):\n",
    "        return f\"\"\"latsize = {self.lat}\n",
    "volume = {self.volume}\n",
    "beta = {self.beta}\n",
    "trajs = {self.ntraj}\n",
    "tau = {self.tau}\n",
    "steps = {self.nstep}\n",
    "seed = {self.seed}\n",
    "nth = {self.nth}\n",
    "nth_interop = {self.nth_interop}\n",
    "\"\"\"\n",
    "    def uniquestr(self):\n",
    "        lat = \".\".join(str(x) for x in self.lat)\n",
    "        return f\"out_l{lat}_b{param.beta}_n{param.ntraj}_t{param.tau}_s{param.nstep}.out\"\n",
    "\n",
    "def action(param, f):\n",
    "    return (-param.beta)*torch.sum(torch.cos(plaqphase(f)))\n",
    "\n",
    "def force(param, f):\n",
    "    f.requires_grad_(True)\n",
    "    s = action(param, f)\n",
    "    s.backward()\n",
    "    ff = f.grad\n",
    "    f.requires_grad_(False)\n",
    "    return ff\n",
    "\n",
    "plaqphase = lambda f: f[0,:] - f[1,:] - torch.roll(f[0,:], shifts=-1, dims=1) + torch.roll(f[1,:], shifts=-1, dims=0)\n",
    "topocharge = lambda f: torch.floor(0.1 + torch.sum(regularize(plaqphase(f))) / (2*math.pi))\n",
    "def regularize(f):\n",
    "    p2 = 2*math.pi\n",
    "    f_ = (f - math.pi) / p2\n",
    "    return p2*(f_ - torch.floor(f_) - 0.5)\n",
    "\n",
    "def leapfrog(param, x, p):\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = force(param, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    print(f'plaq(x) {action(param, x) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(f)}')\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        p_ = p_ + (-dt)*force(param, x_)\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    return (x_, p_)\n",
    "def hmc(param, x):\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = action(param, x) + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = leapfrog(param, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = action(param, xr) + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    newx = xr if acc else x\n",
    "    return (dH, exp_mdH, acc, newx)\n",
    "\n",
    "put = lambda s: sys.stdout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ BEGIN CODE FROM https://arxiv.org/abs/2101.08176\n",
    "# Introduction to Normalizing Flows for Lattice Field Theory\n",
    "# Michael S. Albergo, Denis Boyda, Daniel C. Hackett, Gurtej Kanwar, Kyle Cranmer, Sébastien Racanière, Danilo Jimenez Rezende, Phiala E. Shanahan\n",
    "# License: CC BY 4.0\n",
    "# With slight modifications by Xiao-Yong Jin to reduce global variables\n",
    "\n",
    "import numpy as np\n",
    "import packaging.version\n",
    "torch_device = 'cpu'\n",
    "float_dtype = np.float64\n",
    "def torch_mod(x):\n",
    "    return torch.remainder(x, 2*np.pi)\n",
    "def torch_wrap(x):\n",
    "    return torch_mod(x+np.pi) - np.pi\n",
    "def grab(var):\n",
    "    return var.detach().cpu().numpy()\n",
    "def compute_ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*torch.logsumexp(logw, dim=0) - torch.logsumexp(2*logw, dim=0)\n",
    "    ess_per_cfg = torch.exp(log_ess) / len(logw)\n",
    "    return ess_per_cfg\n",
    "def bootstrap(x, *, Nboot, binsize):\n",
    "    boots = []\n",
    "    x = x.reshape(-1, binsize, *x.shape[1:])\n",
    "    for i in range(Nboot):\n",
    "        boots.append(np.mean(x[np.random.randint(len(x), size=len(x))], axis=(0,1)))\n",
    "    return np.mean(boots), np.std(boots)\n",
    "def print_metrics(history, avg_last_N_epochs, era, epoch):\n",
    "    print(f'== Era {era} | Epoch {epoch} metrics ==')\n",
    "    for key, val in history.items():\n",
    "        avgd = np.mean(val[-avg_last_N_epochs:])\n",
    "        print(f'\\t{key} {avgd:g}')\n",
    "def serial_sample_generator(model, action, batch_size, N_samples):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    layers.eval()\n",
    "    x, logq, logp = None, None, None\n",
    "    for i in range(N_samples):\n",
    "        batch_i = i % batch_size\n",
    "        if batch_i == 0:\n",
    "            # we're out of samples to propose, generate a new batch\n",
    "            x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size)\n",
    "            logp = -action(x)\n",
    "        yield x[batch_i], logq[batch_i], logp[batch_i]\n",
    "def make_mcmc_ensemble(model, action, batch_size, N_samples):\n",
    "    history = {\n",
    "        'x' : [],\n",
    "        'logq' : [],\n",
    "        'logp' : [],\n",
    "        'accepted' : []\n",
    "    }\n",
    "\n",
    "    # build Markov chain\n",
    "    sample_gen = serial_sample_generator(model, action, batch_size, N_samples)\n",
    "    for new_x, new_logq, new_logp in sample_gen:\n",
    "        if len(history['logp']) == 0:\n",
    "            # always accept first proposal, Markov chain must start somewhere\n",
    "            accepted = True\n",
    "        else: \n",
    "            # Metropolis acceptance condition\n",
    "            last_logp = history['logp'][-1]\n",
    "            last_logq = history['logq'][-1]\n",
    "            p_accept = torch.exp((new_logp - new_logq) - (last_logp - last_logq))\n",
    "            p_accept = min(1, p_accept)\n",
    "            draw = torch.rand(1) # ~ [0,1]\n",
    "            if draw < p_accept:\n",
    "                accepted = True\n",
    "            else:\n",
    "                accepted = False\n",
    "                new_x = history['x'][-1]\n",
    "                new_logp = last_logp\n",
    "                new_logq = last_logq\n",
    "        # Update Markov chain\n",
    "        history['logp'].append(new_logp)\n",
    "        history['logq'].append(new_logq)\n",
    "        history['x'].append(new_x)\n",
    "        history['accepted'].append(accepted)\n",
    "    return history\n",
    "def make_conv_net(*, hidden_sizes, kernel_size, in_channels, out_channels, use_final_tanh):\n",
    "    sizes = [in_channels] + hidden_sizes + [out_channels]\n",
    "    assert packaging.version.parse(torch.__version__) >= packaging.version.parse('1.5.0')\n",
    "    assert kernel_size % 2 == 1, 'kernel size must be odd for PyTorch >= 1.5.0'\n",
    "    padding_size = (kernel_size // 2)\n",
    "    net = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        net.append(torch.nn.Conv2d(\n",
    "            sizes[i], sizes[i+1], kernel_size, padding=padding_size,\n",
    "            stride=1, padding_mode='circular'))\n",
    "        if i != len(sizes) - 2:\n",
    "            net.append(torch.nn.SiLU())\n",
    "        else:\n",
    "            if use_final_tanh:\n",
    "                net.append(torch.nn.Tanh())\n",
    "    return torch.nn.Sequential(*net)\n",
    "def set_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight is not None:\n",
    "        torch.nn.init.normal_(m.weight, mean=1, std=2)\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        m.bias.data.fill_(-1)\n",
    "def calc_dkl(logp, logq):\n",
    "    return (logq - logp).mean()  # reverse KL, assuming samples from q\n",
    "def apply_flow_to_prior(prior, coupling_layers, *, batch_size):\n",
    "    x = prior.sample_n(batch_size)\n",
    "    logq = prior.log_prob(x)\n",
    "    for layer in coupling_layers:\n",
    "        x, logJ = layer.forward(x)\n",
    "        logq = logq - logJ\n",
    "    return x, logq\n",
    "def train_step(model, action, optimizer, metrics, batch_size):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size)\n",
    "    logp = -action(x)\n",
    "    loss = calc_dkl(logp, logq)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    metrics['loss'].append(grab(loss))\n",
    "    metrics['logp'].append(grab(logp))\n",
    "    metrics['logq'].append(grab(logq))\n",
    "    metrics['ess'].append(grab( compute_ess(logp, logq) ))\n",
    "def compute_u1_plaq(links, mu, nu):\n",
    "    \"\"\"Compute U(1) plaqs in the (mu,nu) plane given `links` = arg(U)\"\"\"\n",
    "    return (links[:,mu] + torch.roll(links[:,nu], -1, mu+1)\n",
    "            - torch.roll(links[:,mu], -1, nu+1) - links[:,nu])\n",
    "class U1GaugeAction:\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "    def __call__(self, cfgs):\n",
    "        Nd = cfgs.shape[1]\n",
    "        action_density = 0\n",
    "        for mu in range(Nd):\n",
    "            for nu in range(mu+1,Nd):\n",
    "                action_density = action_density + torch.cos(\n",
    "                    compute_u1_plaq(cfgs, mu, nu))\n",
    "        return -self.beta * torch.sum(action_density, dim=tuple(range(1,Nd+1)))\n",
    "def gauge_transform(links, alpha):\n",
    "    for mu in range(len(links.shape[2:])):\n",
    "        links[:,mu] = alpha + links[:,mu] - torch.roll(alpha, -1, mu+1)\n",
    "    return links\n",
    "def random_gauge_transform(x):\n",
    "    Nconf, VolShape = x.shape[0], x.shape[2:]\n",
    "    return gauge_transform(x, 2*np.pi*torch.rand((Nconf,) + VolShape))\n",
    "def topo_charge(x):\n",
    "    P01 = torch_wrap(compute_u1_plaq(x, mu=0, nu=1))\n",
    "    axes = tuple(range(1, len(P01.shape)))\n",
    "    return torch.sum(P01, dim=axes) / (2*np.pi)\n",
    "class MultivariateUniform(torch.nn.Module):\n",
    "    \"\"\"Uniformly draw samples from [a,b]\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__()\n",
    "        self.dist = torch.distributions.uniform.Uniform(a, b)\n",
    "    def log_prob(self, x):\n",
    "        axes = range(1, len(x.shape))\n",
    "        return torch.sum(self.dist.log_prob(x), dim=tuple(axes))\n",
    "    def sample_n(self, batch_size):\n",
    "        return self.dist.sample((batch_size,))\n",
    "class GaugeEquivCouplingLayer(torch.nn.Module):\n",
    "    \"\"\"U(1) gauge equiv coupling layer defined by `plaq_coupling` acting on plaquettes.\"\"\"\n",
    "    def __init__(self, *, lattice_shape, mask_mu, mask_off, plaq_coupling):\n",
    "        super().__init__()\n",
    "        link_mask_shape = (len(lattice_shape),) + lattice_shape\n",
    "        self.active_mask = make_2d_link_active_stripes(link_mask_shape, mask_mu, mask_off)\n",
    "        self.plaq_coupling = plaq_coupling\n",
    "\n",
    "    def forward(self, x):\n",
    "        plaq = compute_u1_plaq(x, mu=0, nu=1)\n",
    "        new_plaq, logJ = self.plaq_coupling(plaq)\n",
    "        delta_plaq = new_plaq - plaq\n",
    "        delta_links = torch.stack((delta_plaq, -delta_plaq), dim=1) # signs for U vs Udagger\n",
    "        fx = self.active_mask * torch_mod(delta_links + x) + (1-self.active_mask) * x\n",
    "        return fx, logJ\n",
    "\n",
    "    def reverse(self, fx):\n",
    "        new_plaq = compute_u1_plaq(fx, mu=0, nu=1)\n",
    "        plaq, logJ = self.plaq_coupling.reverse(new_plaq)\n",
    "        delta_plaq = plaq - new_plaq\n",
    "        delta_links = torch.stack((delta_plaq, -delta_plaq), dim=1) # signs for U vs Udagger\n",
    "        x = self.active_mask * torch_mod(delta_links + fx) + (1-self.active_mask) * fx\n",
    "        return x, logJ\n",
    "def make_2d_link_active_stripes(shape, mu, off):\n",
    "    \"\"\"\n",
    "    Stripes mask looks like in the `mu` channel (mu-oriented links)::\n",
    "\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "\n",
    "    where vertical is the `mu` direction, and the pattern is offset in the nu\n",
    "    direction by `off` (mod 4). The other channel is identically 0.\n",
    "    \"\"\"\n",
    "    assert len(shape) == 2+1, 'need to pass shape suitable for 2D gauge theory'\n",
    "    assert shape[0] == len(shape[1:]), 'first dim of shape must be Nd'\n",
    "    assert mu in (0,1), 'mu must be 0 or 1'\n",
    "\n",
    "    mask = np.zeros(shape).astype(np.uint8)\n",
    "    if mu == 0:\n",
    "        mask[mu,:,0::4] = 1\n",
    "    elif mu == 1:\n",
    "        mask[mu,0::4] = 1\n",
    "    nu = 1-mu\n",
    "    mask = np.roll(mask, off, axis=nu+1)\n",
    "    return torch.from_numpy(mask.astype(float_dtype)).to(torch_device)\n",
    "def make_single_stripes(shape, mu, off):\n",
    "    \"\"\"\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "\n",
    "    where vertical is the `mu` direction. Vector of 1 is repeated every 4.\n",
    "    The pattern is offset in perpendicular to the mu direction by `off` (mod 4).\n",
    "    \"\"\"\n",
    "    assert len(shape) == 2, 'need to pass 2D shape'\n",
    "    assert mu in (0,1), 'mu must be 0 or 1'\n",
    "\n",
    "    mask = np.zeros(shape).astype(np.uint8)\n",
    "    if mu == 0:\n",
    "        mask[:,0::4] = 1\n",
    "    elif mu == 1:\n",
    "        mask[0::4] = 1\n",
    "    mask = np.roll(mask, off, axis=1-mu)\n",
    "    return torch.from_numpy(mask).to(torch_device)\n",
    "def make_double_stripes(shape, mu, off):\n",
    "    \"\"\"\n",
    "    Double stripes mask looks like::\n",
    "\n",
    "      1 1 0 0 1 1 0 0\n",
    "      1 1 0 0 1 1 0 0\n",
    "      1 1 0 0 1 1 0 0\n",
    "      1 1 0 0 1 1 0 0\n",
    "\n",
    "    where vertical is the `mu` direction. The pattern is offset in perpendicular\n",
    "    to the mu direction by `off` (mod 4).\n",
    "    \"\"\"\n",
    "    assert len(shape) == 2, 'need to pass 2D shape'\n",
    "    assert mu in (0,1), 'mu must be 0 or 1'\n",
    "\n",
    "    mask = np.zeros(shape).astype(np.uint8)\n",
    "    if mu == 0:\n",
    "        mask[:,0::4] = 1\n",
    "        mask[:,1::4] = 1\n",
    "    elif mu == 1:\n",
    "        mask[0::4] = 1\n",
    "        mask[1::4] = 1\n",
    "    mask = np.roll(mask, off, axis=1-mu)\n",
    "    return torch.from_numpy(mask).to(torch_device)\n",
    "def make_plaq_masks(mask_shape, mask_mu, mask_off):\n",
    "    mask = {}\n",
    "    mask['frozen'] = make_double_stripes(mask_shape, mask_mu, mask_off+1)\n",
    "    mask['active'] = make_single_stripes(mask_shape, mask_mu, mask_off)\n",
    "    mask['passive'] = 1 - mask['frozen'] - mask['active']\n",
    "    return mask\n",
    "def tan_transform(x, s):\n",
    "    return torch_mod(2*torch.atan(torch.exp(s)*torch.tan(x/2)))\n",
    "\n",
    "def tan_transform_logJ(x, s):\n",
    "    return -torch.log(torch.exp(-s)*torch.cos(x/2)**2 + torch.exp(s)*torch.sin(x/2)**2)\n",
    "def mixture_tan_transform(x, s):\n",
    "    assert len(x.shape) == len(s.shape), \\\n",
    "        f'Dimension mismatch between x and s {x.shape} vs {s.shape}'\n",
    "    return torch.mean(tan_transform(x, s), dim=1, keepdim=True)\n",
    "\n",
    "def mixture_tan_transform_logJ(x, s):\n",
    "    assert len(x.shape) == len(s.shape), \\\n",
    "        f'Dimension mismatch between x and s {x.shape} vs {s.shape}'\n",
    "    return torch.logsumexp(tan_transform_logJ(x, s), dim=1) - np.log(s.shape[1])\n",
    "def invert_transform_bisect(y, *, f, tol, max_iter, a=0, b=2*np.pi):\n",
    "    min_x = a*torch.ones_like(y)\n",
    "    max_x = b*torch.ones_like(y)\n",
    "    min_val = f(min_x)\n",
    "    max_val = f(max_x)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_iter):\n",
    "            mid_x = (min_x + max_x) / 2\n",
    "            mid_val = f(mid_x)\n",
    "            greater_mask = (y > mid_val).int()\n",
    "            greater_mask = greater_mask.float()\n",
    "            err = torch.max(torch.abs(y - mid_val))\n",
    "            if err < tol: return mid_x\n",
    "            if torch.all((mid_x == min_x) + (mid_x == max_x)):\n",
    "                print('WARNING: Reached floating point precision before tolerance '\n",
    "                      f'(iter {i}, err {err})')\n",
    "                return mid_x\n",
    "            min_x = greater_mask*mid_x + (1-greater_mask)*min_x\n",
    "            min_val = greater_mask*mid_val + (1-greater_mask)*min_val\n",
    "            max_x = (1-greater_mask)*mid_x + greater_mask*max_x\n",
    "            max_val = (1-greater_mask)*mid_val + greater_mask*max_val\n",
    "        print(f'WARNING: Did not converge to tol {tol} in {max_iter} iters! Error was {err}')\n",
    "        return mid_x\n",
    "def stack_cos_sin(x):\n",
    "    return torch.stack((torch.cos(x), torch.sin(x)), dim=1)\n",
    "class NCPPlaqCouplingLayer(torch.nn.Module):\n",
    "    def __init__(self, net, *, mask_shape, mask_mu, mask_off,\n",
    "                 inv_prec=1e-6, inv_max_iter=1000):\n",
    "        super().__init__()\n",
    "        assert len(mask_shape) == 2, (\n",
    "            f'NCPPlaqCouplingLayer is implemented only in 2D, '\n",
    "            f'mask shape {mask_shape} is invalid')\n",
    "        self.mask = make_plaq_masks(mask_shape, mask_mu, mask_off)\n",
    "        self.net = net\n",
    "        self.inv_prec = inv_prec\n",
    "        self.inv_max_iter = inv_max_iter\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.mask['frozen'] * x\n",
    "        net_out = self.net(stack_cos_sin(x2))\n",
    "        assert net_out.shape[1] >= 2, 'CNN must output n_mix (s_i) + 1 (t) channels'\n",
    "        s, t = net_out[:,:-1], net_out[:,-1]\n",
    "\n",
    "        x1 = self.mask['active'] * x\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        local_logJ = self.mask['active'] * mixture_tan_transform_logJ(x1, s)\n",
    "        axes = tuple(range(1, len(local_logJ.shape)))\n",
    "        logJ = torch.sum(local_logJ, dim=axes)\n",
    "        fx1 = self.mask['active'] * mixture_tan_transform(x1, s).squeeze(1)\n",
    "\n",
    "        fx = (\n",
    "            self.mask['active'] * torch_mod(fx1 + t) +\n",
    "            self.mask['passive'] * x +\n",
    "            self.mask['frozen'] * x)\n",
    "        return fx, logJ\n",
    "\n",
    "    def reverse(self, fx):\n",
    "        fx2 = self.mask['frozen'] * fx\n",
    "        net_out = self.net(stack_cos_sin(fx2))\n",
    "        assert net_out.shape[1] >= 2, 'CNN must output n_mix (s_i) + 1 (t) channels'\n",
    "        s, t = net_out[:,:-1], net_out[:,-1]\n",
    "\n",
    "        x1 = torch_mod(self.mask['active'] * (fx - t).unsqueeze(1))\n",
    "        transform = lambda x: self.mask['active'] * mixture_tan_transform(x, s)\n",
    "        x1 = invert_transform_bisect(\n",
    "            x1, f=transform, tol=self.inv_prec, max_iter=self.inv_max_iter)\n",
    "        local_logJ = self.mask['active'] * mixture_tan_transform_logJ(x1, s)\n",
    "        axes = tuple(range(1, len(local_logJ.shape)))\n",
    "        logJ = -torch.sum(local_logJ, dim=axes)\n",
    "        x1 = x1.squeeze(1)\n",
    "\n",
    "        x = (\n",
    "            self.mask['active'] * x1 +\n",
    "            self.mask['passive'] * fx +\n",
    "            self.mask['frozen'] * fx2)\n",
    "        return x, logJ\n",
    "def make_u1_equiv_layers(*, n_layers, n_mixture_comps, lattice_shape, hidden_sizes, kernel_size):\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        # periodically loop through all arrangements of maskings\n",
    "        mu = i % 2\n",
    "        off = (i//2) % 4\n",
    "        in_channels = 2 # x - > (cos(x), sin(x))\n",
    "        out_channels = n_mixture_comps + 1 # for mixture s and t, respectively\n",
    "        net = make_conv_net(in_channels=in_channels, out_channels=out_channels,\n",
    "            hidden_sizes=hidden_sizes, kernel_size=kernel_size,\n",
    "            use_final_tanh=False)\n",
    "        plaq_coupling = NCPPlaqCouplingLayer(\n",
    "            net, mask_shape=lattice_shape, mask_mu=mu, mask_off=off)\n",
    "        link_coupling = GaugeEquivCouplingLayer(\n",
    "            lattice_shape=lattice_shape, mask_mu=mu, mask_off=off, \n",
    "            plaq_coupling=plaq_coupling)\n",
    "        layers.append(link_coupling)\n",
    "    return torch.nn.ModuleList(layers)\n",
    "\n",
    "def flow_train(param):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    # Theory\n",
    "    lattice_shape = param.lat\n",
    "    link_shape = (2,*param.lat)\n",
    "    beta = param.beta\n",
    "    u1_action = U1GaugeAction(beta)\n",
    "\n",
    "    # Model\n",
    "    prior = MultivariateUniform(torch.zeros(link_shape), 2*np.pi*torch.ones(link_shape))\n",
    "\n",
    "    n_layers = 16\n",
    "    n_s_nets = 2\n",
    "    hidden_sizes = [8,8]\n",
    "    kernel_size = 3\n",
    "    layers = make_u1_equiv_layers(lattice_shape=lattice_shape, n_layers=n_layers, n_mixture_comps=n_s_nets,\n",
    "                                  hidden_sizes=hidden_sizes, kernel_size=kernel_size)\n",
    "    set_weights(layers)\n",
    "    model = {'layers': layers, 'prior': prior}\n",
    "\n",
    "    # Training\n",
    "    base_lr = .001\n",
    "    optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)\n",
    "\n",
    "    # ADJUST ME\n",
    "    N_era = 10\n",
    "    #\n",
    "    N_epoch = 100\n",
    "    batch_size = 64\n",
    "    print_freq = N_epoch # epochs\n",
    "    plot_freq = 1 # epochs\n",
    "\n",
    "    history = {\n",
    "        'loss' : [],\n",
    "        'logp' : [],\n",
    "        'logq' : [],\n",
    "        'ess' : []\n",
    "    }\n",
    "\n",
    "    for era in range(N_era):\n",
    "        for epoch in range(N_epoch):\n",
    "            train_step(model, u1_action, optimizer, history, batch_size)\n",
    "            if epoch % print_freq == 0:\n",
    "                print_metrics(history, print_freq, era, epoch)\n",
    "    return model,u1_action\n",
    "\n",
    "def flow_eval(model, u1_action):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    ensemble_size = 1024\n",
    "    u1_ens = make_mcmc_ensemble(model, u1_action, 64, ensemble_size)\n",
    "    print(\"Accept rate:\", np.mean(u1_ens['accepted']))\n",
    "    Q = grab(topo_charge(torch.stack(u1_ens['x'], axis=0)))\n",
    "    X_mean, X_err = bootstrap(Q**2, Nboot=100, binsize=16)\n",
    "    print(f'Topological susceptibility = {X_mean:.2f} +/- {X_err:.2f}')\n",
    "    print(f'... vs HMC estimate = 1.23 +/- 0.02')\n",
    "\n",
    "# ------ END CODE FROM https://arxiv.org/abs/2101.08176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 2, # 0.3\n",
    "    nstep = 8, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 32, # 2**16 # 2**10 # 2**15\n",
    "    #\n",
    "    nprint = 8,\n",
    "    seed = 1331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(param.seed)\n",
    "\n",
    "torch.set_num_threads(param.nth)\n",
    "torch.set_num_interop_threads(param.nth_interop)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(param.nth)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"0\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 32\n",
      "tau = 2\n",
      "steps = 8\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0\n",
      "plaq(x) 1.0  force.norm 9.246826593536493\n",
      "plaq(x) 0.8716592163190823  force.norm 13.60065719819702\n",
      "plaq(x) 0.851135619433136  force.norm 17.54309727438331\n",
      "plaq(x) 0.8122971355705815  force.norm 17.226420992986522\n",
      "Traj:    4  ACCEPT:  dH:  0.039753441  exp(-dH):  0.96102636   plaq:  0.78167924   topo:  0.0\n",
      "plaq(x) 0.7816792404187667  force.norm 17.04668749206334\n",
      "plaq(x) 0.803925239737245  force.norm 17.257422008841356\n",
      "plaq(x) 0.7820204923371585  force.norm 18.801543612929564\n",
      "plaq(x) 0.6928380205992224  force.norm 20.552012705600852\n",
      "Traj:    8  ACCEPT:  dH: -0.49108779   exp(-dH):  1.6340928    plaq:  0.66276864   topo:  0.0\n",
      "plaq(x) 0.6627686419055754  force.norm 20.666904963059917\n",
      "plaq(x) 0.6587275632025721  force.norm 20.307144284814655\n",
      "plaq(x) 0.6587275632025721  force.norm 21.37507207659306\n",
      "plaq(x) 0.6587275632025721  force.norm 20.887549533044353\n",
      "Traj:   12  ACCEPT:  dH:  0.7666949    exp(-dH):  0.4645459    plaq:  0.70901563   topo: -2.0\n",
      "plaq(x) 0.709015630993363  force.norm 19.08487536919451\n",
      "plaq(x) 0.69441037824107  force.norm 20.012485533111928\n",
      "plaq(x) 0.6814929912841954  force.norm 22.43739935037723\n",
      "plaq(x) 0.607579264807221  force.norm 20.900933911129485\n",
      "Traj:   16  ACCEPT:  dH:  0.17406958   exp(-dH):  0.84023844   plaq:  0.60173258   topo:  0.0\n",
      "plaq(x) 0.6017325782119243  force.norm 20.07646186162139\n",
      "plaq(x) 0.6134123231028221  force.norm 18.580739008071603\n",
      "plaq(x) 0.6034836308796291  force.norm 17.796693663429085\n",
      "plaq(x) 0.6034836308796291  force.norm 19.697065707605496\n",
      "Traj:   20  ACCEPT:  dH: -0.14114069   exp(-dH):  1.1515867    plaq:  0.56800807   topo:  0.0\n",
      "plaq(x) 0.568008068721731  force.norm 18.446475073667134\n",
      "plaq(x) 0.5443926998704746  force.norm 21.604299191483864\n",
      "plaq(x) 0.620152064238269  force.norm 19.07307486449033\n",
      "plaq(x) 0.620152064238269  force.norm 19.562801649821136\n",
      "Traj:   24  ACCEPT:  dH: -0.011416495  exp(-dH):  1.0114819    plaq:  0.67421294   topo: -1.0\n",
      "plaq(x) 0.6742129426567363  force.norm 19.823543323193842\n",
      "plaq(x) 0.6742129426567363  force.norm 19.27414819634527\n",
      "plaq(x) 0.6593194794177437  force.norm 17.913209371052073\n",
      "plaq(x) 0.7319619354235716  force.norm 17.907177182753443\n",
      "Traj:   28  ACCEPT:  dH: -0.23455398   exp(-dH):  1.2643447    plaq:  0.75243731   topo: -1.0\n",
      "plaq(x) 0.7524373051276104  force.norm 19.32175236618232\n",
      "plaq(x) 0.7183540372831848  force.norm 17.935025726272585\n",
      "plaq(x) 0.770627943471277  force.norm 17.90255770797596\n",
      "plaq(x) 0.6974063175018334  force.norm 17.312300424852953\n",
      "Traj:   32  REJECT:  dH:  0.06732888   exp(-dH):  0.93488768   plaq:  0.69740632   topo:  2.0\n",
      "plaq(x) 0.6974063175018334  force.norm 19.26130571679194\n",
      "plaq(x) 0.6974063175018334  force.norm 19.34693110807932\n",
      "plaq(x) 0.6416246071908924  force.norm 20.460901938442515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaq(x) 0.7610479287922444  force.norm 20.050901058812386\n",
      "Traj:   36  ACCEPT:  dH: -0.44681544   exp(-dH):  1.5633257    plaq:  0.6830973    topo: -1.0\n",
      "plaq(x) 0.6830972988381023  force.norm 19.89120090541536\n",
      "plaq(x) 0.792882147716837  force.norm 18.926432620913957\n",
      "plaq(x) 0.7942271941545354  force.norm 17.955239644923235\n",
      "plaq(x) 0.7395974365303474  force.norm 19.774720037352132\n",
      "Traj:   40  REJECT:  dH:  0.86492765   exp(-dH):  0.42108201   plaq:  0.73959744   topo:  0.0\n",
      "plaq(x) 0.7395974365303474  force.norm 18.804644638462666\n",
      "plaq(x) 0.7106992517638243  force.norm 19.420981297151343\n",
      "plaq(x) 0.7213639942543659  force.norm 22.596734514850795\n",
      "plaq(x) 0.6845860428125358  force.norm 20.137514494359916\n",
      "Traj:   44  REJECT:  dH:  0.36753542   exp(-dH):  0.6924388    plaq:  0.68458604   topo:  2.0\n",
      "plaq(x) 0.6845860428125358  force.norm 19.888313840695513\n",
      "plaq(x) 0.6586164502888421  force.norm 20.963819547319783\n",
      "plaq(x) 0.5943452464356014  force.norm 18.435901371082316\n",
      "plaq(x) 0.6317722243924963  force.norm 21.810470096009333\n",
      "Traj:   48  REJECT:  dH:  0.43155819   exp(-dH):  0.64949627   plaq:  0.63177222   topo:  2.0\n",
      "plaq(x) 0.6317722243924963  force.norm 21.611857267165693\n",
      "plaq(x) 0.5083313733496306  force.norm 22.723057159768757\n",
      "plaq(x) 0.5261891873309926  force.norm 21.563884353938757\n",
      "plaq(x) 0.5261891873309926  force.norm 20.929403885546765\n",
      "Traj:   52  ACCEPT:  dH:  1.0436684    exp(-dH):  0.35216044   plaq:  0.65825261   topo:  1.0\n",
      "plaq(x) 0.6582526101758437  force.norm 20.599734996518503\n",
      "plaq(x) 0.7071682676848364  force.norm 19.77216430121231\n",
      "plaq(x) 0.6521096552752886  force.norm 20.175558578336336\n",
      "plaq(x) 0.6072700936343428  force.norm 20.659572304307716\n",
      "Traj:   56  REJECT:  dH:  0.56667265   exp(-dH):  0.56741027   plaq:  0.60727009   topo:  1.0\n",
      "plaq(x) 0.6072700936343428  force.norm 21.145999767433622\n",
      "plaq(x) 0.6072700936343428  force.norm 20.782120131754084\n",
      "plaq(x) 0.611834770236996  force.norm 20.46694450984353\n",
      "plaq(x) 0.611834770236996  force.norm 20.27542662699178\n",
      "Traj:   60  ACCEPT:  dH:  0.692275     exp(-dH):  0.50043628   plaq:  0.62536314   topo:  0.0\n",
      "plaq(x) 0.6253631425465283  force.norm 18.781060725194358\n",
      "plaq(x) 0.6056370632451842  force.norm 19.84117204473842\n",
      "plaq(x) 0.6326020399078496  force.norm 19.373735401843547\n",
      "plaq(x) 0.5973550440007644  force.norm 21.976028212194418\n",
      "Traj:   64  REJECT:  dH:  1.177859     exp(-dH):  0.30793734   plaq:  0.59735504   topo:  0.0\n",
      "plaq(x) 0.5973550440007644  force.norm 21.8083569760828\n",
      "plaq(x) 0.5973550440007644  force.norm 21.70260333824023\n",
      "plaq(x) 0.6499213607834335  force.norm 19.317722401924254\n",
      "plaq(x) 0.60806192039327  force.norm 19.034080322811853\n",
      "Traj:   68  ACCEPT:  dH: -0.37683844   exp(-dH):  1.4576688    plaq:  0.62808767   topo:  0.0\n",
      "plaq(x) 0.6280876667812965  force.norm 21.623988106240084\n",
      "plaq(x) 0.6280876667812965  force.norm 21.857041545254294\n",
      "plaq(x) 0.6280876667812965  force.norm 21.57966713443519\n",
      "plaq(x) 0.673228578449823  force.norm 19.812880753821755\n",
      "Traj:   72  REJECT:  dH:  0.46368642   exp(-dH):  0.62896075   plaq:  0.67322858   topo:  0.0\n",
      "plaq(x) 0.673228578449823  force.norm 21.228483860214045\n",
      "plaq(x) 0.7503148142803109  force.norm 19.693444829730517\n",
      "plaq(x) 0.6713055332894666  force.norm 18.602564277027145\n",
      "plaq(x) 0.7033955145418381  force.norm 20.475029248553767\n",
      "Traj:   76  ACCEPT:  dH: -0.55810913   exp(-dH):  1.7473653    plaq:  0.63541361   topo:  0.0\n",
      "plaq(x) 0.6354136085514601  force.norm 21.89908058813222\n",
      "plaq(x) 0.6348089824903048  force.norm 20.348075398532806\n",
      "plaq(x) 0.6986955250530685  force.norm 19.455013679201922\n",
      "plaq(x) 0.6981429800787341  force.norm 19.83443664817256\n",
      "Traj:   80  ACCEPT:  dH: -0.16648215   exp(-dH):  1.1811424    plaq:  0.67580361   topo: -1.0\n",
      "plaq(x) 0.6758036125533773  force.norm 20.571604590980993\n",
      "plaq(x) 0.6406269326959924  force.norm 19.81899287175162\n",
      "plaq(x) 0.6870395996764331  force.norm 19.455377852815232\n",
      "plaq(x) 0.6870395996764331  force.norm 19.4445352368779\n",
      "Traj:   84  ACCEPT:  dH:  0.19689444   exp(-dH):  0.82127732   plaq:  0.69334874   topo:  1.0\n",
      "plaq(x) 0.6933487424049062  force.norm 18.680766870920518\n",
      "plaq(x) 0.6621115742366961  force.norm 18.988192445944897\n",
      "plaq(x) 0.728583769972538  force.norm 19.077209292637054\n",
      "plaq(x) 0.6651580178674125  force.norm 20.779282301981414\n",
      "Traj:   88  ACCEPT:  dH: -0.27715851   exp(-dH):  1.3193755    plaq:  0.61787692   topo:  2.0\n",
      "plaq(x) 0.6178769236293191  force.norm 20.838983943679658\n",
      "plaq(x) 0.6250474076411697  force.norm 16.767432392555467\n",
      "plaq(x) 0.741436493849918  force.norm 17.74208672121567\n",
      "plaq(x) 0.752847917093104  force.norm 19.284624212190867\n",
      "Traj:   92  ACCEPT:  dH:  0.062879541  exp(-dH):  0.93905658   plaq:  0.81406279   topo:  0.0\n",
      "plaq(x) 0.8140627882067851  force.norm 18.494788257486217\n",
      "plaq(x) 0.8140627882067851  force.norm 17.945936864911488\n",
      "plaq(x) 0.8047785551756401  force.norm 18.64232863277329\n",
      "plaq(x) 0.7354925698192221  force.norm 21.161721007121493\n",
      "Traj:   96  ACCEPT:  dH: -0.30953308   exp(-dH):  1.3627887    plaq:  0.67172111   topo: -1.0\n",
      "plaq(x) 0.6717211081026555  force.norm 20.27578307335573\n",
      "plaq(x) 0.6717211081026555  force.norm 20.492247850785244\n",
      "plaq(x) 0.6808999903118518  force.norm 17.514474121367957\n",
      "plaq(x) 0.7360716326491462  force.norm 19.390950904314455\n",
      "Traj:  100  ACCEPT:  dH:  0.73136404   exp(-dH):  0.48125209   plaq:  0.74855392   topo:  2.0\n",
      "plaq(x) 0.748553921703853  force.norm 18.342188220635695\n",
      "plaq(x) 0.7426242323535831  force.norm 19.321263179645392\n",
      "plaq(x) 0.7997418597013812  force.norm 21.342102340907434\n",
      "plaq(x) 0.7373356733879503  force.norm 19.866387307253202\n",
      "Traj:  104  ACCEPT:  dH:  0.21522269   exp(-dH):  0.80636185   plaq:  0.74338431   topo:  1.0\n",
      "plaq(x) 0.7433843124729658  force.norm 18.83758580404758\n",
      "plaq(x) 0.6577075661058334  force.norm 21.386072545502074\n",
      "plaq(x) 0.6577075661058334  force.norm 21.560849662981802\n",
      "plaq(x) 0.6577075661058334  force.norm 21.78482516050461\n",
      "Traj:  108  ACCEPT:  dH: -0.25866765   exp(-dH):  1.2952033    plaq:  0.6430394    topo: -1.0\n",
      "plaq(x) 0.6430393971787716  force.norm 22.901888319744778\n",
      "plaq(x) 0.6001929513765227  force.norm 22.952017897179786\n",
      "plaq(x) 0.6001929513765227  force.norm 20.77839338626362\n",
      "plaq(x) 0.6609436018743244  force.norm 19.922128329846814\n",
      "Traj:  112  ACCEPT:  dH:  0.21683768   exp(-dH):  0.80506064   plaq:  0.64486135   topo:  1.0\n",
      "plaq(x) 0.6448613466892972  force.norm 19.56356529937314\n",
      "plaq(x) 0.6942212507381165  force.norm 18.571318800454105\n",
      "plaq(x) 0.7213033342554187  force.norm 17.57993999672081\n",
      "plaq(x) 0.7301758515236567  force.norm 17.397422797052467\n",
      "Traj:  116  ACCEPT:  dH: -0.26975268   exp(-dH):  1.3096405    plaq:  0.76216958   topo:  1.0\n",
      "plaq(x) 0.7621695831236005  force.norm 19.768909812885536\n",
      "plaq(x) 0.7016575374662454  force.norm 20.478793979479082\n",
      "plaq(x) 0.6593931783548345  force.norm 20.535315542789206\n",
      "plaq(x) 0.6720042017942384  force.norm 19.772513689076792\n",
      "Traj:  120  ACCEPT:  dH:  0.60807233   exp(-dH):  0.54439928   plaq:  0.74089889   topo:  0.0\n",
      "plaq(x) 0.7408988881614605  force.norm 19.998327718981734\n",
      "plaq(x) 0.6318865474781088  force.norm 20.77405025651251\n",
      "plaq(x) 0.6138793367675293  force.norm 20.515470726945974\n",
      "plaq(x) 0.6368389794557721  force.norm 18.692714003967325\n",
      "Traj:  124  REJECT:  dH:  0.88785077   exp(-dH):  0.4115393    plaq:  0.63683898   topo:  1.0\n",
      "plaq(x) 0.6368389794557721  force.norm 19.08530354191878\n",
      "plaq(x) 0.7018753429297195  force.norm 20.30954225169742\n",
      "plaq(x) 0.6758868043512647  force.norm 21.163313539264212\n",
      "plaq(x) 0.6930409242958642  force.norm 17.40592933354375\n",
      "Traj:  128  ACCEPT:  dH: -0.059267479  exp(-dH):  1.061059     plaq:  0.72237555   topo:  0.0\n",
      "Run times:  [0.18394596502184868, 0.19716565404087305, 0.17967123701237142, 0.190457995980978]\n",
      "Per trajectory:  [0.005748311406932771, 0.006161426688777283, 0.005614726156636607, 0.005951812374405563]\n"
     ]
    }
   ],
   "source": [
    "def run(param, field = param.initializer()):\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                dH, exp_mdH, acc, field = hmc(param, field)\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field\n",
    "field = run(param)\n",
    "field = torch.reshape(field,(1,)+field.shape)\n",
    "field_run = field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Era 0 | Epoch 0 metrics ==\n",
      "\tloss -231.992\n",
      "\tlogp -3.00205\n",
      "\tlogq -234.994\n",
      "\tess 0.0170693\n",
      "== Era 1 | Epoch 0 metrics ==\n",
      "\tloss -275.905\n",
      "\tlogp 63.6682\n",
      "\tlogq -212.237\n",
      "\tess 0.0416148\n",
      "== Era 2 | Epoch 0 metrics ==\n",
      "\tloss -283.787\n",
      "\tlogp 78.9282\n",
      "\tlogq -204.858\n",
      "\tess 0.0788061\n",
      "== Era 3 | Epoch 0 metrics ==\n",
      "\tloss -285.188\n",
      "\tlogp 82.4937\n",
      "\tlogq -202.694\n",
      "\tess 0.108933\n",
      "== Era 4 | Epoch 0 metrics ==\n",
      "\tloss -285.484\n",
      "\tlogp 83.097\n",
      "\tlogq -202.387\n",
      "\tess 0.116336\n",
      "== Era 5 | Epoch 0 metrics ==\n",
      "\tloss -285.729\n",
      "\tlogp 83.6639\n",
      "\tlogq -202.065\n",
      "\tess 0.12435\n",
      "== Era 6 | Epoch 0 metrics ==\n",
      "\tloss -285.945\n",
      "\tlogp 84.1015\n",
      "\tlogq -201.843\n",
      "\tess 0.1345\n",
      "== Era 7 | Epoch 0 metrics ==\n",
      "\tloss -286.055\n",
      "\tlogp 84.2972\n",
      "\tlogq -201.758\n",
      "\tess 0.138752\n",
      "== Era 8 | Epoch 0 metrics ==\n",
      "\tloss -286.116\n",
      "\tlogp 84.4423\n",
      "\tlogq -201.674\n",
      "\tess 0.160338\n",
      "== Era 9 | Epoch 0 metrics ==\n",
      "\tloss -286.197\n",
      "\tlogp 84.4832\n",
      "\tlogq -201.714\n",
      "\tess 0.164783\n",
      "Accept rate: 0.26171875\n",
      "Topological susceptibility = 1.37 +/- 0.14\n",
      "... vs HMC estimate = 1.23 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "flow_model,flow_act = flow_train(param)\n",
    "flow_eval(flow_model,flow_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 32\n",
      "tau = 2\n",
      "steps = 8\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.7223755501192342  topo: 0.0\n",
      "plaq(x) 0.7223755501192342  force.norm 19.30637415866414\n",
      "plaq(x) 0.7092413400314266  force.norm 19.579542362304178\n",
      "plaq(x) 0.7092413400314266  force.norm 21.65793129711244\n",
      "plaq(x) 0.6584466370821707  force.norm 19.247408971983518\n",
      "Traj:    4  ACCEPT:  dH:  0.012577828  exp(-dH):  0.98750094   plaq:  0.66572024   topo: -1.0\n",
      "plaq(x) 0.6657202368924617  force.norm 18.817072826029083\n",
      "plaq(x) 0.7590899443935631  force.norm 18.66068694677559\n",
      "plaq(x) 0.7285725452059615  force.norm 18.461465002838946\n",
      "plaq(x) 0.789086555427265  force.norm 16.053745607639613\n",
      "Traj:    8  ACCEPT:  dH: -0.46751239   exp(-dH):  1.596019     plaq:  0.78343368   topo:  1.0\n",
      "plaq(x) 0.7834336779911539  force.norm 18.281320437634804\n",
      "plaq(x) 0.7885207875089345  force.norm 16.827694784913863\n",
      "plaq(x) 0.8025785003884783  force.norm 17.23904898330872\n",
      "plaq(x) 0.7901324662903704  force.norm 17.962393906483808\n",
      "Traj:   12  ACCEPT:  dH:  0.15729599   exp(-dH):  0.85445111   plaq:  0.80688231   topo:  0.0\n",
      "plaq(x) 0.8068823119223634  force.norm 16.690898861717507\n",
      "plaq(x) 0.7949170438963686  force.norm 17.633258259916232\n",
      "plaq(x) 0.7587679012402723  force.norm 16.16130726476137\n",
      "plaq(x) 0.7612165243765989  force.norm 19.536859006752824\n",
      "Traj:   16  ACCEPT:  dH:  0.11272969   exp(-dH):  0.89339212   plaq:  0.77915896   topo:  0.0\n",
      "plaq(x) 0.7791589621321566  force.norm 18.03324626062873\n",
      "plaq(x) 0.7600337198759026  force.norm 20.100094707480206\n",
      "plaq(x) 0.7944690000276946  force.norm 19.118076584170048\n",
      "plaq(x) 0.7619115395766165  force.norm 19.040587535487536\n",
      "Traj:   20  ACCEPT:  dH: -0.29113606   exp(-dH):  1.3379466    plaq:  0.69842014   topo:  1.0\n",
      "plaq(x) 0.698420139628736  force.norm 20.375180340305167\n",
      "plaq(x) 0.6472576588744077  force.norm 19.12064133623439\n",
      "plaq(x) 0.6650975649666958  force.norm 19.043911862185773\n",
      "plaq(x) 0.662814981014084  force.norm 18.120131105786527\n",
      "Traj:   24  ACCEPT:  dH:  0.12318187   exp(-dH):  0.88410286   plaq:  0.68450648   topo: -1.0\n",
      "plaq(x) 0.6845064794779433  force.norm 19.707154956444814\n",
      "plaq(x) 0.6845064794779433  force.norm 19.463919258202402\n",
      "plaq(x) 0.6845064794779433  force.norm 18.815186438576433\n",
      "plaq(x) 0.7316206713004886  force.norm 18.91233218542414\n",
      "Traj:   28  ACCEPT:  dH: -0.2244632    exp(-dH):  1.2516507    plaq:  0.71862816   topo: -1.0\n",
      "plaq(x) 0.7186281629034262  force.norm 19.455047762810075\n",
      "plaq(x) 0.7186281629034262  force.norm 19.30205645012017\n",
      "plaq(x) 0.720374831623207  force.norm 18.90942365103475\n",
      "plaq(x) 0.7463372117084901  force.norm 18.89694322533485\n",
      "Traj:   32  REJECT:  dH:  0.31426705   exp(-dH):  0.73032397   plaq:  0.74633721   topo:  1.0\n",
      "plaq(x) 0.7463372117084901  force.norm 19.809416277514607\n",
      "plaq(x) 0.7303751194985223  force.norm 19.918376809420945\n",
      "plaq(x) 0.7303751194985223  force.norm 18.36840124002528\n",
      "plaq(x) 0.7021077326356859  force.norm 20.4358284803779\n",
      "Traj:   36  ACCEPT:  dH: -0.15836948   exp(-dH):  1.171599     plaq:  0.68300822   topo:  1.0\n",
      "plaq(x) 0.6830082161343443  force.norm 19.67155113199689\n",
      "plaq(x) 0.6830082161343443  force.norm 18.877931320190353\n",
      "plaq(x) 0.6830082161343443  force.norm 20.13185209684803\n",
      "plaq(x) 0.6818866976352382  force.norm 18.733853009016713\n",
      "Traj:   40  ACCEPT:  dH: -0.38826022   exp(-dH):  1.4744134    plaq:  0.67735338   topo:  1.0\n",
      "plaq(x) 0.6773533806752159  force.norm 21.544649421208973\n",
      "plaq(x) 0.6422187692544072  force.norm 20.588893353582524\n",
      "plaq(x) 0.6457114920004555  force.norm 20.943768751048832\n",
      "plaq(x) 0.659414656892944  force.norm 17.833542216974138\n",
      "Traj:   44  ACCEPT:  dH: -0.13890818   exp(-dH):  1.1490186    plaq:  0.66472229   topo:  1.0\n",
      "plaq(x) 0.6647222905412202  force.norm 18.65666645592199\n",
      "plaq(x) 0.6647222905412202  force.norm 20.56575394409316\n",
      "plaq(x) 0.7125662875542929  force.norm 17.217972494546977\n",
      "plaq(x) 0.7125662875542929  force.norm 18.683048700250996\n",
      "Traj:   48  REJECT:  dH:  0.865874     exp(-dH):  0.42068371   plaq:  0.71256629   topo:  1.0\n",
      "plaq(x) 0.7125662875542929  force.norm 18.656927563172573\n",
      "plaq(x) 0.7352571345394141  force.norm 17.10216298788443\n",
      "plaq(x) 0.7036050799894089  force.norm 18.892156778835897\n",
      "plaq(x) 0.6970760314511072  force.norm 19.22030485807383\n",
      "Traj:   52  ACCEPT:  dH: -0.22589937   exp(-dH):  1.2534495    plaq:  0.71177343   topo:  0.0\n",
      "plaq(x) 0.7117734329043195  force.norm 21.85069781312718\n",
      "plaq(x) 0.7117734329043195  force.norm 20.5614158530764\n",
      "plaq(x) 0.7229301202683235  force.norm 18.697739824352965\n",
      "plaq(x) 0.7190675811207714  force.norm 16.62213907350353\n",
      "Traj:   56  REJECT:  dH:  0.21047634   exp(-dH):  0.81019823   plaq:  0.71906758   topo:  0.0\n",
      "plaq(x) 0.7190675811207714  force.norm 18.950093999438188\n",
      "plaq(x) 0.7202051179113582  force.norm 16.347820861573343\n",
      "plaq(x) 0.7189322241439076  force.norm 17.52202891439359\n",
      "plaq(x) 0.7401931169634202  force.norm 19.0442584646547\n",
      "Traj:   60  ACCEPT:  dH:  0.29245249   exp(-dH):  0.74643071   plaq:  0.77092408   topo:  0.0\n",
      "plaq(x) 0.7709240790258982  force.norm 17.75416046993102\n",
      "plaq(x) 0.7821795246745085  force.norm 17.077824926469383\n",
      "plaq(x) 0.8579244377272182  force.norm 18.19903490324076\n",
      "plaq(x) 0.8165612643180846  force.norm 17.373325521096277\n",
      "Traj:   64  ACCEPT:  dH:  0.3583051    exp(-dH):  0.69885982   plaq:  0.8243162    topo:  1.0\n",
      "plaq(x) 0.824316202378238  force.norm 17.598782852863856\n",
      "plaq(x) 0.767970455081326  force.norm 18.2668351194892\n",
      "plaq(x) 0.7491770143551302  force.norm 19.948629556949033\n",
      "plaq(x) 0.7777364387938808  force.norm 17.929856332994472\n",
      "Traj:   68  ACCEPT:  dH: -0.81539892   exp(-dH):  2.2600771    plaq:  0.64460031   topo:  1.0\n",
      "plaq(x) 0.6446003079842123  force.norm 20.339345759288477\n",
      "plaq(x) 0.6445574113434402  force.norm 20.13091265000211\n",
      "plaq(x) 0.6184846751582227  force.norm 20.556449047872622\n",
      "plaq(x) 0.6184846751582227  force.norm 21.142827426399283\n",
      "Traj:   72  ACCEPT:  dH:  0.44393561   exp(-dH):  0.64150673   plaq:  0.68180023   topo:  1.0\n",
      "plaq(x) 0.6818002266138341  force.norm 20.498540646118574\n",
      "plaq(x) 0.6965247282722348  force.norm 20.293292430558687\n",
      "plaq(x) 0.7075009317250488  force.norm 20.13417328623772\n",
      "plaq(x) 0.7134372885005389  force.norm 18.60777618648136\n",
      "Traj:   76  ACCEPT:  dH:  0.33751306   exp(-dH):  0.71354266   plaq:  0.69933454   topo: -1.0\n",
      "plaq(x) 0.6993345366890975  force.norm 16.624309375318088\n",
      "plaq(x) 0.6945169495120925  force.norm 17.74729044814418\n",
      "plaq(x) 0.6692464721599944  force.norm 18.98239841799817\n",
      "plaq(x) 0.6818432995466754  force.norm 19.673660956834706\n",
      "Traj:   80  ACCEPT:  dH: -0.17190619   exp(-dH):  1.1875664    plaq:  0.71341946   topo:  0.0\n",
      "plaq(x) 0.7134194550627517  force.norm 20.318057728586965\n",
      "plaq(x) 0.7069801749105789  force.norm 20.67264099958783\n",
      "plaq(x) 0.6916822210128879  force.norm 19.718603334694926\n",
      "plaq(x) 0.7430087389435401  force.norm 18.166668582958128\n",
      "Traj:   84  ACCEPT:  dH: -0.02212248   exp(-dH):  1.022369     plaq:  0.73619593   topo: -1.0\n",
      "plaq(x) 0.7361959321436322  force.norm 17.672030476585064\n",
      "plaq(x) 0.7184398415925574  force.norm 20.616169513458757\n",
      "plaq(x) 0.6745189675859733  force.norm 21.357376332662447\n",
      "plaq(x) 0.7319388091044484  force.norm 20.175239371342702\n",
      "Traj:   88  ACCEPT:  dH: -0.11750679   exp(-dH):  1.1246893    plaq:  0.7348251    topo:  1.0\n",
      "plaq(x) 0.7348251029237951  force.norm 18.992795235375347\n",
      "plaq(x) 0.761339349076577  force.norm 17.147673032261757\n",
      "plaq(x) 0.7209694626690696  force.norm 18.274538623675454\n",
      "plaq(x) 0.7274990276257721  force.norm 16.072824329361495\n",
      "Traj:   92  ACCEPT:  dH: -0.77083341   exp(-dH):  2.161567     plaq:  0.69405949   topo: -1.0\n",
      "plaq(x) 0.6940594860151383  force.norm 20.014061783488884\n",
      "plaq(x) 0.7312889866201009  force.norm 17.674492971260882\n",
      "plaq(x) 0.7334434721819256  force.norm 20.193466712147597\n",
      "plaq(x) 0.733409533382524  force.norm 18.9055618474045\n",
      "Traj:   96  ACCEPT:  dH: -0.13307014   exp(-dH):  1.1423301    plaq:  0.71538257   topo:  1.0\n",
      "plaq(x) 0.7153825679642798  force.norm 20.602089510249673\n",
      "plaq(x) 0.7153825679642798  force.norm 19.561283825540553\n",
      "plaq(x) 0.6771940268324926  force.norm 18.146646264844843\n",
      "plaq(x) 0.6371462072790901  force.norm 19.94936875256119\n",
      "Traj:  100  ACCEPT:  dH:  0.39054175   exp(-dH):  0.67669018   plaq:  0.71417788   topo:  1.0\n",
      "plaq(x) 0.7141778801814056  force.norm 19.591801172606132\n",
      "plaq(x) 0.7015488266120227  force.norm 19.30033194582134\n",
      "plaq(x) 0.7743810012930953  force.norm 19.485150077426496\n",
      "plaq(x) 0.6993564120020603  force.norm 21.368087723544818\n",
      "Traj:  104  ACCEPT:  dH:  0.48143816   exp(-dH):  0.61789412   plaq:  0.77396763   topo:  0.0\n",
      "plaq(x) 0.7739676316058368  force.norm 18.409864156374905\n",
      "plaq(x) 0.7323298502143774  force.norm 20.025890973724007\n",
      "plaq(x) 0.7597062727802999  force.norm 18.693599097359222\n",
      "plaq(x) 0.7725502002113094  force.norm 18.717023240995776\n",
      "Traj:  108  ACCEPT:  dH: -0.30320705   exp(-dH):  1.3541948    plaq:  0.74357585   topo:  0.0\n",
      "plaq(x) 0.7435758451519897  force.norm 18.13345068774477\n",
      "plaq(x) 0.7282291507426364  force.norm 18.978963328727925\n",
      "plaq(x) 0.6703866837103593  force.norm 21.210316608176218\n",
      "plaq(x) 0.728571265708339  force.norm 20.383763102311786\n",
      "Traj:  112  ACCEPT:  dH:  0.42243946   exp(-dH):  0.65544593   plaq:  0.69749761   topo: -2.0\n",
      "plaq(x) 0.6974976121895745  force.norm 20.038961248383824\n",
      "plaq(x) 0.7548948284227515  force.norm 20.45202342149126\n",
      "plaq(x) 0.6890924675059742  force.norm 18.01935498330348\n",
      "plaq(x) 0.697475656043066  force.norm 17.982178036209827\n",
      "Traj:  116  ACCEPT:  dH:  0.57526187   exp(-dH):  0.56255753   plaq:  0.74666714   topo:  0.0\n",
      "plaq(x) 0.7466671380222251  force.norm 17.543482158714113\n",
      "plaq(x) 0.7034567424136334  force.norm 17.550364270765108\n",
      "plaq(x) 0.6909861002054107  force.norm 18.40421016333465\n",
      "plaq(x) 0.7295575910692398  force.norm 20.336290407887468\n",
      "Traj:  120  ACCEPT:  dH:  0.12768982   exp(-dH):  0.88012633   plaq:  0.69037555   topo: -1.0\n",
      "plaq(x) 0.6903755484136251  force.norm 22.045017434521586\n",
      "plaq(x) 0.666338052762651  force.norm 21.49245906085477\n",
      "plaq(x) 0.7050983460771316  force.norm 18.75565745769732\n",
      "plaq(x) 0.74898991238551  force.norm 19.860417731519934\n",
      "Traj:  124  ACCEPT:  dH:  0.68590704   exp(-dH):  0.5036332    plaq:  0.71161595   topo:  0.0\n",
      "plaq(x) 0.7116159530822382  force.norm 16.21603287642893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaq(x) 0.7572547744062662  force.norm 16.276997516897946\n",
      "plaq(x) 0.7616016631073543  force.norm 17.54997776519086\n",
      "plaq(x) 0.7261685241250102  force.norm 18.169333565595853\n",
      "Traj:  128  ACCEPT:  dH: -0.12255145   exp(-dH):  1.1303773    plaq:  0.75748162   topo:  0.0\n",
      "Run times:  [0.21432100201491266, 0.18551558000035584, 0.18589406600221992, 0.20823513704817742]\n",
      "Per trajectory:  [0.006697531312966021, 0.00579736187501112, 0.005809189562569372, 0.006507348032755544]\n"
     ]
    }
   ],
   "source": [
    "field_run = run(param, field_run[0])\n",
    "field_run = torch.reshape(field_run,(1,)+field_run.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaq(field) 0.7574816190699719\n",
      "tensor([42.7758], grad_fn=<AddBackward0>) tensor([-42.7759], grad_fn=<AddBackward0>)\n",
      "original_action tensor(-5.9577, grad_fn=<AddBackward0>)\n",
      "eff_action tensor([1.8182], grad_fn=<AddBackward0>)\n",
      "plaq(x) 0.14702788742215822  logJ tensor([42.7758], grad_fn=<AddBackward0>)  force.norm 7.141560428067136\n",
      "plaq(y) 0.7574816718267348\n",
      "plaq(x) 0.7574816190699719  force.norm 16.639073153230758\n"
     ]
    }
   ],
   "source": [
    "flow = flow_model['layers']\n",
    "flow.eval()\n",
    "\n",
    "flows = flow\n",
    "\n",
    "field = torch.clone(field_run)\n",
    "\n",
    "print(f'plaq(field) {action(param, field[0]) / (-param.beta*param.volume)}')\n",
    "# field.requires_grad_(True)\n",
    "x = field\n",
    "logJ = 0.0\n",
    "for layer in reversed(flows):\n",
    "    x, lJ = layer.reverse(x)\n",
    "    logJ += lJ\n",
    "\n",
    "# x is the prior distribution now\n",
    "    \n",
    "x.requires_grad_(True)\n",
    "    \n",
    "y = x\n",
    "logJy = 0.0\n",
    "for layer in flows:\n",
    "    y, lJ = layer.forward(y)\n",
    "    logJy += lJ\n",
    "\n",
    "print(logJ,logJy)\n",
    "\n",
    "s = action(param, y[0]) - logJy\n",
    "\n",
    "# print(\"eff_action\", s + 136.3786)\n",
    "\n",
    "print(\"original_action\", action(param, y[0]) + 91)\n",
    "\n",
    "print(\"eff_action\", s + 56)\n",
    "\n",
    "s.backward()\n",
    "\n",
    "f = x.grad\n",
    "\n",
    "x.requires_grad_(False)\n",
    "\n",
    "print(f'plaq(x) {action(param, x[0]) / (-param.beta*param.volume)}  logJ {logJ}  force.norm {torch.linalg.norm(f)}')\n",
    "\n",
    "print(f'plaq(y) {action(param, y[0]) / (-param.beta*param.volume)}')\n",
    "\n",
    "print(f'plaq(x) {action(param, field_run[0]) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(force(param, field_run[0]))}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
