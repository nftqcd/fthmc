{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "\n",
    "# FT-HMC implemented for 8x8 2D QED (using SiLU as activation function).\n",
    "\n",
    "# Try to minimize size of the force in training. No significant improvements.\n",
    "\n",
    "# Some test on ergodicity\n",
    "# (calculate the probablity of generating the configs obtained via conventional HMC).\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Plot the force size distribution\n",
    "# Is the large force from the original action or Field-Transformation the determinant?\n",
    "# If from the determinant, then the fermion force won't cause problem for HMC\n",
    "\n",
    "# Use the same Field-Transformation for larger system (say 16x16, 32x32, 64x64, etc)\n",
    "# Study how the delta H depends on the system size ( perhaps delta H ~ sqrt(volume) )\n",
    "\n",
    "# Study the auto-correlation for observables, topo, plaq, flowed plaq, etc.\n",
    "\n",
    "# Improving the Field-Transformation to reduce force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from functools import reduce\n",
    "from field_transformation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Xiao-Yong\n",
    "\n",
    "class Param:\n",
    "    def __init__(self, beta = 6.0, lat = [64, 64], tau = 2.0, nstep = 50, ntraj = 256, nrun = 4, nprint = 256, seed = 11*13, randinit = False, nth = int(os.environ.get('OMP_NUM_THREADS', '2')), nth_interop = 2):\n",
    "        self.beta = beta\n",
    "        self.lat = lat\n",
    "        self.nd = len(lat)\n",
    "        self.volume = reduce(lambda x,y:x*y, lat)\n",
    "        self.tau = tau\n",
    "        self.nstep = nstep\n",
    "        self.dt = self.tau / self.nstep\n",
    "        self.ntraj = ntraj\n",
    "        self.nrun = nrun\n",
    "        self.nprint = nprint\n",
    "        self.seed = seed\n",
    "        self.randinit = randinit\n",
    "        self.nth = nth\n",
    "        self.nth_interop = nth_interop\n",
    "    def initializer(self):\n",
    "        if self.randinit:\n",
    "            return torch.empty((self.nd,) + self.lat).uniform_(-math.pi, math.pi)\n",
    "        else:\n",
    "            return torch.zeros((self.nd,) + self.lat)\n",
    "    def summary(self):\n",
    "        return f\"\"\"latsize = {self.lat}\n",
    "volume = {self.volume}\n",
    "beta = {self.beta}\n",
    "trajs = {self.ntraj}\n",
    "tau = {self.tau}\n",
    "steps = {self.nstep}\n",
    "seed = {self.seed}\n",
    "nth = {self.nth}\n",
    "nth_interop = {self.nth_interop}\n",
    "\"\"\"\n",
    "    def uniquestr(self):\n",
    "        lat = \".\".join(str(x) for x in self.lat)\n",
    "        return f\"out_l{lat}_b{self.beta}_n{self.ntraj}_t{self.tau}_s{self.nstep}.out\"\n",
    "\n",
    "def action(param, f):\n",
    "    return (-param.beta)*torch.sum(torch.cos(plaqphase(f)))\n",
    "\n",
    "def force(param, f):\n",
    "    f.requires_grad_(True)\n",
    "    s = action(param, f)\n",
    "    f.grad = None\n",
    "    s.backward()\n",
    "    ff = f.grad\n",
    "    f.requires_grad_(False)\n",
    "    return ff\n",
    "\n",
    "plaqphase = lambda f: f[0,:] - f[1,:] - torch.roll(f[0,:], shifts=-1, dims=1) + torch.roll(f[1,:], shifts=-1, dims=0)\n",
    "topocharge = lambda f: torch.floor(0.1 + torch.sum(regularize(plaqphase(f))) / (2*math.pi))\n",
    "def regularize(f):\n",
    "    p2 = 2*math.pi\n",
    "    f_ = (f - math.pi) / p2\n",
    "    return p2*(f_ - torch.floor(f_) - 0.5)\n",
    "\n",
    "def leapfrog(param, x, p):\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = force(param, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    print(f'plaq(x) {action(param, x) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(f)}')\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        p_ = p_ + (-dt)*force(param, x_)\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    return (x_, p_)\n",
    "def hmc(param, x):\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = action(param, x) + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = leapfrog(param, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = action(param, xr) + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    newx = xr if acc else x\n",
    "    return (dH, exp_mdH, acc, newx)\n",
    "\n",
    "put = lambda s: sys.stdout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 2, # 0.3\n",
    "    nstep = 8, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 2, # 2**16 # 2**10 # 2**15\n",
    "    #\n",
    "    nprint = 2,\n",
    "    seed = 1331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(param.seed)\n",
    "\n",
    "torch.set_num_threads(param.nth)\n",
    "torch.set_num_interop_threads(param.nth_interop)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(param.nth)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"0\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0 torch.Size([2, 8, 8])\n",
      "plaq(x) 1.0  force.norm 0.28484840702762815\n",
      "Traj:    1  ACCEPT:  dH: -0.0030182743  exp(-dH):  1.0030228    plaq:  0.79750934   topo:  0.0\n",
      "plaq(x) 0.7975093376581902  force.norm 18.620514556154312\n",
      "Traj:    2  ACCEPT:  dH: -0.00058593622  exp(-dH):  1.0005861    plaq:  0.69746949   topo:  0.0\n",
      "plaq(x) 0.6974694869063531  force.norm 19.25952813968039\n",
      "Traj:    3  ACCEPT:  dH:  0.00035171744  exp(-dH):  0.99964834   plaq:  0.69743842   topo:  0.0\n",
      "plaq(x) 0.69743841846725  force.norm 18.004364386744186\n",
      "Traj:    4  ACCEPT:  dH:  0.00058309356  exp(-dH):  0.99941708   plaq:  0.78234744   topo:  0.0\n",
      "plaq(x) 0.7823474412175998  force.norm 16.620225849561447\n",
      "Traj:    5  ACCEPT:  dH: -0.00069129855  exp(-dH):  1.0006915    plaq:  0.7515997    topo:  0.0\n",
      "plaq(x) 0.7515996999323443  force.norm 18.63207422212415\n",
      "Traj:    6  ACCEPT:  dH:  3.2753275e-05  exp(-dH):  0.99996725   plaq:  0.77755838   topo:  0.0\n",
      "plaq(x) 0.7775583839911958  force.norm 18.756388570885953\n",
      "Traj:    7  ACCEPT:  dH:  0.00020333239  exp(-dH):  0.99979669   plaq:  0.70249757   topo:  0.0\n",
      "plaq(x) 0.7024975721180136  force.norm 17.031635337518416\n",
      "Traj:    8  ACCEPT:  dH: -0.00055174724  exp(-dH):  1.0005519    plaq:  0.69431944   topo:  1.0\n",
      "plaq(x) 0.694319435256533  force.norm 19.319877543294286\n",
      "Traj:    9  ACCEPT:  dH:  0.00050527994  exp(-dH):  0.99949485   plaq:  0.7116651    topo:  0.0\n",
      "plaq(x) 0.7116650962328882  force.norm 18.07499895946621\n",
      "Traj:   10  ACCEPT:  dH:  0.0004742555  exp(-dH):  0.99952586   plaq:  0.76292746   topo:  1.0\n",
      "plaq(x) 0.7629274574129337  force.norm 16.56848380713752\n",
      "Traj:   11  ACCEPT:  dH: -0.0015806083  exp(-dH):  1.0015819    plaq:  0.65123865   topo:  1.0\n",
      "plaq(x) 0.6512386473249815  force.norm 20.900040164815568\n",
      "Traj:   12  ACCEPT:  dH: -4.253385e-06  exp(-dH):  1.0000043    plaq:  0.68051993   topo:  1.0\n",
      "plaq(x) 0.6805199255485757  force.norm 21.359908664734032\n",
      "Traj:   13  ACCEPT:  dH:  0.00099762845  exp(-dH):  0.99900287   plaq:  0.76192232   topo:  1.0\n",
      "plaq(x) 0.7619223171586532  force.norm 18.946119560193587\n",
      "Traj:   14  ACCEPT:  dH: -0.0011331109  exp(-dH):  1.0011338    plaq:  0.69641191   topo:  1.0\n",
      "plaq(x) 0.6964119060032214  force.norm 22.064135285087357\n",
      "Traj:   15  ACCEPT:  dH:  0.00090813985  exp(-dH):  0.99909227   plaq:  0.75146181   topo:  1.0\n",
      "plaq(x) 0.7514618057708159  force.norm 19.325425073725203\n",
      "Traj:   16  ACCEPT:  dH:  0.00042881266  exp(-dH):  0.99957128   plaq:  0.78871357   topo:  1.0\n",
      "Run times:  [0.1636185009847395, 0.14224834300694056, 0.1282898779900279, 0.1447775700071361]\n",
      "Per trajectory:  [0.04090462524618488, 0.03556208575173514, 0.03207246949750697, 0.03619439250178402]\n"
     ]
    }
   ],
   "source": [
    "def run(param, field = None):\n",
    "    if field is None:\n",
    "        field = param.initializer()\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo} {field.shape}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                dH, exp_mdH, acc, field = hmc(param, field)\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field\n",
    "field = run(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_flow(flow, f):\n",
    "    for layer in flow:\n",
    "        f, lJ = layer.forward(f)\n",
    "    return f.detach()\n",
    "\n",
    "def ft_flow_inv(flow, f):\n",
    "    for layer in reversed(flow):\n",
    "        f, lJ = layer.reverse(f)\n",
    "    return f.detach()\n",
    "\n",
    "def ft_action(param, flow, f):\n",
    "    y = f\n",
    "    logJy = 0.0\n",
    "    for layer in flow:\n",
    "        y, lJ = layer.forward(y)\n",
    "        logJy += lJ\n",
    "    action = U1GaugeAction(param.beta)\n",
    "    s = action(y) - logJy\n",
    "    return s\n",
    "\n",
    "def ft_force(param, flow, field, create_graph = False):\n",
    "    # f is the field follows the transformed distribution (close to prior distribution)\n",
    "    f = field\n",
    "    f.requires_grad_(True)\n",
    "    s = ft_action(param, flow, f)\n",
    "    ss = torch.sum(s)\n",
    "    # f.grad = None\n",
    "    ff, = torch.autograd.grad(ss, f, create_graph = create_graph)\n",
    "    f.requires_grad_(False)\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, action, optimizer, metrics, batch_size, with_force = False, pre_model = None):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    optimizer.zero_grad()\n",
    "    #\n",
    "    xi = None\n",
    "    if pre_model != None:\n",
    "        pre_layers, pre_prior = pre_model['layers'], pre_model['prior']\n",
    "        pre_xi = pre_prior.sample_n(batch_size)\n",
    "        x = ft_flow(pre_layers, pre_xi)\n",
    "        xi = ft_flow_inv(layers, x)\n",
    "    #\n",
    "    xi, x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size, xi=xi)\n",
    "    logp = -action(x)\n",
    "    #\n",
    "    force_size = torch.tensor(0.0)\n",
    "    dkl = calc_dkl(logp, logq)\n",
    "    loss = torch.tensor(0.0)\n",
    "    if with_force:\n",
    "        assert pre_model != None\n",
    "        force = ft_force(param, layers, xi, True)\n",
    "        force_size = torch.sum(torch.square(force))\n",
    "        loss = force_size\n",
    "    else:\n",
    "        loss = dkl\n",
    "    #\n",
    "    loss.backward()\n",
    "    #\n",
    "    # minimization target\n",
    "    # loss mini\n",
    "    # -> (logq - logp) mini\n",
    "    # -> (action - logJ) mini\n",
    "    #\n",
    "    optimizer.step()\n",
    "    ess = compute_ess(logp, logq)\n",
    "    #\n",
    "    print(grab(loss),\n",
    "          grab(force_size),\n",
    "          grab(dkl),\n",
    "          grab(ess),\n",
    "          torch.linalg.norm(ft_force(param, layers, xi)))\n",
    "    #\n",
    "    metrics['loss'].append(grab(loss))\n",
    "    metrics['force'].append(grab(force_size))\n",
    "    metrics['dkl'].append(grab(dkl))\n",
    "    metrics['logp'].append(grab(logp))\n",
    "    metrics['logq'].append(grab(logq))\n",
    "    metrics['ess'].append(grab(ess))\n",
    "\n",
    "def flow_train(param, with_force = False, pre_model = None):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    # Theory\n",
    "    lattice_shape = param.lat\n",
    "    link_shape = (2,*param.lat)\n",
    "    beta = param.beta\n",
    "    u1_action = U1GaugeAction(beta)\n",
    "    # Model\n",
    "    prior = MultivariateUniform(torch.zeros(link_shape), 2*np.pi*torch.ones(link_shape))\n",
    "    #\n",
    "    n_layers = 24\n",
    "    n_s_nets = 2\n",
    "    hidden_sizes = [8,8]\n",
    "    kernel_size = 3\n",
    "    layers = make_u1_equiv_layers(lattice_shape=lattice_shape, n_layers=n_layers, n_mixture_comps=n_s_nets,\n",
    "                                  hidden_sizes=hidden_sizes, kernel_size=kernel_size)\n",
    "    set_weights(layers)\n",
    "    model = {'layers': layers, 'prior': prior}\n",
    "    # Training\n",
    "    base_lr = .001\n",
    "    optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)\n",
    "    optimizer_wf = torch.optim.Adam(model['layers'].parameters(), lr=base_lr / 100.0)\n",
    "    #\n",
    "    # ADJUST ME\n",
    "    N_era = 10\n",
    "    N_epoch = 100\n",
    "    #\n",
    "    batch_size = 64\n",
    "    print_freq = N_epoch # epochs\n",
    "    plot_freq = 1 # epochs\n",
    "    history = {\n",
    "        'loss' : [],\n",
    "        'force' : [],\n",
    "        'dkl' : [],\n",
    "        'logp' : [],\n",
    "        'logq' : [],\n",
    "        'ess' : []\n",
    "    }\n",
    "    for era in range(N_era):\n",
    "        for epoch in range(N_epoch):\n",
    "            train_step(model, u1_action, optimizer, history, batch_size)\n",
    "            if with_force:\n",
    "                train_step(model, u1_action, optimizer_wf, history, batch_size,\n",
    "                           with_force = with_force, pre_model = pre_model)\n",
    "            if epoch % print_freq == 0:\n",
    "                print_metrics(history, print_freq, era, epoch)\n",
    "    return model,u1_action\n",
    "\n",
    "def flow_eval(model, u1_action):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    ensemble_size = 1024\n",
    "    u1_ens = make_mcmc_ensemble(model, u1_action, 64, ensemble_size)\n",
    "    print(\"Accept rate:\", np.mean(u1_ens['accepted']))\n",
    "    Q = grab(topo_charge(torch.stack(u1_ens['x'], axis=0)))\n",
    "    X_mean, X_err = bootstrap(Q**2, Nboot=100, binsize=16)\n",
    "    print(f'Topological susceptibility = {X_mean:.2f} +/- {X_err:.2f}')\n",
    "    print(f'... vs HMC estimate = 1.23 +/- 0.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-235.80379178382825 0.0 -235.80379178382825 0.015742986900554163 tensor(173.9276)\n",
      "== Era 0 | Epoch 0 metrics ==\n",
      "\tloss -235.804\n",
      "\tforce 0\n",
      "\tdkl -235.804\n",
      "\tlogp 0.693854\n",
      "\tlogq -235.11\n",
      "\tess 0.015743\n",
      "-240.22592204900428 0.0 -240.22592204900428 0.01726504117436026 tensor(169.3064)\n",
      "-241.73488535507389 0.0 -241.73488535507389 0.016400911777968756 tensor(167.0606)\n",
      "-247.4616937980544 0.0 -247.4616937980544 0.015810153849540865 tensor(160.3539)\n",
      "-253.25240939507333 0.0 -253.25240939507333 0.01941417680419557 tensor(158.7390)\n",
      "-256.75166216575127 0.0 -256.75166216575127 0.015804690643426364 tensor(154.5671)\n",
      "-257.21331835493976 0.0 -257.21331835493976 0.01562887239859159 tensor(157.1432)\n",
      "-261.5854650563474 0.0 -261.5854650563474 0.020297799438260253 tensor(151.7423)\n",
      "-265.36695333901866 0.0 -265.36695333901866 0.0156412599546812 tensor(151.7542)\n",
      "-266.2862499543668 0.0 -266.2862499543668 0.016146700456890064 tensor(155.4196)\n",
      "-270.4000967148002 0.0 -270.4000967148002 0.01831122343147412 tensor(156.1291)\n",
      "-272.47532494175294 0.0 -272.47532494175294 0.04926610112492897 tensor(161.7628)\n",
      "-274.0188336018449 0.0 -274.0188336018449 0.01720192443603824 tensor(164.9150)\n",
      "-276.62558016413567 0.0 -276.62558016413567 0.02571803697602018 tensor(171.3087)\n",
      "-278.0437584060316 0.0 -278.0437584060316 0.0465059199441165 tensor(167.8843)\n",
      "-278.55970711893843 0.0 -278.55970711893843 0.12872091592793028 tensor(172.7423)\n",
      "-279.621035979776 0.0 -279.621035979776 0.05532086467648965 tensor(190.1240)\n",
      "-279.01972318243645 0.0 -279.01972318243645 0.03938253604333164 tensor(200.9539)\n",
      "-279.76487981304757 0.0 -279.76487981304757 0.054519163219636 tensor(197.8722)\n",
      "-278.52186411795196 0.0 -278.52186411795196 0.04685051049393984 tensor(190.9811)\n",
      "-279.3359481493468 0.0 -279.3359481493468 0.045447452894190415 tensor(200.5659)\n",
      "-278.2460912674644 0.0 -278.2460912674644 0.062389632631352804 tensor(220.7544)\n",
      "-278.34046858181995 0.0 -278.34046858181995 0.05001545440329313 tensor(206.7338)\n",
      "-278.8382760601778 0.0 -278.8382760601778 0.044844557072964274 tensor(204.9199)\n",
      "-277.82948138271104 0.0 -277.82948138271104 0.018040034645773903 tensor(209.8352)\n",
      "-277.5916656840476 0.0 -277.5916656840476 0.01636556036707938 tensor(206.6545)\n",
      "-278.1507596029114 0.0 -278.1507596029114 0.06118730824765313 tensor(207.9465)\n",
      "-278.8101103706662 0.0 -278.8101103706662 0.034171797405028785 tensor(197.2281)\n",
      "-279.4655671051157 0.0 -279.4655671051157 0.04598678738027564 tensor(184.8082)\n",
      "-280.6271984343321 0.0 -280.6271984343321 0.01813896051119748 tensor(195.5126)\n",
      "-280.49219102778613 0.0 -280.49219102778613 0.05813158193591589 tensor(188.1255)\n",
      "-280.2068988846669 0.0 -280.2068988846669 0.02102994632752189 tensor(190.1216)\n",
      "-280.07557284962 0.0 -280.07557284962 0.03678629248046407 tensor(194.5040)\n",
      "-280.27826695532247 0.0 -280.27826695532247 0.02874239268579077 tensor(192.5850)\n",
      "-280.1873088901689 0.0 -280.1873088901689 0.08962251720645786 tensor(180.5103)\n",
      "-280.8979853661697 0.0 -280.8979853661697 0.02286624294172809 tensor(176.2874)\n",
      "-279.7099388229209 0.0 -279.7099388229209 0.07881664348467764 tensor(178.6955)\n",
      "-280.85005605236887 0.0 -280.85005605236887 0.055791318173407024 tensor(172.9682)\n",
      "-281.22864291613104 0.0 -281.22864291613104 0.022475915863308292 tensor(172.0726)\n",
      "-280.76011951779475 0.0 -280.76011951779475 0.034573171480805534 tensor(171.1435)\n",
      "-280.7015899077326 0.0 -280.7015899077326 0.041598648103628816 tensor(175.0371)\n",
      "-280.07599874405184 0.0 -280.07599874405184 0.01571257750061264 tensor(179.4415)\n",
      "-280.15991748188526 0.0 -280.15991748188526 0.0890754718294826 tensor(172.4989)\n",
      "-280.51062148805613 0.0 -280.51062148805613 0.08400647854928746 tensor(173.4484)\n",
      "-281.0749944569544 0.0 -281.0749944569544 0.019619205321069054 tensor(173.8982)\n",
      "-280.32840611227647 0.0 -280.32840611227647 0.039978622586709346 tensor(165.3508)\n",
      "-280.55509908945055 0.0 -280.55509908945055 0.06077742684745242 tensor(172.4607)\n",
      "-281.0819986780939 0.0 -281.0819986780939 0.042309647445771294 tensor(176.0983)\n",
      "-280.09489949679704 0.0 -280.09489949679704 0.07837078262497556 tensor(183.6973)\n",
      "-280.802478379073 0.0 -280.802478379073 0.032545641534498564 tensor(182.5193)\n",
      "-281.49567224445815 0.0 -281.49567224445815 0.027581042553766463 tensor(176.5550)\n",
      "-280.4007903365286 0.0 -280.4007903365286 0.034283307159461957 tensor(172.2658)\n",
      "-281.93450647976016 0.0 -281.93450647976016 0.09477756280786527 tensor(180.6309)\n",
      "-280.86506097144945 0.0 -280.86506097144945 0.02059988318219647 tensor(181.0129)\n",
      "-280.89788182581094 0.0 -280.89788182581094 0.022247776021793435 tensor(187.7786)\n",
      "-281.1891778348007 0.0 -281.1891778348007 0.061989714949322615 tensor(186.5429)\n",
      "-280.31052060625746 0.0 -280.31052060625746 0.12670764213761346 tensor(177.8267)\n",
      "-282.51824385883754 0.0 -282.51824385883754 0.04110197507992704 tensor(179.2902)\n",
      "-280.8897961363683 0.0 -280.8897961363683 0.03578817012174448 tensor(180.6850)\n",
      "-281.67261027289476 0.0 -281.67261027289476 0.020884132175673625 tensor(187.8300)\n",
      "-281.2433286605999 0.0 -281.2433286605999 0.07685460093425996 tensor(182.5710)\n",
      "-281.39941651518035 0.0 -281.39941651518035 0.01967491947136179 tensor(184.8858)\n",
      "-281.44447396798336 0.0 -281.44447396798336 0.015986711230498888 tensor(182.5059)\n",
      "-282.1103157083836 0.0 -282.1103157083836 0.04504358008329908 tensor(183.4524)\n",
      "-280.9787463953471 0.0 -280.9787463953471 0.036725767512740426 tensor(168.4463)\n",
      "-280.53249089993466 0.0 -280.53249089993466 0.056355509864126865 tensor(182.7690)\n",
      "-282.19144397426135 0.0 -282.19144397426135 0.04076380493124668 tensor(179.7477)\n",
      "-281.7119518561641 0.0 -281.7119518561641 0.058305202359699326 tensor(176.5427)\n",
      "-281.6959079076538 0.0 -281.6959079076538 0.024023842314468972 tensor(166.3750)\n",
      "-280.2845728767014 0.0 -280.2845728767014 0.036981267900423144 tensor(180.1940)\n",
      "-281.0296800962542 0.0 -281.0296800962542 0.05498758351809235 tensor(175.1306)\n",
      "-281.3336376168702 0.0 -281.3336376168702 0.0273282222066487 tensor(173.1741)\n",
      "-281.2295763241233 0.0 -281.2295763241233 0.03630821176004081 tensor(179.3191)\n",
      "-281.1784295910469 0.0 -281.1784295910469 0.038285968286917334 tensor(181.6012)\n",
      "-281.6338148299032 0.0 -281.6338148299032 0.10277893799146202 tensor(174.1253)\n",
      "-281.6642892115454 0.0 -281.6642892115454 0.042474873501440136 tensor(175.1895)\n",
      "-282.04426859565956 0.0 -282.04426859565956 0.11302439180849502 tensor(181.0515)\n",
      "-282.2171246535169 0.0 -282.2171246535169 0.04026248875481169 tensor(181.2859)\n",
      "-282.10812986663086 0.0 -282.10812986663086 0.07920310769711404 tensor(176.3626)\n",
      "-281.86096576294597 0.0 -281.86096576294597 0.15869952263176476 tensor(185.5843)\n",
      "-282.21128894428523 0.0 -282.21128894428523 0.02898837593474887 tensor(179.8114)\n",
      "-280.8509128977074 0.0 -280.8509128977074 0.08700997061605481 tensor(186.5723)\n",
      "-281.1877732527942 0.0 -281.1877732527942 0.023828303839758713 tensor(177.4217)\n",
      "-281.761208043354 0.0 -281.761208043354 0.04351549662214405 tensor(177.1457)\n",
      "-281.6323167069575 0.0 -281.6323167069575 0.020703850250403968 tensor(196.2096)\n",
      "-281.70737119266073 0.0 -281.70737119266073 0.030385222654674224 tensor(191.7956)\n",
      "-282.26686509613796 0.0 -282.26686509613796 0.018121128150640247 tensor(178.5193)\n",
      "-282.3289394185748 0.0 -282.3289394185748 0.015834773442371275 tensor(185.0120)\n",
      "-282.5646146845484 0.0 -282.5646146845484 0.03721718187262966 tensor(182.8940)\n",
      "-281.66630905225867 0.0 -281.66630905225867 0.09016990979064439 tensor(180.6176)\n",
      "-281.8691022646892 0.0 -281.8691022646892 0.03242604322875938 tensor(182.4766)\n",
      "-281.8105358433904 0.0 -281.8105358433904 0.03087916641312881 tensor(191.2861)\n",
      "-282.2159812639174 0.0 -282.2159812639174 0.05421624568195883 tensor(178.4135)\n",
      "-282.85015366684627 0.0 -282.85015366684627 0.10421619782066457 tensor(174.2103)\n",
      "-281.9970554553979 0.0 -281.9970554553979 0.04501972828116775 tensor(175.0787)\n",
      "-282.94012552043495 0.0 -282.94012552043495 0.021274293550750265 tensor(172.8371)\n",
      "-281.89170141835814 0.0 -281.89170141835814 0.0389766673337409 tensor(172.9879)\n",
      "-283.15599364900083 0.0 -283.15599364900083 0.01696672651246978 tensor(169.7644)\n",
      "-282.47809447276256 0.0 -282.47809447276256 0.057577135099320324 tensor(170.1345)\n",
      "-282.64467072393984 0.0 -282.64467072393984 0.0561849518070143 tensor(173.9761)\n",
      "-282.00670714588455 0.0 -282.00670714588455 0.04757257023644638 tensor(170.8849)\n",
      "== Era 1 | Epoch 0 metrics ==\n",
      "\tloss -278.203\n",
      "\tforce 0\n",
      "\tdkl -278.203\n",
      "\tlogp 68.451\n",
      "\tlogq -209.752\n",
      "\tess 0.0447474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-282.3031800495452 0.0 -282.3031800495452 0.03519414026309184 tensor(173.6366)\n",
      "-281.7106893399856 0.0 -281.7106893399856 0.044429059801847706 tensor(189.9906)\n",
      "-282.02463668393204 0.0 -282.02463668393204 0.02869624631395041 tensor(171.9115)\n",
      "-283.132869239756 0.0 -283.132869239756 0.019255968703855045 tensor(165.2447)\n",
      "-282.06928167181843 0.0 -282.06928167181843 0.02305745703770662 tensor(170.5575)\n",
      "-282.63741013978205 0.0 -282.63741013978205 0.03463303707863622 tensor(174.0301)\n",
      "-282.69424815211875 0.0 -282.69424815211875 0.07775488816439124 tensor(182.1365)\n",
      "-282.83367070770777 0.0 -282.83367070770777 0.1173986095789971 tensor(173.4950)\n",
      "-282.4292215109003 0.0 -282.4292215109003 0.04353524233108277 tensor(174.8651)\n",
      "-282.63920432682426 0.0 -282.63920432682426 0.057467884980354454 tensor(174.7497)\n",
      "-282.52266144131096 0.0 -282.52266144131096 0.1869519973917699 tensor(175.9306)\n",
      "-282.3960594537949 0.0 -282.3960594537949 0.10639430865819953 tensor(175.4130)\n",
      "-283.1874588939615 0.0 -283.1874588939615 0.07316614556035579 tensor(185.5502)\n",
      "-283.1071742096166 0.0 -283.1071742096166 0.11033916417667007 tensor(172.6518)\n",
      "-284.22318622434653 0.0 -284.22318622434653 0.07341238050431044 tensor(178.3674)\n",
      "-282.6782644014969 0.0 -282.6782644014969 0.04964608190177719 tensor(188.2040)\n",
      "-282.5867002071591 0.0 -282.5867002071591 0.01979867941971493 tensor(176.1086)\n",
      "-282.96012646123756 0.0 -282.96012646123756 0.06513096496416332 tensor(176.9925)\n",
      "-283.507229866559 0.0 -283.507229866559 0.019451505417378105 tensor(185.1243)\n",
      "-282.7754210874775 0.0 -282.7754210874775 0.07204193091620131 tensor(181.8959)\n",
      "-283.2024171454899 0.0 -283.2024171454899 0.058742829418482 tensor(170.0647)\n",
      "-282.7496708762466 0.0 -282.7496708762466 0.06744229921810016 tensor(189.1918)\n",
      "-282.19984885633585 0.0 -282.19984885633585 0.08623079508400139 tensor(183.4364)\n",
      "-283.46424653000787 0.0 -283.46424653000787 0.04030354199547442 tensor(169.0150)\n",
      "-283.0612352735594 0.0 -283.0612352735594 0.11139450370861136 tensor(176.9349)\n",
      "-282.29700158833884 0.0 -282.29700158833884 0.029870035508316312 tensor(173.2359)\n",
      "-283.04789136564364 0.0 -283.04789136564364 0.07469371942878723 tensor(180.8702)\n",
      "-283.29997675709865 0.0 -283.29997675709865 0.10333386148569358 tensor(184.7027)\n",
      "-283.2584641423234 0.0 -283.2584641423234 0.042256543041590114 tensor(164.9398)\n",
      "-283.5549297190919 0.0 -283.5549297190919 0.05284754294273788 tensor(176.7548)\n",
      "-283.4467956384508 0.0 -283.4467956384508 0.0313291659129965 tensor(169.0613)\n",
      "-283.2957660064998 0.0 -283.2957660064998 0.1090892861495012 tensor(168.3540)\n",
      "-283.36231638432264 0.0 -283.36231638432264 0.13317080561339126 tensor(178.3065)\n",
      "-283.33356307004675 0.0 -283.33356307004675 0.037717101793621026 tensor(169.9870)\n",
      "-283.6851986794627 0.0 -283.6851986794627 0.022691849782946284 tensor(162.7710)\n",
      "-283.82469976623 0.0 -283.82469976623 0.11088494592471682 tensor(173.6911)\n",
      "-283.263203303026 0.0 -283.263203303026 0.04599732008526972 tensor(166.0525)\n",
      "-283.96825444200033 0.0 -283.96825444200033 0.06766684517053939 tensor(176.9502)\n",
      "-283.1736355657762 0.0 -283.1736355657762 0.07462851811542069 tensor(185.8444)\n",
      "-283.46881889312465 0.0 -283.46881889312465 0.11987147049097922 tensor(180.1229)\n",
      "-283.4064167559816 0.0 -283.4064167559816 0.04601947783379247 tensor(182.7823)\n",
      "-283.5793508013917 0.0 -283.5793508013917 0.0783565986006164 tensor(175.5434)\n",
      "-283.93621492741386 0.0 -283.93621492741386 0.06598119556474961 tensor(178.8233)\n",
      "-283.91012809830215 0.0 -283.91012809830215 0.04993670394996056 tensor(182.1567)\n",
      "-284.3697748446327 0.0 -284.3697748446327 0.12103288366830631 tensor(160.9178)\n",
      "-283.9412174668041 0.0 -283.9412174668041 0.1767342221839888 tensor(169.1831)\n",
      "-284.29502791472567 0.0 -284.29502791472567 0.04776530373026755 tensor(171.9146)\n",
      "-283.96319330502206 0.0 -283.96319330502206 0.0741857367761963 tensor(165.7994)\n",
      "-284.2201830671402 0.0 -284.2201830671402 0.06989594801602658 tensor(172.9130)\n",
      "-284.45051366883473 0.0 -284.45051366883473 0.060882074213737696 tensor(175.0090)\n",
      "-283.8741287280267 0.0 -283.8741287280267 0.06459306405637323 tensor(179.6951)\n",
      "-283.6781404872687 0.0 -283.6781404872687 0.049603066985664704 tensor(178.4403)\n",
      "-283.82481370678715 0.0 -283.82481370678715 0.016166408115057963 tensor(169.3131)\n",
      "-284.1102161921149 0.0 -284.1102161921149 0.1006973460227664 tensor(182.6901)\n",
      "-284.08864210160567 0.0 -284.08864210160567 0.11976644547209168 tensor(179.7751)\n",
      "-283.6011374584261 0.0 -283.6011374584261 0.047718070371877336 tensor(175.5429)\n",
      "-284.53002920665483 0.0 -284.53002920665483 0.09251990685106692 tensor(168.7565)\n",
      "-284.02228372118014 0.0 -284.02228372118014 0.10871693142446873 tensor(194.3816)\n",
      "-284.6389806430451 0.0 -284.6389806430451 0.12263617010085445 tensor(162.3994)\n",
      "-283.85040911298563 0.0 -283.85040911298563 0.04448038210275689 tensor(185.6694)\n",
      "-283.64363095979536 0.0 -283.64363095979536 0.1589295256722647 tensor(177.9785)\n",
      "-284.08528412178595 0.0 -284.08528412178595 0.12204017675398636 tensor(164.1414)\n",
      "-284.5201044180991 0.0 -284.5201044180991 0.17213783609117464 tensor(174.7066)\n",
      "-284.09742051473444 0.0 -284.09742051473444 0.11808280917733889 tensor(171.3153)\n",
      "-284.10384556593675 0.0 -284.10384556593675 0.11329840995755039 tensor(157.0927)\n",
      "-284.48818801213673 0.0 -284.48818801213673 0.04377952157985698 tensor(175.0124)\n",
      "-284.6844803917569 0.0 -284.6844803917569 0.1400977834540603 tensor(172.9611)\n",
      "-284.8279505794618 0.0 -284.8279505794618 0.06381134877239508 tensor(181.9822)\n",
      "-283.84731463673705 0.0 -283.84731463673705 0.14575420698426356 tensor(170.0130)\n",
      "-284.5270323027262 0.0 -284.5270323027262 0.16989467527463406 tensor(160.4349)\n",
      "-284.3056092565243 0.0 -284.3056092565243 0.11429048397444173 tensor(170.5609)\n",
      "-284.6048803190805 0.0 -284.6048803190805 0.04444173957319538 tensor(165.0588)\n",
      "-284.84107993179305 0.0 -284.84107993179305 0.042264704696756375 tensor(156.7691)\n",
      "-285.0982292100611 0.0 -285.0982292100611 0.07452433132458808 tensor(169.0757)\n",
      "-284.5679132181375 0.0 -284.5679132181375 0.04548152510590806 tensor(166.8612)\n",
      "-284.28800473966487 0.0 -284.28800473966487 0.0586094215028961 tensor(177.2119)\n",
      "-285.10734248210156 0.0 -285.10734248210156 0.027581707935021547 tensor(182.1247)\n",
      "-284.7711944624117 0.0 -284.7711944624117 0.16990258486277215 tensor(161.8611)\n",
      "-284.9485453738905 0.0 -284.9485453738905 0.04237198967617256 tensor(179.6818)\n",
      "-284.54075318600235 0.0 -284.54075318600235 0.10447810921394249 tensor(168.9022)\n",
      "-284.99020154302303 0.0 -284.99020154302303 0.05931719238078446 tensor(178.7228)\n",
      "-284.492945606835 0.0 -284.492945606835 0.14653502961057366 tensor(198.6983)\n",
      "-284.70163936834473 0.0 -284.70163936834473 0.0904705489354465 tensor(169.7947)\n",
      "-285.24293129256785 0.0 -285.24293129256785 0.02412901724074726 tensor(172.3599)\n",
      "-284.0130427928664 0.0 -284.0130427928664 0.15249106412388816 tensor(169.2721)\n",
      "-284.75996831295873 0.0 -284.75996831295873 0.02632855942286506 tensor(179.1257)\n",
      "-285.75495444126886 0.0 -285.75495444126886 0.10864935249242606 tensor(153.9854)\n",
      "-284.8106414209853 0.0 -284.8106414209853 0.1052191440177331 tensor(172.1167)\n",
      "-285.54486035399725 0.0 -285.54486035399725 0.18552025122484742 tensor(186.2401)\n",
      "-285.1338640784657 0.0 -285.1338640784657 0.057733275784532266 tensor(175.2438)\n",
      "-285.27303873318783 0.0 -285.27303873318783 0.10084277279360045 tensor(155.3600)\n",
      "-285.169421877806 0.0 -285.169421877806 0.16481802801904358 tensor(162.0158)\n",
      "-285.4812602201305 0.0 -285.4812602201305 0.21647479998460994 tensor(150.0655)\n",
      "-285.18648045414227 0.0 -285.18648045414227 0.20412787577288885 tensor(166.8762)\n",
      "-284.74394280943994 0.0 -284.74394280943994 0.05471426533879927 tensor(170.6931)\n",
      "-285.08235824509353 0.0 -285.08235824509353 0.20725302838207318 tensor(173.3536)\n",
      "-284.9361512862854 0.0 -284.9361512862854 0.06251251654264173 tensor(178.2972)\n",
      "-285.4491674382099 0.0 -285.4491674382099 0.13403303249778092 tensor(163.1847)\n",
      "-285.5311706336511 0.0 -285.5311706336511 0.0524799363568913 tensor(170.9179)\n",
      "-285.14919858896826 0.0 -285.14919858896826 0.055671792326049005 tensor(155.7343)\n",
      "== Era 2 | Epoch 0 metrics ==\n",
      "\tloss -283.91\n",
      "\tforce 0\n",
      "\tdkl -283.91\n",
      "\tlogp 79.3769\n",
      "\tlogq -204.533\n",
      "\tess 0.084167\n",
      "-285.2184109390458 0.0 -285.2184109390458 0.058313902742299285 tensor(174.0637)\n",
      "-285.5080370194683 0.0 -285.5080370194683 0.1166042724279074 tensor(158.2435)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-285.47454534289994 0.0 -285.47454534289994 0.08551370972478504 tensor(159.2134)\n",
      "-285.7410910826486 0.0 -285.7410910826486 0.09694266245843546 tensor(166.2664)\n",
      "-285.1816577242462 0.0 -285.1816577242462 0.18581720592138307 tensor(160.6161)\n",
      "-284.9456329769272 0.0 -284.9456329769272 0.037887675923887025 tensor(165.5799)\n",
      "-285.1367904117073 0.0 -285.1367904117073 0.0709500809313619 tensor(161.8093)\n",
      "-284.8899231454957 0.0 -284.8899231454957 0.1292122816474404 tensor(175.4481)\n",
      "-285.48417859246416 0.0 -285.48417859246416 0.09883028991632839 tensor(170.3434)\n",
      "-285.20711737021287 0.0 -285.20711737021287 0.1686882618690036 tensor(159.7044)\n",
      "-285.5761087567976 0.0 -285.5761087567976 0.10562976216060008 tensor(153.3455)\n",
      "-284.9573830256865 0.0 -284.9573830256865 0.1613954717331739 tensor(166.5366)\n",
      "-285.52831036667357 0.0 -285.52831036667357 0.06433101101355754 tensor(157.1581)\n",
      "-285.5380265392524 0.0 -285.5380265392524 0.08475295738353089 tensor(168.5715)\n",
      "-285.52151778254535 0.0 -285.52151778254535 0.046224439999741886 tensor(145.8244)\n",
      "-284.73798765022883 0.0 -284.73798765022883 0.12556293282681696 tensor(177.4749)\n",
      "-285.0976324712978 0.0 -285.0976324712978 0.051669530337644536 tensor(171.8913)\n",
      "-285.3677441486632 0.0 -285.3677441486632 0.079609325826442 tensor(160.3034)\n",
      "-285.172117130299 0.0 -285.172117130299 0.14246042925814437 tensor(178.2159)\n",
      "-284.95505389019525 0.0 -284.95505389019525 0.09333795996034315 tensor(171.3762)\n",
      "-285.210791449917 0.0 -285.210791449917 0.02345102202011016 tensor(163.6698)\n",
      "-285.77290295724396 0.0 -285.77290295724396 0.1590933247135578 tensor(177.1358)\n",
      "-285.7009747862055 0.0 -285.7009747862055 0.07724134735040727 tensor(149.6147)\n",
      "-285.391373608909 0.0 -285.391373608909 0.06450193671957691 tensor(176.4151)\n",
      "-285.40595957764197 0.0 -285.40595957764197 0.2008997334958704 tensor(161.9324)\n",
      "-285.0249742498494 0.0 -285.0249742498494 0.16469151906215282 tensor(168.7602)\n",
      "-285.34496399192614 0.0 -285.34496399192614 0.08451825216577855 tensor(180.1230)\n",
      "-285.4542273420165 0.0 -285.4542273420165 0.1512933289333432 tensor(160.0746)\n",
      "-285.7394136091922 0.0 -285.7394136091922 0.07095784232521866 tensor(143.8819)\n",
      "-285.35257360520876 0.0 -285.35257360520876 0.22523899715807372 tensor(157.8147)\n",
      "-285.6514614256912 0.0 -285.6514614256912 0.12542073232142328 tensor(170.8566)\n",
      "-285.3086019845266 0.0 -285.3086019845266 0.24665216109748223 tensor(163.0398)\n",
      "-285.2200104747799 0.0 -285.2200104747799 0.19890735548451866 tensor(162.9307)\n",
      "-286.03461410440093 0.0 -286.03461410440093 0.060724437521455184 tensor(154.9434)\n",
      "-284.8816161975323 0.0 -284.8816161975323 0.14563991030520204 tensor(163.2169)\n",
      "-285.4859866499263 0.0 -285.4859866499263 0.15903325152069092 tensor(170.9442)\n",
      "-285.1185583000024 0.0 -285.1185583000024 0.10976198201358528 tensor(174.2226)\n",
      "-285.49877886890516 0.0 -285.49877886890516 0.2624752126350842 tensor(170.8950)\n",
      "-285.37324658736617 0.0 -285.37324658736617 0.07718100543474717 tensor(187.6936)\n",
      "-285.40862417951 0.0 -285.40862417951 0.03999043262133996 tensor(178.2936)\n",
      "-284.8063485431211 0.0 -284.8063485431211 0.13733116813623214 tensor(170.3104)\n",
      "-285.6282098060283 0.0 -285.6282098060283 0.10152011862808323 tensor(159.3864)\n",
      "-285.1068810633505 0.0 -285.1068810633505 0.10477600508486526 tensor(162.4142)\n",
      "-285.27508423020356 0.0 -285.27508423020356 0.10034933840116515 tensor(156.2405)\n",
      "-285.38100204360006 0.0 -285.38100204360006 0.07143800805349917 tensor(161.7828)\n",
      "-285.18245914653824 0.0 -285.18245914653824 0.12577538158486656 tensor(161.4112)\n",
      "-285.6478578258196 0.0 -285.6478578258196 0.20742398966617662 tensor(170.1285)\n",
      "-285.6591782869963 0.0 -285.6591782869963 0.08653047755324107 tensor(175.8828)\n",
      "-285.2087385595846 0.0 -285.2087385595846 0.15381965053536437 tensor(168.5506)\n",
      "-286.1258561011099 0.0 -286.1258561011099 0.07900627174344212 tensor(163.5564)\n",
      "-285.1630109045656 0.0 -285.1630109045656 0.10736049842533142 tensor(170.3251)\n",
      "-285.2642666096143 0.0 -285.2642666096143 0.05824219574695596 tensor(159.8768)\n",
      "-285.2407533016273 0.0 -285.2407533016273 0.025133883177783676 tensor(149.0011)\n",
      "-285.3878000143176 0.0 -285.3878000143176 0.13754289894840557 tensor(167.0263)\n",
      "-285.74013543106435 0.0 -285.74013543106435 0.2709526156846939 tensor(161.0943)\n",
      "-285.3269827196391 0.0 -285.3269827196391 0.14765894018364692 tensor(146.9850)\n",
      "-285.74927499460284 0.0 -285.74927499460284 0.0486481967757248 tensor(149.4949)\n",
      "-284.84690469649837 0.0 -284.84690469649837 0.04697929940557509 tensor(175.6567)\n",
      "-285.42050268531966 0.0 -285.42050268531966 0.1887581592851099 tensor(165.2185)\n",
      "-285.50508590616903 0.0 -285.50508590616903 0.2267452268441627 tensor(158.1366)\n",
      "-285.11426066992476 0.0 -285.11426066992476 0.11170305946448954 tensor(171.1510)\n",
      "-285.4200717483932 0.0 -285.4200717483932 0.08255416918835029 tensor(174.0107)\n",
      "-285.65882420936384 0.0 -285.65882420936384 0.2340175471610539 tensor(152.2888)\n",
      "-285.4040448912074 0.0 -285.4040448912074 0.12415036562652546 tensor(171.3159)\n",
      "-285.8326220236139 0.0 -285.8326220236139 0.132049420221156 tensor(159.6966)\n",
      "-286.12827467087465 0.0 -286.12827467087465 0.08635103738068722 tensor(160.6796)\n",
      "-285.40239419303754 0.0 -285.40239419303754 0.2114754887098206 tensor(149.3049)\n",
      "-285.9532353569974 0.0 -285.9532353569974 0.0900110540250892 tensor(161.5633)\n",
      "-285.7232486283 0.0 -285.7232486283 0.08756597541289161 tensor(162.3246)\n",
      "-285.4911551790798 0.0 -285.4911551790798 0.26313651422099804 tensor(159.4343)\n",
      "-285.9975754643381 0.0 -285.9975754643381 0.11244521943024492 tensor(143.8802)\n",
      "-286.167799137468 0.0 -286.167799137468 0.13057303432118428 tensor(193.3245)\n",
      "-285.9101314902404 0.0 -285.9101314902404 0.16758019302611932 tensor(179.1590)\n",
      "-286.02838524043216 0.0 -286.02838524043216 0.10425423095816423 tensor(175.1772)\n",
      "-285.2856559015512 0.0 -285.2856559015512 0.2417775563287509 tensor(178.5364)\n",
      "-285.55992223407884 0.0 -285.55992223407884 0.16093645744025906 tensor(174.1965)\n",
      "-285.5090475273259 0.0 -285.5090475273259 0.19199648176177359 tensor(165.6718)\n",
      "-285.5643496720979 0.0 -285.5643496720979 0.025165735053620773 tensor(149.2694)\n",
      "-285.6594626697932 0.0 -285.6594626697932 0.23745579630994743 tensor(171.5936)\n",
      "-285.97686956721475 0.0 -285.97686956721475 0.23899631519822898 tensor(171.9225)\n",
      "-285.78032656022407 0.0 -285.78032656022407 0.18065905708737143 tensor(164.8595)\n",
      "-286.1399537964311 0.0 -286.1399537964311 0.28431228718302154 tensor(149.7332)\n",
      "-285.0679787753785 0.0 -285.0679787753785 0.06653915860297331 tensor(189.3691)\n",
      "-285.95221940619604 0.0 -285.95221940619604 0.16713016144301554 tensor(147.3088)\n",
      "-285.102728405786 0.0 -285.102728405786 0.16943164752548928 tensor(154.0403)\n",
      "-285.9449710904536 0.0 -285.9449710904536 0.19058660901902102 tensor(156.4954)\n",
      "-285.33594366726646 0.0 -285.33594366726646 0.05919862536077018 tensor(147.4999)\n",
      "-285.8493522440596 0.0 -285.8493522440596 0.029165421390852556 tensor(172.8778)\n",
      "-285.6722934403898 0.0 -285.6722934403898 0.07606397676175877 tensor(149.7635)\n",
      "-286.04833985253185 0.0 -286.04833985253185 0.1684493414353128 tensor(143.6541)\n",
      "-285.3165836052095 0.0 -285.3165836052095 0.15058063873564667 tensor(184.5201)\n",
      "-286.04855617132927 0.0 -286.04855617132927 0.1369129194398237 tensor(161.3928)\n",
      "-285.85490224589705 0.0 -285.85490224589705 0.14551214859478198 tensor(159.7537)\n",
      "-286.0995853766601 0.0 -286.0995853766601 0.06316112282929096 tensor(168.0187)\n",
      "-285.54512764712405 0.0 -285.54512764712405 0.20625890614216713 tensor(178.3941)\n",
      "-285.8807443244534 0.0 -285.8807443244534 0.09429638143331438 tensor(187.6910)\n",
      "-285.3657718193308 0.0 -285.3657718193308 0.13484266850204138 tensor(160.1588)\n",
      "-285.55937227388847 0.0 -285.55937227388847 0.1751961648359966 tensor(162.6509)\n",
      "-285.5763420966449 0.0 -285.5763420966449 0.18018996038568258 tensor(147.5945)\n",
      "-285.70283333807157 0.0 -285.70283333807157 0.17589751229650613 tensor(148.2741)\n",
      "== Era 3 | Epoch 0 metrics ==\n",
      "\tloss -285.489\n",
      "\tforce 0\n",
      "\tdkl -285.489\n",
      "\tlogp 83.2097\n",
      "\tlogq -202.279\n",
      "\tess 0.12966\n",
      "-285.7668026517827 0.0 -285.7668026517827 0.10178840738929257 tensor(150.5435)\n",
      "-285.4680570493844 0.0 -285.4680570493844 0.057700858076692486 tensor(171.2564)\n",
      "-285.43206279194635 0.0 -285.43206279194635 0.1302547484754297 tensor(177.0882)\n",
      "-285.9759109310212 0.0 -285.9759109310212 0.15499422698246493 tensor(146.2332)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-285.2153668369717 0.0 -285.2153668369717 0.2189449581050638 tensor(171.8966)\n",
      "-285.48660306425217 0.0 -285.48660306425217 0.11321807829975229 tensor(187.9403)\n",
      "-285.5833949046988 0.0 -285.5833949046988 0.0637315382550267 tensor(158.9213)\n",
      "-285.48727051687985 0.0 -285.48727051687985 0.0839953258493482 tensor(182.5070)\n",
      "-285.54272284288584 0.0 -285.54272284288584 0.049358031771688855 tensor(175.0742)\n",
      "-285.1236794662193 0.0 -285.1236794662193 0.17802011448794844 tensor(176.3286)\n",
      "-285.8453044992894 0.0 -285.8453044992894 0.1956267839023526 tensor(155.9885)\n",
      "-286.1826945515369 0.0 -286.1826945515369 0.20304134682897104 tensor(165.7703)\n",
      "-285.6863122326545 0.0 -285.6863122326545 0.2116240683831535 tensor(173.1717)\n",
      "-285.771364346384 0.0 -285.771364346384 0.23513026087984623 tensor(168.5351)\n",
      "-285.317134575164 0.0 -285.317134575164 0.09205607013542556 tensor(185.0386)\n",
      "-285.3653766845782 0.0 -285.3653766845782 0.07107531753232464 tensor(166.2538)\n",
      "-285.29057236143143 0.0 -285.29057236143143 0.15304830221564364 tensor(146.5159)\n",
      "-286.194993757234 0.0 -286.194993757234 0.09924179134499941 tensor(140.5681)\n",
      "-285.59927213349647 0.0 -285.59927213349647 0.138491585194817 tensor(143.8572)\n",
      "-285.59891436134194 0.0 -285.59891436134194 0.06521984104092111 tensor(156.7595)\n",
      "-285.4790959430529 0.0 -285.4790959430529 0.0893927053917242 tensor(161.1441)\n",
      "-285.79073031763255 0.0 -285.79073031763255 0.23053553118396541 tensor(159.3303)\n",
      "-285.825169582547 0.0 -285.825169582547 0.14869992292426606 tensor(160.4531)\n",
      "-285.960931029814 0.0 -285.960931029814 0.04379959138608634 tensor(168.2859)\n",
      "-286.41944446739956 0.0 -286.41944446739956 0.20034135140201728 tensor(164.1387)\n",
      "-285.94514798346086 0.0 -285.94514798346086 0.13872383396379284 tensor(175.2018)\n",
      "-286.3225116826421 0.0 -286.3225116826421 0.03848087795000333 tensor(153.5619)\n",
      "-285.42136722124803 0.0 -285.42136722124803 0.17850418260058928 tensor(205.6805)\n",
      "-285.7594490444576 0.0 -285.7594490444576 0.05138424468062407 tensor(169.3042)\n",
      "-286.03765187765583 0.0 -286.03765187765583 0.026644427522127943 tensor(171.5180)\n",
      "-285.893226269323 0.0 -285.893226269323 0.17379707100365932 tensor(171.0951)\n",
      "-285.5779945169894 0.0 -285.5779945169894 0.22343545947785573 tensor(191.7971)\n",
      "-285.9783164167696 0.0 -285.9783164167696 0.12836732978514007 tensor(160.6582)\n",
      "-285.976227651725 0.0 -285.976227651725 0.13150862877771724 tensor(150.0928)\n",
      "-285.47954380100407 0.0 -285.47954380100407 0.1372598823553014 tensor(149.2705)\n",
      "-285.7372194510652 0.0 -285.7372194510652 0.22963611398454944 tensor(176.6230)\n",
      "-285.7004468420616 0.0 -285.7004468420616 0.18084951748839945 tensor(157.5890)\n",
      "-285.8348683267615 0.0 -285.8348683267615 0.04183876779618971 tensor(174.3954)\n",
      "-285.9213032885973 0.0 -285.9213032885973 0.10114782101158613 tensor(154.7946)\n",
      "-286.07203455042077 0.0 -286.07203455042077 0.024293322666988148 tensor(191.2808)\n",
      "-285.75147052969476 0.0 -285.75147052969476 0.24202431801110105 tensor(177.6644)\n",
      "-285.8471073049055 0.0 -285.8471073049055 0.20185830771962382 tensor(164.8439)\n",
      "-286.53460251288374 0.0 -286.53460251288374 0.1417978584258482 tensor(167.3637)\n",
      "-286.2659464288586 0.0 -286.2659464288586 0.24229393931184007 tensor(155.5533)\n",
      "-286.4695441249701 0.0 -286.4695441249701 0.16740404884957835 tensor(157.0791)\n",
      "-286.1955829134787 0.0 -286.1955829134787 0.12412521575576381 tensor(166.4006)\n",
      "-286.0118338041941 0.0 -286.0118338041941 0.13708992634026532 tensor(203.2334)\n",
      "-285.8520907490885 0.0 -285.8520907490885 0.1355173860068636 tensor(166.1003)\n",
      "-285.29339214728486 0.0 -285.29339214728486 0.236216330869415 tensor(181.9316)\n",
      "-285.50445627492047 0.0 -285.50445627492047 0.08912614334850243 tensor(202.8846)\n",
      "-285.9996268283801 0.0 -285.9996268283801 0.12103188366229659 tensor(185.2486)\n",
      "-286.33014126016315 0.0 -286.33014126016315 0.1345635379587717 tensor(170.2457)\n",
      "-286.1475059152067 0.0 -286.1475059152067 0.03190149422646425 tensor(174.7110)\n",
      "-286.02588765326664 0.0 -286.02588765326664 0.05249145428280649 tensor(161.2350)\n",
      "-285.5365893689633 0.0 -285.5365893689633 0.25670770516902813 tensor(223.7720)\n",
      "-285.95947827717254 0.0 -285.95947827717254 0.15799818594200016 tensor(202.8918)\n",
      "-285.64003971468253 0.0 -285.64003971468253 0.2400711275459224 tensor(174.7470)\n",
      "-286.44553667868763 0.0 -286.44553667868763 0.08333277784466502 tensor(164.8188)\n",
      "-285.88780823095675 0.0 -285.88780823095675 0.160988712513073 tensor(185.6120)\n",
      "-285.82899198080185 0.0 -285.82899198080185 0.2417705114210651 tensor(154.5666)\n",
      "-286.33167448156263 0.0 -286.33167448156263 0.06472993476055898 tensor(173.8204)\n",
      "-285.9209899770033 0.0 -285.9209899770033 0.19738540597555748 tensor(163.4354)\n",
      "-286.0823241301759 0.0 -286.0823241301759 0.212177361085175 tensor(187.3811)\n",
      "-286.30812725754413 0.0 -286.30812725754413 0.18531590044809873 tensor(161.9840)\n",
      "-285.80295934298 0.0 -285.80295934298 0.08005205322248768 tensor(182.2111)\n",
      "-286.3197841969144 0.0 -286.3197841969144 0.08261502238094594 tensor(135.4652)\n",
      "-285.8738479639461 0.0 -285.8738479639461 0.1738107507580644 tensor(146.1151)\n",
      "-285.98768762089713 0.0 -285.98768762089713 0.12174536353855252 tensor(172.5823)\n",
      "-286.5597552168987 0.0 -286.5597552168987 0.21983654002952804 tensor(166.0589)\n",
      "-285.92202251612144 0.0 -285.92202251612144 0.18766772418424385 tensor(167.5606)\n",
      "-286.07659879897176 0.0 -286.07659879897176 0.18850989608396096 tensor(181.0600)\n",
      "-286.1543955642345 0.0 -286.1543955642345 0.09019733089245727 tensor(152.5548)\n",
      "-285.7425184634802 0.0 -285.7425184634802 0.26456792411775604 tensor(155.5382)\n",
      "-285.97452982100253 0.0 -285.97452982100253 0.038548553330455274 tensor(182.2066)\n",
      "-285.44268101964155 0.0 -285.44268101964155 0.09961071888928177 tensor(205.5914)\n",
      "-286.4502022307531 0.0 -286.4502022307531 0.12253955978200791 tensor(209.6046)\n",
      "-285.8741550638284 0.0 -285.8741550638284 0.18067527794613053 tensor(176.3223)\n",
      "-285.633172957895 0.0 -285.633172957895 0.10505634224945798 tensor(170.0886)\n",
      "-286.2982599840824 0.0 -286.2982599840824 0.11071059692256931 tensor(173.9313)\n",
      "-286.3767390941001 0.0 -286.3767390941001 0.29578966140107604 tensor(154.7378)\n",
      "-285.8748773295963 0.0 -285.8748773295963 0.15075267960759203 tensor(199.8584)\n",
      "-286.5401057141869 0.0 -286.5401057141869 0.17198996883377932 tensor(163.6161)\n",
      "-285.68995636126783 0.0 -285.68995636126783 0.13228359304585086 tensor(173.1019)\n",
      "-285.6192443435153 0.0 -285.6192443435153 0.23296359856791202 tensor(155.4756)\n",
      "-286.1689216052405 0.0 -286.1689216052405 0.06991240478145914 tensor(187.4093)\n",
      "-286.469665153297 0.0 -286.469665153297 0.27151431023116446 tensor(165.6034)\n",
      "-286.2173028679772 0.0 -286.2173028679772 0.16109965486904093 tensor(140.0564)\n",
      "-286.5079164410247 0.0 -286.5079164410247 0.17137915789110197 tensor(193.0755)\n",
      "-286.36780802193186 0.0 -286.36780802193186 0.1587564325065711 tensor(164.9792)\n",
      "-285.82416104775854 0.0 -285.82416104775854 0.11775348729400831 tensor(175.3829)\n",
      "-286.1667406572813 0.0 -286.1667406572813 0.020939657942269114 tensor(173.2131)\n",
      "-286.3545227268159 0.0 -286.3545227268159 0.17067485404761334 tensor(167.2548)\n",
      "-285.95632021929134 0.0 -285.95632021929134 0.22817950998027725 tensor(185.0857)\n",
      "-286.09767824203186 0.0 -286.09767824203186 0.27414884142177987 tensor(172.3105)\n",
      "-285.9022473386044 0.0 -285.9022473386044 0.18135584811500857 tensor(185.0857)\n",
      "-286.5479033295619 0.0 -286.5479033295619 0.18590415915280464 tensor(158.8732)\n",
      "-286.4339286364049 0.0 -286.4339286364049 0.0878695450025577 tensor(152.5512)\n",
      "-285.872264321486 0.0 -285.872264321486 0.09119058925626776 tensor(194.9570)\n",
      "-286.2734946360099 0.0 -286.2734946360099 0.12517089549031316 tensor(177.3496)\n",
      "-285.7866118709935 0.0 -285.7866118709935 0.10672450011165611 tensor(189.8093)\n",
      "== Era 4 | Epoch 0 metrics ==\n",
      "\tloss -285.924\n",
      "\tforce 0\n",
      "\tdkl -285.924\n",
      "\tlogp 84.217\n",
      "\tlogq -201.707\n",
      "\tess 0.144656\n",
      "-286.42959024970844 0.0 -286.42959024970844 0.2435104437590573 tensor(170.9347)\n",
      "-286.07403823706034 0.0 -286.07403823706034 0.1273267669609673 tensor(180.7658)\n",
      "-286.1718182842367 0.0 -286.1718182842367 0.11671141404648318 tensor(164.3912)\n",
      "-286.3227836646726 0.0 -286.3227836646726 0.06557855207649757 tensor(159.9655)\n",
      "-286.2990792959473 0.0 -286.2990792959473 0.24660392220896196 tensor(178.4996)\n",
      "-286.7034507485218 0.0 -286.7034507485218 0.13025084497708797 tensor(180.4461)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.22982986846034 0.0 -286.22982986846034 0.20246256551850836 tensor(171.2551)\n",
      "-286.1820292900853 0.0 -286.1820292900853 0.15544772488880687 tensor(171.7185)\n",
      "-286.3019592642024 0.0 -286.3019592642024 0.2082460121477486 tensor(164.9257)\n",
      "-286.8255649181763 0.0 -286.8255649181763 0.29026847375336307 tensor(173.1734)\n",
      "-286.4177581006928 0.0 -286.4177581006928 0.1026103578595871 tensor(182.5192)\n",
      "-286.35691993734576 0.0 -286.35691993734576 0.05777089000652167 tensor(164.2266)\n",
      "-286.1784712944655 0.0 -286.1784712944655 0.09900306937663494 tensor(172.7846)\n",
      "-286.43381642512065 0.0 -286.43381642512065 0.09164166897145622 tensor(175.3758)\n",
      "-286.512558083341 0.0 -286.512558083341 0.08394243379631174 tensor(184.6040)\n",
      "-286.49338604286334 0.0 -286.49338604286334 0.1519100506888046 tensor(171.0245)\n",
      "-286.11971537533964 0.0 -286.11971537533964 0.09586047127971038 tensor(221.3763)\n",
      "-286.27289834545763 0.0 -286.27289834545763 0.14550963010592424 tensor(202.0609)\n",
      "-286.6831738102135 0.0 -286.6831738102135 0.12920912774986554 tensor(201.8453)\n",
      "-286.3946246644635 0.0 -286.3946246644635 0.1945971212525807 tensor(164.9536)\n",
      "-286.36048745359153 0.0 -286.36048745359153 0.1296597048496922 tensor(178.8615)\n",
      "-286.3670568591506 0.0 -286.3670568591506 0.2742197354199863 tensor(155.8174)\n",
      "-286.2046256838675 0.0 -286.2046256838675 0.2248518309982485 tensor(185.4790)\n",
      "-286.1283679176645 0.0 -286.1283679176645 0.08275124585751469 tensor(281.1231)\n",
      "-286.61171097380117 0.0 -286.61171097380117 0.10207747123691811 tensor(173.9787)\n",
      "-286.2626055690123 0.0 -286.2626055690123 0.07285659243378868 tensor(222.5802)\n",
      "-286.13390836780866 0.0 -286.13390836780866 0.10001094752428992 tensor(147.5014)\n",
      "-286.3440629633407 0.0 -286.3440629633407 0.2655427345021246 tensor(170.0566)\n",
      "-286.3701797320317 0.0 -286.3701797320317 0.1364192302698296 tensor(191.7171)\n",
      "-286.34998636419596 0.0 -286.34998636419596 0.30428175089309706 tensor(177.0218)\n",
      "-286.43223628244226 0.0 -286.43223628244226 0.14243097979147445 tensor(172.3253)\n",
      "-286.12028355920785 0.0 -286.12028355920785 0.2492646938615191 tensor(158.2405)\n",
      "-286.13506804534285 0.0 -286.13506804534285 0.18106336223466216 tensor(209.8453)\n",
      "-286.0834476451128 0.0 -286.0834476451128 0.19807097946918664 tensor(193.3350)\n",
      "-286.4808490817827 0.0 -286.4808490817827 0.20646733192527633 tensor(181.6948)\n",
      "-285.84325815515854 0.0 -285.84325815515854 0.22920793710165238 tensor(179.3602)\n",
      "-286.76837619528254 0.0 -286.76837619528254 0.05899978045586499 tensor(163.0777)\n",
      "-286.0108866845277 0.0 -286.0108866845277 0.16985894635801896 tensor(192.0954)\n",
      "-286.2966274324266 0.0 -286.2966274324266 0.16332491835380553 tensor(166.4388)\n",
      "-286.17721796619435 0.0 -286.17721796619435 0.17502150856502138 tensor(187.4265)\n",
      "-286.27578239643526 0.0 -286.27578239643526 0.16013411326142873 tensor(161.2580)\n",
      "-286.38526643810667 0.0 -286.38526643810667 0.29068029604160767 tensor(205.7795)\n",
      "-286.7289083150833 0.0 -286.7289083150833 0.24168238666206948 tensor(217.0607)\n",
      "-286.2096723499478 0.0 -286.2096723499478 0.18256483201334256 tensor(163.0453)\n",
      "-286.3417710899293 0.0 -286.3417710899293 0.16533312798822253 tensor(151.1778)\n",
      "-286.30692503767557 0.0 -286.30692503767557 0.09986431235389724 tensor(143.4916)\n",
      "-286.16368281278216 0.0 -286.16368281278216 0.0821569899603584 tensor(176.1274)\n",
      "-286.4737532601988 0.0 -286.4737532601988 0.18105489219796997 tensor(159.3108)\n",
      "-286.71136545753876 0.0 -286.71136545753876 0.17136245123007207 tensor(228.7653)\n",
      "-286.49124067745095 0.0 -286.49124067745095 0.13058832109327215 tensor(180.9383)\n",
      "-286.3822576660275 0.0 -286.3822576660275 0.18351742046138486 tensor(189.0697)\n",
      "-286.41512519173546 0.0 -286.41512519173546 0.15125805002791445 tensor(163.2630)\n",
      "-286.76191518248237 0.0 -286.76191518248237 0.27399422488107017 tensor(158.4886)\n",
      "-286.5277788489549 0.0 -286.5277788489549 0.1833123083927852 tensor(174.5121)\n",
      "-285.9495677562029 0.0 -285.9495677562029 0.07384014198375587 tensor(181.3682)\n",
      "-286.3835881863734 0.0 -286.3835881863734 0.10113706071757474 tensor(158.0260)\n",
      "-286.19524658000785 0.0 -286.19524658000785 0.1288375998939914 tensor(176.9169)\n",
      "-286.6455236473358 0.0 -286.6455236473358 0.30285692706854717 tensor(184.4518)\n",
      "-286.72693652522514 0.0 -286.72693652522514 0.035749128891141006 tensor(157.6688)\n",
      "-286.7663803602377 0.0 -286.7663803602377 0.2460087513627044 tensor(155.9466)\n",
      "-286.0743490017036 0.0 -286.0743490017036 0.2224735843640062 tensor(185.2419)\n",
      "-286.7687555973049 0.0 -286.7687555973049 0.197075551644442 tensor(163.4817)\n",
      "-286.4833996630447 0.0 -286.4833996630447 0.12648303910217312 tensor(153.5934)\n",
      "-286.04819027750494 0.0 -286.04819027750494 0.14572587049268892 tensor(171.2230)\n",
      "-286.5048777915239 0.0 -286.5048777915239 0.1690994649499268 tensor(155.9793)\n",
      "-286.40930767600196 0.0 -286.40930767600196 0.04236132174867439 tensor(184.4347)\n",
      "-286.45500869842516 0.0 -286.45500869842516 0.21585988580926585 tensor(164.3824)\n",
      "-286.24318174240955 0.0 -286.24318174240955 0.21595584844151292 tensor(190.5562)\n",
      "-286.51721846333095 0.0 -286.51721846333095 0.13239597991007526 tensor(172.8468)\n",
      "-285.9467410978414 0.0 -285.9467410978414 0.14854029343831074 tensor(169.3668)\n",
      "-286.7659284988713 0.0 -286.7659284988713 0.13750762183700507 tensor(166.9726)\n",
      "-286.2570264485332 0.0 -286.2570264485332 0.15855186052324555 tensor(182.2817)\n",
      "-286.53131415743565 0.0 -286.53131415743565 0.14406973651769028 tensor(179.6070)\n",
      "-286.4876605485586 0.0 -286.4876605485586 0.20212125909639336 tensor(162.7002)\n",
      "-286.2840868469028 0.0 -286.2840868469028 0.24052119920353118 tensor(152.6455)\n",
      "-286.2800081722962 0.0 -286.2800081722962 0.18777512806316857 tensor(185.0891)\n",
      "-286.51070299088985 0.0 -286.51070299088985 0.15455932954528379 tensor(230.3852)\n",
      "-286.67119881979363 0.0 -286.67119881979363 0.14159019665891065 tensor(166.9662)\n",
      "-286.5364960771438 0.0 -286.5364960771438 0.13383364923500607 tensor(247.8737)\n",
      "-286.1618468575757 0.0 -286.1618468575757 0.16904615302769627 tensor(189.1767)\n",
      "-286.56434913034343 0.0 -286.56434913034343 0.21586749499571256 tensor(186.4219)\n",
      "-286.3273430443526 0.0 -286.3273430443526 0.2720651700622085 tensor(197.2730)\n",
      "-286.36938716925545 0.0 -286.36938716925545 0.27864863899084424 tensor(194.3047)\n",
      "-286.14769982957597 0.0 -286.14769982957597 0.30468252570185755 tensor(173.2453)\n",
      "-286.4963324040551 0.0 -286.4963324040551 0.24442785279539864 tensor(135.9470)\n",
      "-286.67331473333854 0.0 -286.67331473333854 0.2715268282273839 tensor(184.6945)\n",
      "-286.42480446057107 0.0 -286.42480446057107 0.26856642173740913 tensor(176.1258)\n",
      "-286.63052127737626 0.0 -286.63052127737626 0.13970776219634068 tensor(162.9548)\n",
      "-286.2573226238018 0.0 -286.2573226238018 0.20868512705234565 tensor(165.9216)\n",
      "-286.6079943157121 0.0 -286.6079943157121 0.1130114257712197 tensor(154.4492)\n",
      "-285.689110352953 0.0 -285.689110352953 0.3069080186637527 tensor(206.1992)\n",
      "-286.34235687713687 0.0 -286.34235687713687 0.09849810110271283 tensor(158.1010)\n",
      "-286.3373000227845 0.0 -286.3373000227845 0.3317959084088003 tensor(208.6203)\n",
      "-286.06390505558215 0.0 -286.06390505558215 0.056358300984339166 tensor(187.8571)\n",
      "-286.51840885598136 0.0 -286.51840885598136 0.1530772473158128 tensor(178.8530)\n",
      "-286.46229994266037 0.0 -286.46229994266037 0.2959265342059735 tensor(170.6431)\n",
      "-286.8128822085714 0.0 -286.8128822085714 0.2625704190600825 tensor(177.0776)\n",
      "-286.40073561977385 0.0 -286.40073561977385 0.18434493471122693 tensor(163.2859)\n",
      "-286.58865676641807 0.0 -286.58865676641807 0.29876959235610673 tensor(172.5751)\n",
      "-286.74933526563393 0.0 -286.74933526563393 0.23340131117841 tensor(171.6258)\n",
      "== Era 5 | Epoch 0 metrics ==\n",
      "\tloss -286.381\n",
      "\tforce 0\n",
      "\tdkl -286.381\n",
      "\tlogp 85.1085\n",
      "\tlogq -201.273\n",
      "\tess 0.176152\n",
      "-286.46947283478397 0.0 -286.46947283478397 0.06487870408450072 tensor(233.4533)\n",
      "-286.5149657154835 0.0 -286.5149657154835 0.2956836379836378 tensor(194.0043)\n",
      "-286.185682779325 0.0 -286.185682779325 0.1226851204764522 tensor(184.5077)\n",
      "-286.28570242519675 0.0 -286.28570242519675 0.10894453087252014 tensor(209.0626)\n",
      "-287.0014646467158 0.0 -287.0014646467158 0.1451389266846456 tensor(238.8169)\n",
      "-286.3977406892347 0.0 -286.3977406892347 0.1537423283263161 tensor(191.8957)\n",
      "-286.820086834373 0.0 -286.820086834373 0.11441787360541553 tensor(222.3287)\n",
      "-286.92992908275863 0.0 -286.92992908275863 0.06636762698905235 tensor(167.1097)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.5381218721758 0.0 -286.5381218721758 0.21287292229146673 tensor(259.0772)\n",
      "-286.10304876748773 0.0 -286.10304876748773 0.14757232188692027 tensor(177.4414)\n",
      "-286.7988142692842 0.0 -286.7988142692842 0.1992576221148997 tensor(175.7745)\n",
      "-286.88955565189383 0.0 -286.88955565189383 0.2066341960387955 tensor(212.8828)\n",
      "-286.73436938298346 0.0 -286.73436938298346 0.23961185995327375 tensor(198.8368)\n",
      "-286.3219745816996 0.0 -286.3219745816996 0.14143779851890073 tensor(222.2222)\n",
      "-286.25043914365307 0.0 -286.25043914365307 0.14604459521002403 tensor(155.7067)\n",
      "-286.80813812640616 0.0 -286.80813812640616 0.25228464373634757 tensor(207.6366)\n",
      "-286.7118538872136 0.0 -286.7118538872136 0.10008081764022479 tensor(187.6564)\n",
      "-286.7248982664024 0.0 -286.7248982664024 0.24895956847001513 tensor(175.3379)\n",
      "-286.2574371719129 0.0 -286.2574371719129 0.2283220077260529 tensor(182.6109)\n",
      "-286.70812550199224 0.0 -286.70812550199224 0.12354206328072535 tensor(155.5527)\n",
      "-287.0242918548703 0.0 -287.0242918548703 0.20749112974829265 tensor(183.9654)\n",
      "-286.7605332923482 0.0 -286.7605332923482 0.18745167358644163 tensor(158.8124)\n",
      "-286.17046882460295 0.0 -286.17046882460295 0.3208952781519172 tensor(226.3543)\n",
      "-286.58821270302235 0.0 -286.58821270302235 0.10410378391791099 tensor(154.4433)\n",
      "-286.7117242655123 0.0 -286.7117242655123 0.05887054200908576 tensor(197.1583)\n",
      "-287.01808510865817 0.0 -287.01808510865817 0.14303277386919724 tensor(183.5023)\n",
      "-286.69876849647267 0.0 -286.69876849647267 0.061287258219878335 tensor(166.1700)\n",
      "-286.48528612817836 0.0 -286.48528612817836 0.08404510499797213 tensor(206.7670)\n",
      "-286.8280337964841 0.0 -286.8280337964841 0.10290575137723139 tensor(221.7487)\n",
      "-286.68425852082237 0.0 -286.68425852082237 0.21024849878883797 tensor(149.8141)\n",
      "-286.27803629628187 0.0 -286.27803629628187 0.17444551160589625 tensor(174.8035)\n",
      "-286.5572779699945 0.0 -286.5572779699945 0.38354634124612397 tensor(179.0352)\n",
      "-286.64753322560546 0.0 -286.64753322560546 0.1878943633698194 tensor(170.0627)\n",
      "-287.26555914349376 0.0 -287.26555914349376 0.11634278278320856 tensor(181.3099)\n",
      "-286.5768805473424 0.0 -286.5768805473424 0.16344975454102695 tensor(153.4133)\n",
      "-286.528451201987 0.0 -286.528451201987 0.28615729452443656 tensor(169.6821)\n",
      "-286.4155612380195 0.0 -286.4155612380195 0.2120927417751911 tensor(176.9616)\n",
      "-286.7876625779283 0.0 -286.7876625779283 0.22988717525791416 tensor(200.1625)\n",
      "-286.51526179356927 0.0 -286.51526179356927 0.19018519515002047 tensor(176.7538)\n",
      "-286.3527133986641 0.0 -286.3527133986641 0.0817979253262581 tensor(184.3849)\n",
      "-286.92164274375153 0.0 -286.92164274375153 0.24815558193078593 tensor(177.7186)\n",
      "-286.5355535742691 0.0 -286.5355535742691 0.15126096576322676 tensor(220.5007)\n",
      "-286.0439491238865 0.0 -286.0439491238865 0.291767788800703 tensor(209.4176)\n",
      "-286.3549369299899 0.0 -286.3549369299899 0.2086820494851692 tensor(243.2676)\n",
      "-286.33183494451697 0.0 -286.33183494451697 0.15151229917746997 tensor(192.7315)\n",
      "-286.56805329266837 0.0 -286.56805329266837 0.3048053057314402 tensor(156.6109)\n",
      "-286.5783845815805 0.0 -286.5783845815805 0.17918392298833516 tensor(165.7022)\n",
      "-286.66870635980916 0.0 -286.66870635980916 0.3255808396180807 tensor(267.9098)\n",
      "-286.65636602478617 0.0 -286.65636602478617 0.11479358468395304 tensor(187.0022)\n",
      "-286.6149532887308 0.0 -286.6149532887308 0.17581434090675133 tensor(239.4020)\n",
      "-286.3797728264898 0.0 -286.3797728264898 0.23107990452295146 tensor(167.0402)\n",
      "-286.72540306162927 0.0 -286.72540306162927 0.15484462945230862 tensor(230.5324)\n",
      "-286.2337514482 0.0 -286.2337514482 0.061624929565387274 tensor(200.0953)\n",
      "-286.2878414092004 0.0 -286.2878414092004 0.18309349412919604 tensor(176.0887)\n",
      "-286.32500640660203 0.0 -286.32500640660203 0.1816310008425154 tensor(195.9829)\n",
      "-286.3737273666747 0.0 -286.3737273666747 0.16577210497858694 tensor(252.9528)\n",
      "-286.6722872977528 0.0 -286.6722872977528 0.30953741702507104 tensor(243.4074)\n",
      "-286.19620047312605 0.0 -286.19620047312605 0.1829637804200159 tensor(172.7544)\n",
      "-286.7697668144259 0.0 -286.7697668144259 0.15133248619676795 tensor(189.4724)\n",
      "-286.5913254034989 0.0 -286.5913254034989 0.23917472892206376 tensor(200.3641)\n",
      "-286.81713171002866 0.0 -286.81713171002866 0.17891717648986274 tensor(197.6193)\n",
      "-286.14476542185207 0.0 -286.14476542185207 0.30480665454188927 tensor(180.5477)\n",
      "-286.26811197084584 0.0 -286.26811197084584 0.23083953020755682 tensor(220.7432)\n",
      "-286.6228704485679 0.0 -286.6228704485679 0.21782239211377086 tensor(324.4716)\n",
      "-286.51989652954586 0.0 -286.51989652954586 0.1853496126572922 tensor(182.7521)\n",
      "-286.8695591128535 0.0 -286.8695591128535 0.02682629932061869 tensor(153.9097)\n",
      "-286.88992831703524 0.0 -286.88992831703524 0.2975787615478732 tensor(183.4833)\n",
      "-286.4178567244618 0.0 -286.4178567244618 0.09804426502927306 tensor(154.8524)\n",
      "-286.82715687404686 0.0 -286.82715687404686 0.10369779242236625 tensor(155.6991)\n",
      "-286.61711989004937 0.0 -286.61711989004937 0.1316787105690755 tensor(186.0081)\n",
      "-286.8377967203512 0.0 -286.8377967203512 0.07999511130257816 tensor(177.0422)\n",
      "-286.41989369800206 0.0 -286.41989369800206 0.21035928650516222 tensor(195.7338)\n",
      "-286.7344751830499 0.0 -286.7344751830499 0.307299151700704 tensor(166.8233)\n",
      "-286.37719390511296 0.0 -286.37719390511296 0.16197827067566578 tensor(215.9955)\n",
      "-286.3667771868669 0.0 -286.3667771868669 0.21811714817292382 tensor(202.8112)\n",
      "-286.4024540376263 0.0 -286.4024540376263 0.1496566850368846 tensor(241.9607)\n",
      "-286.8842269922043 0.0 -286.8842269922043 0.22479366165082787 tensor(160.6115)\n",
      "-286.5886329235075 0.0 -286.5886329235075 0.2001819842049708 tensor(235.6666)\n",
      "-286.5917077870038 0.0 -286.5917077870038 0.31267365101223044 tensor(202.0222)\n",
      "-286.5990810877847 0.0 -286.5990810877847 0.3934638708192776 tensor(164.6092)\n",
      "-286.1774537968056 0.0 -286.1774537968056 0.10444206165339429 tensor(174.6839)\n",
      "-286.8953880752156 0.0 -286.8953880752156 0.0917593102816481 tensor(148.5917)\n",
      "-286.44408454340834 0.0 -286.44408454340834 0.24286855823547285 tensor(221.4481)\n",
      "-286.7113982268581 0.0 -286.7113982268581 0.16081735838974615 tensor(198.2297)\n",
      "-286.7460597379732 0.0 -286.7460597379732 0.17554052777130752 tensor(431.1631)\n",
      "-286.2740859501681 0.0 -286.2740859501681 0.11974278898633045 tensor(172.8775)\n",
      "-286.59090433600284 0.0 -286.59090433600284 0.11723801086780854 tensor(210.9138)\n",
      "-286.4042346947684 0.0 -286.4042346947684 0.18805597333042484 tensor(209.6004)\n",
      "-286.4571477177519 0.0 -286.4571477177519 0.21405447015488835 tensor(169.5390)\n",
      "-286.9068296494409 0.0 -286.9068296494409 0.2484213038175298 tensor(153.4132)\n",
      "-286.5380180922294 0.0 -286.5380180922294 0.21356970152134863 tensor(196.4168)\n",
      "-286.6485399609205 0.0 -286.6485399609205 0.15280591562152987 tensor(200.8963)\n",
      "-286.9072311318304 0.0 -286.9072311318304 0.12424519954920574 tensor(225.1027)\n",
      "-286.5708324896723 0.0 -286.5708324896723 0.33590132296281416 tensor(168.0175)\n",
      "-286.4510063890597 0.0 -286.4510063890597 0.07789495270804282 tensor(142.6255)\n",
      "-286.5796159990239 0.0 -286.5796159990239 0.1751332500486407 tensor(161.6243)\n",
      "-286.73110687811584 0.0 -286.73110687811584 0.1133908536120241 tensor(193.6215)\n",
      "-286.7395538562929 0.0 -286.7395538562929 0.2572407530257334 tensor(199.9445)\n",
      "-286.9150497859257 0.0 -286.9150497859257 0.24097412925193892 tensor(165.8817)\n",
      "-286.97680450634925 0.0 -286.97680450634925 0.3518596350042725 tensor(175.6196)\n",
      "== Era 6 | Epoch 0 metrics ==\n",
      "\tloss -286.586\n",
      "\tforce 0\n",
      "\tdkl -286.586\n",
      "\tlogp 85.4844\n",
      "\tlogq -201.102\n",
      "\tess 0.185773\n",
      "-286.21250074289674 0.0 -286.21250074289674 0.11219675106799569 tensor(198.8597)\n",
      "-286.7053756221329 0.0 -286.7053756221329 0.2723606748889234 tensor(226.5222)\n",
      "-286.756144764759 0.0 -286.756144764759 0.05604808531247425 tensor(182.7325)\n",
      "-286.65105319882707 0.0 -286.65105319882707 0.06796693557048168 tensor(222.1213)\n",
      "-286.8060090761999 0.0 -286.8060090761999 0.2206663879373694 tensor(205.1283)\n",
      "-286.83962092659806 0.0 -286.83962092659806 0.04996853583569918 tensor(330.5075)\n",
      "-286.5782347711478 0.0 -286.5782347711478 0.25769687717360595 tensor(173.5427)\n",
      "-286.43768491246107 0.0 -286.43768491246107 0.24694264558416856 tensor(202.9832)\n",
      "-285.9956995384972 0.0 -285.9956995384972 0.09332177161698726 tensor(196.0997)\n",
      "-286.8836865135844 0.0 -286.8836865135844 0.3210552380886799 tensor(172.0458)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.9252326632137 0.0 -286.9252326632137 0.11408531725165307 tensor(236.2626)\n",
      "-286.3742494605614 0.0 -286.3742494605614 0.2763150612122356 tensor(187.5516)\n",
      "-286.8776457983935 0.0 -286.8776457983935 0.3356031422660996 tensor(196.6000)\n",
      "-286.6968894792275 0.0 -286.6968894792275 0.18031224314753178 tensor(187.4603)\n",
      "-286.52879833852296 0.0 -286.52879833852296 0.20499660196628036 tensor(196.2932)\n",
      "-286.23646035534557 0.0 -286.23646035534557 0.11616869645490181 tensor(197.9263)\n",
      "-286.1971812894324 0.0 -286.1971812894324 0.11281677575252079 tensor(253.9945)\n",
      "-287.03140355543803 0.0 -287.03140355543803 0.0809041436410346 tensor(195.7730)\n",
      "-286.3210987269473 0.0 -286.3210987269473 0.25428902571409967 tensor(172.7984)\n",
      "-286.5775680122872 0.0 -286.5775680122872 0.23256129857538327 tensor(189.0569)\n",
      "-286.53799951534137 0.0 -286.53799951534137 0.05962104168480847 tensor(224.7544)\n",
      "-286.8504827948418 0.0 -286.8504827948418 0.07016775816000485 tensor(265.4737)\n",
      "-286.8202145938601 0.0 -286.8202145938601 0.2534131466735155 tensor(286.0130)\n",
      "-286.68235540413457 0.0 -286.68235540413457 0.18071701961684755 tensor(179.6245)\n",
      "-286.3014863937958 0.0 -286.3014863937958 0.21088599204697925 tensor(207.0567)\n",
      "-286.61409212059243 0.0 -286.61409212059243 0.14040900828790967 tensor(298.1315)\n",
      "-286.8067406595739 0.0 -286.8067406595739 0.12410618701472197 tensor(165.5810)\n",
      "-286.7354118390407 0.0 -286.7354118390407 0.16866001829198304 tensor(194.4967)\n",
      "-286.63206448754374 0.0 -286.63206448754374 0.235318830128926 tensor(219.1720)\n",
      "-286.6515848124815 0.0 -286.6515848124815 0.2267386942048688 tensor(183.4203)\n",
      "-286.4291493627984 0.0 -286.4291493627984 0.32839669485415873 tensor(170.0604)\n",
      "-286.42086955878324 0.0 -286.42086955878324 0.2797013756652739 tensor(170.6312)\n",
      "-286.9402372050753 0.0 -286.9402372050753 0.2748197251237719 tensor(197.9817)\n",
      "-286.87461182105113 0.0 -286.87461182105113 0.3208635259105429 tensor(188.4059)\n",
      "-286.78406846664444 0.0 -286.78406846664444 0.15092138683588477 tensor(159.1613)\n",
      "-286.756315444136 0.0 -286.756315444136 0.09215714782080503 tensor(210.1781)\n",
      "-286.6500263408096 0.0 -286.6500263408096 0.2632455956180423 tensor(152.1128)\n",
      "-286.83029860633104 0.0 -286.83029860633104 0.26774521448781785 tensor(225.3009)\n",
      "-286.38730415242594 0.0 -286.38730415242594 0.10095316224644055 tensor(259.9837)\n",
      "-286.60137798049817 0.0 -286.60137798049817 0.2657036411952799 tensor(185.9333)\n",
      "-286.45470883827727 0.0 -286.45470883827727 0.3386978956910067 tensor(186.2503)\n",
      "-286.51877667301954 0.0 -286.51877667301954 0.15518273821162512 tensor(200.1456)\n",
      "-286.5872547449635 0.0 -286.5872547449635 0.14145274317204817 tensor(189.6405)\n",
      "-286.8781467475011 0.0 -286.8781467475011 0.4118391793415306 tensor(196.1774)\n",
      "-286.5791813063263 0.0 -286.5791813063263 0.21737555326541716 tensor(260.5413)\n",
      "-286.627831446948 0.0 -286.627831446948 0.10990700056220448 tensor(162.8724)\n",
      "-286.53153142361197 0.0 -286.53153142361197 0.2652144837016467 tensor(192.0943)\n",
      "-286.7444897963775 0.0 -286.7444897963775 0.18283686172165048 tensor(188.3585)\n",
      "-286.55970011286024 0.0 -286.55970011286024 0.19898363551706746 tensor(194.2815)\n",
      "-286.61158699820396 0.0 -286.61158699820396 0.3086281949188106 tensor(206.4952)\n",
      "-286.3609082685814 0.0 -286.3609082685814 0.16085723221315978 tensor(158.3240)\n",
      "-286.7881321524127 0.0 -286.7881321524127 0.13951503870646162 tensor(204.5644)\n",
      "-286.7217316861679 0.0 -286.7217316861679 0.19788629663144502 tensor(215.1940)\n",
      "-286.4439396883161 0.0 -286.4439396883161 0.06168883982498875 tensor(157.8482)\n",
      "-286.7503897187345 0.0 -286.7503897187345 0.3258263650273744 tensor(216.7405)\n",
      "-286.4095083012687 0.0 -286.4095083012687 0.24790741557810045 tensor(201.4458)\n",
      "-286.6494647290647 0.0 -286.6494647290647 0.28401977373526965 tensor(176.2450)\n",
      "-286.54652334521484 0.0 -286.54652334521484 0.34151184867249323 tensor(189.6893)\n",
      "-286.6230183745218 0.0 -286.6230183745218 0.12009439433736079 tensor(255.1479)\n",
      "-286.7756124124211 0.0 -286.7756124124211 0.3117109476720853 tensor(187.5632)\n",
      "-286.5347067521324 0.0 -286.5347067521324 0.2762521658733951 tensor(236.8665)\n",
      "-286.71414149420286 0.0 -286.71414149420286 0.2845596305895617 tensor(175.9632)\n",
      "-286.54389717981786 0.0 -286.54389717981786 0.1854766886384762 tensor(206.2547)\n",
      "-286.93453490118975 0.0 -286.93453490118975 0.37416251181547544 tensor(183.7529)\n",
      "-286.6980996892783 0.0 -286.6980996892783 0.224746593867675 tensor(209.6117)\n",
      "-286.842970249594 0.0 -286.842970249594 0.2714473126180552 tensor(217.8388)\n",
      "-286.53709158256083 0.0 -286.53709158256083 0.30852340726046257 tensor(189.3455)\n",
      "-286.68009597761557 0.0 -286.68009597761557 0.20119081024006327 tensor(234.4151)\n",
      "-286.4574003243224 0.0 -286.4574003243224 0.3373874935330884 tensor(227.4926)\n",
      "-286.2390865484954 0.0 -286.2390865484954 0.40978981578865303 tensor(213.2928)\n",
      "-286.5689614164353 0.0 -286.5689614164353 0.19376428847302865 tensor(231.6620)\n",
      "-286.5564410814783 0.0 -286.5564410814783 0.11978786513673892 tensor(273.8703)\n",
      "-286.8444376653642 0.0 -286.8444376653642 0.35133666924871826 tensor(197.0014)\n",
      "-286.6871328496332 0.0 -286.6871328496332 0.23436890102968738 tensor(176.7946)\n",
      "-286.7898727441849 0.0 -286.7898727441849 0.2947471308790622 tensor(186.0448)\n",
      "-286.83715540798784 0.0 -286.83715540798784 0.2404210734025936 tensor(219.5359)\n",
      "-287.18656065238133 0.0 -287.18656065238133 0.17751047775186818 tensor(172.1329)\n",
      "-286.92487357532895 0.0 -286.92487357532895 0.2629229151307473 tensor(180.7246)\n",
      "-286.4071738803388 0.0 -286.4071738803388 0.05419486687207998 tensor(197.8698)\n",
      "-286.51947823333745 0.0 -286.51947823333745 0.20676287443400743 tensor(231.6646)\n",
      "-286.67414903192775 0.0 -286.67414903192775 0.28565324239720663 tensor(201.4715)\n",
      "-286.9066570328433 0.0 -286.9066570328433 0.2452334423392447 tensor(153.4771)\n",
      "-286.82709988207876 0.0 -286.82709988207876 0.0831671392282232 tensor(186.9287)\n",
      "-286.5733282839417 0.0 -286.5733282839417 0.17449723676573647 tensor(195.1080)\n",
      "-286.4163486854488 0.0 -286.4163486854488 0.22170446294533638 tensor(221.7621)\n",
      "-286.88253007924556 0.0 -286.88253007924556 0.11498012090894308 tensor(228.2307)\n",
      "-286.6523904694591 0.0 -286.6523904694591 0.17297510061041463 tensor(217.2813)\n",
      "-286.5899766518521 0.0 -286.5899766518521 0.15763430453527436 tensor(175.4702)\n",
      "-286.64806819394073 0.0 -286.64806819394073 0.37677792287846024 tensor(184.9872)\n",
      "-286.5997528908208 0.0 -286.5997528908208 0.18625109373215745 tensor(181.8681)\n",
      "-287.12120341642947 0.0 -287.12120341642947 0.19181362922075984 tensor(190.5652)\n",
      "-286.6360214182665 0.0 -286.6360214182665 0.2306632897620593 tensor(217.6861)\n",
      "-286.948424409895 0.0 -286.948424409895 0.25614432210233956 tensor(302.1015)\n",
      "-286.63964966350557 0.0 -286.63964966350557 0.3253608599161074 tensor(193.6563)\n",
      "-286.62371783366075 0.0 -286.62371783366075 0.16697279424631123 tensor(197.4101)\n",
      "-287.2105777177943 0.0 -287.2105777177943 0.28192762871724236 tensor(149.8235)\n",
      "-286.9549879610956 0.0 -286.9549879610956 0.2761060378106207 tensor(221.6046)\n",
      "-286.8483772608349 0.0 -286.8483772608349 0.30224250226802213 tensor(189.3220)\n",
      "-286.9272202344747 0.0 -286.9272202344747 0.2419666757957286 tensor(232.8594)\n",
      "-287.11452605286865 0.0 -287.11452605286865 0.17531883974407692 tensor(220.5312)\n",
      "== Era 7 | Epoch 0 metrics ==\n",
      "\tloss -286.664\n",
      "\tforce 0\n",
      "\tdkl -286.664\n",
      "\tlogp 85.9265\n",
      "\tlogq -200.738\n",
      "\tess 0.216187\n",
      "-286.70511817569763 0.0 -286.70511817569763 0.235206203821088 tensor(237.1432)\n",
      "-286.47894328253915 0.0 -286.47894328253915 0.25367573964580464 tensor(270.8496)\n",
      "-286.7370119933059 0.0 -286.7370119933059 0.27305322773560653 tensor(174.3618)\n",
      "-286.8733819136273 0.0 -286.8733819136273 0.22467656925719143 tensor(175.4826)\n",
      "-287.2029188020907 0.0 -287.2029188020907 0.13370429836397943 tensor(188.1441)\n",
      "-286.7711611633572 0.0 -286.7711611633572 0.20046667655019312 tensor(192.3636)\n",
      "-286.5221163313606 0.0 -286.5221163313606 0.08701285402304143 tensor(189.5549)\n",
      "-286.96230494392205 0.0 -286.96230494392205 0.37584098934850807 tensor(201.9951)\n",
      "-286.84106931708413 0.0 -286.84106931708413 0.24721458710178368 tensor(230.7413)\n",
      "-286.60407730109165 0.0 -286.60407730109165 0.13268791741286776 tensor(253.0495)\n",
      "-286.67027317104373 0.0 -286.67027317104373 0.2517836235902267 tensor(188.4788)\n",
      "-286.7004491872383 0.0 -286.7004491872383 0.34720497721748117 tensor(223.4571)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.87126068495024 0.0 -286.87126068495024 0.4048365863916854 tensor(187.6384)\n",
      "-286.83001181783123 0.0 -286.83001181783123 0.2898011502067074 tensor(158.2594)\n",
      "-286.55234327009583 0.0 -286.55234327009583 0.22451706944142108 tensor(182.6398)\n",
      "-286.938263208946 0.0 -286.938263208946 0.14171217256452365 tensor(295.2348)\n",
      "-286.9683559039313 0.0 -286.9683559039313 0.1747586482555117 tensor(227.1569)\n",
      "-287.1036383266553 0.0 -287.1036383266553 0.32196595687691765 tensor(219.0140)\n",
      "-286.3360948410062 0.0 -286.3360948410062 0.24192432746154108 tensor(239.9115)\n",
      "-286.4305857291324 0.0 -286.4305857291324 0.26014798894447216 tensor(164.5104)\n",
      "-287.0470088480773 0.0 -287.0470088480773 0.10195253413701251 tensor(257.5622)\n",
      "-286.6679003369488 0.0 -286.6679003369488 0.23087317716387462 tensor(212.7784)\n",
      "-286.8494711464491 0.0 -286.8494711464491 0.18796580405368846 tensor(256.2153)\n",
      "-286.67045897516687 0.0 -286.67045897516687 0.3785439892198088 tensor(181.1894)\n",
      "-287.34418615396976 0.0 -287.34418615396976 0.3343095254705208 tensor(247.9759)\n",
      "-286.9732668697225 0.0 -286.9732668697225 0.0973118390517176 tensor(219.3603)\n",
      "-287.119275214515 0.0 -287.119275214515 0.03791928665620903 tensor(163.9360)\n",
      "-286.50372736861743 0.0 -286.50372736861743 0.08364865102820751 tensor(182.5176)\n",
      "-286.79656836273 0.0 -286.79656836273 0.15170699965205464 tensor(240.0174)\n",
      "-286.60231609881976 0.0 -286.60231609881976 0.2833484986092074 tensor(182.4003)\n",
      "-286.73628665277397 0.0 -286.73628665277397 0.27049547003774516 tensor(310.2971)\n",
      "-287.1404923768357 0.0 -287.1404923768357 0.322619872024959 tensor(169.4657)\n",
      "-286.85460053110876 0.0 -286.85460053110876 0.16198955138694904 tensor(204.6375)\n",
      "-286.78096914339574 0.0 -286.78096914339574 0.4331256295464379 tensor(232.3479)\n",
      "-286.82304219725063 0.0 -286.82304219725063 0.14443488969049095 tensor(210.4108)\n",
      "-286.86188510448295 0.0 -286.86188510448295 0.20936301711587038 tensor(210.4198)\n",
      "-286.8188121889407 0.0 -286.8188121889407 0.10525373318463445 tensor(223.7558)\n",
      "-286.6996259932575 0.0 -286.6996259932575 0.14408937181997195 tensor(205.3041)\n",
      "-286.6817007382723 0.0 -286.6817007382723 0.2765322811533805 tensor(226.5994)\n",
      "-286.3392779991256 0.0 -286.3392779991256 0.2969289097787772 tensor(205.4129)\n",
      "-286.3020447041739 0.0 -286.3020447041739 0.18305568213777096 tensor(207.9752)\n",
      "-286.9943082466387 0.0 -286.9943082466387 0.163503364188198 tensor(174.2430)\n",
      "-286.92136662201926 0.0 -286.92136662201926 0.20842801672662112 tensor(195.0334)\n",
      "-286.8349729637939 0.0 -286.8349729637939 0.16262355689498034 tensor(163.6767)\n",
      "-286.76505416432707 0.0 -286.76505416432707 0.34960502731141996 tensor(256.1561)\n",
      "-286.83740393472704 0.0 -286.83740393472704 0.20076265805308793 tensor(246.4429)\n",
      "-286.816333389058 0.0 -286.816333389058 0.07306552070374966 tensor(166.8848)\n",
      "-286.8819501072821 0.0 -286.8819501072821 0.28052615811067366 tensor(186.6790)\n",
      "-286.52448978856785 0.0 -286.52448978856785 0.27967355265267174 tensor(355.2635)\n",
      "-286.85265652561804 0.0 -286.85265652561804 0.29734074631386864 tensor(178.7593)\n",
      "-286.55213000705334 0.0 -286.55213000705334 0.2313954850280897 tensor(263.6768)\n",
      "-286.6788920876637 0.0 -286.6788920876637 0.3329359545128717 tensor(188.7649)\n",
      "-287.12466970102014 0.0 -287.12466970102014 0.2513398500091998 tensor(197.9285)\n",
      "-286.69645414182065 0.0 -286.69645414182065 0.17539783317316068 tensor(317.1178)\n",
      "-286.79943033330835 0.0 -286.79943033330835 0.25460756400103196 tensor(186.7689)\n",
      "-286.8197615197248 0.0 -286.8197615197248 0.26169365958528545 tensor(209.7898)\n",
      "-286.7246034720853 0.0 -286.7246034720853 0.3554868765946914 tensor(199.5650)\n",
      "-286.6687386763879 0.0 -286.6687386763879 0.39095449065676874 tensor(237.8008)\n",
      "-287.0871311175689 0.0 -287.0871311175689 0.23515541761907688 tensor(160.7898)\n",
      "-286.57519153182466 0.0 -286.57519153182466 0.2328065462152405 tensor(193.4631)\n",
      "-287.0233843644771 0.0 -287.0233843644771 0.24378209047908883 tensor(240.8661)\n",
      "-286.65463411952226 0.0 -286.65463411952226 0.33367191206373686 tensor(205.0807)\n",
      "-286.62638743970575 0.0 -286.62638743970575 0.3754348709871795 tensor(190.8175)\n",
      "-286.6258956633917 0.0 -286.6258956633917 0.2522173635129489 tensor(164.9317)\n",
      "-286.88836868863797 0.0 -286.88836868863797 0.19882646914668578 tensor(179.2045)\n",
      "-286.8301089030935 0.0 -286.8301089030935 0.15593359016123054 tensor(166.3240)\n",
      "-286.4875255560047 0.0 -286.4875255560047 0.0936545875432448 tensor(182.6316)\n",
      "-286.7683191102547 0.0 -286.7683191102547 0.16912172275690665 tensor(407.2162)\n",
      "-286.5689287906266 0.0 -286.5689287906266 0.2056326033062971 tensor(162.2423)\n",
      "-286.7926651337498 0.0 -286.7926651337498 0.15806247743016577 tensor(176.9347)\n",
      "-286.54250140953536 0.0 -286.54250140953536 0.17701755035759584 tensor(178.0152)\n",
      "-287.2865989595142 0.0 -287.2865989595142 0.24327354060986847 tensor(169.9933)\n",
      "-286.9381333639436 0.0 -286.9381333639436 0.320386503867536 tensor(244.5143)\n",
      "-286.87721624802765 0.0 -286.87721624802765 0.20314743913509137 tensor(221.2371)\n",
      "-287.17251500738985 0.0 -287.17251500738985 0.3237408312797797 tensor(217.2898)\n",
      "-286.6743594272872 0.0 -286.6743594272872 0.18974338532328414 tensor(230.5648)\n",
      "-287.0063191491509 0.0 -287.0063191491509 0.22567833532258083 tensor(241.3563)\n",
      "-287.0533305398427 0.0 -287.0533305398427 0.20168576025198842 tensor(237.8678)\n",
      "-286.9993881935234 0.0 -286.9993881935234 0.18828491031080197 tensor(212.1207)\n",
      "-286.8624604575666 0.0 -286.8624604575666 0.24618654398317177 tensor(227.1595)\n",
      "-286.9067085399918 0.0 -286.9067085399918 0.15972617818666948 tensor(186.7731)\n",
      "-286.7168348982988 0.0 -286.7168348982988 0.11552567897586379 tensor(235.7991)\n",
      "-286.7880447100829 0.0 -286.7880447100829 0.3109194791351115 tensor(222.4084)\n",
      "-287.2362909432778 0.0 -287.2362909432778 0.18557675692707237 tensor(217.8824)\n",
      "-286.45983593672076 0.0 -286.45983593672076 0.2663930065663018 tensor(176.5863)\n",
      "-287.0382961787885 0.0 -287.0382961787885 0.2710827545712561 tensor(188.4932)\n",
      "-286.51664393758637 0.0 -286.51664393758637 0.12064777426534297 tensor(167.1279)\n",
      "-286.8717829074894 0.0 -286.8717829074894 0.36183746866523286 tensor(255.5337)\n",
      "-286.68178852199685 0.0 -286.68178852199685 0.16378827316146582 tensor(255.3761)\n",
      "-286.88306624525984 0.0 -286.88306624525984 0.13246659997077023 tensor(195.6359)\n",
      "-287.0200782428304 0.0 -287.0200782428304 0.33335836905515775 tensor(270.5794)\n",
      "-286.97392894848747 0.0 -286.97392894848747 0.22934095951013211 tensor(162.6244)\n",
      "-286.63237652648445 0.0 -286.63237652648445 0.16108108644958227 tensor(193.5320)\n",
      "-286.9106781108348 0.0 -286.9106781108348 0.22450080281981596 tensor(178.7236)\n",
      "-287.09227458610906 0.0 -287.09227458610906 0.1738964002000169 tensor(191.9944)\n",
      "-287.0220654877123 0.0 -287.0220654877123 0.1755716900404395 tensor(163.2928)\n",
      "-287.2060458192233 0.0 -287.2060458192233 0.3909833121098991 tensor(350.3890)\n",
      "-286.6920240129506 0.0 -286.6920240129506 0.15324810515887516 tensor(179.4312)\n",
      "-286.73765494192116 0.0 -286.73765494192116 0.1280706604046044 tensor(221.0800)\n",
      "-286.5336789756785 0.0 -286.5336789756785 0.27585830761345337 tensor(197.4863)\n",
      "== Era 8 | Epoch 0 metrics ==\n",
      "\tloss -286.805\n",
      "\tforce 0\n",
      "\tdkl -286.805\n",
      "\tlogp 86.2211\n",
      "\tlogq -200.584\n",
      "\tess 0.228343\n",
      "-287.0503009975665 0.0 -287.0503009975665 0.08894542213934425 tensor(174.7967)\n",
      "-286.8855929341528 0.0 -286.8855929341528 0.23378641124532162 tensor(206.5247)\n",
      "-286.6245273032139 0.0 -286.6245273032139 0.09244428919586345 tensor(199.9145)\n",
      "-286.7680750532395 0.0 -286.7680750532395 0.2552202936407401 tensor(300.2532)\n",
      "-286.8166587434272 0.0 -286.8166587434272 0.17886331899841612 tensor(236.5333)\n",
      "-286.92784444877066 0.0 -286.92784444877066 0.22202795266689324 tensor(217.3436)\n",
      "-286.6904998253022 0.0 -286.6904998253022 0.19575741343780434 tensor(420.6152)\n",
      "-287.12971595514006 0.0 -287.12971595514006 0.37073389033316134 tensor(203.0794)\n",
      "-286.71351050088094 0.0 -286.71351050088094 0.19847852627759865 tensor(158.6437)\n",
      "-286.841633912695 0.0 -286.841633912695 0.20183884437823013 tensor(189.0452)\n",
      "-287.15387301767663 0.0 -287.15387301767663 0.46113791993440356 tensor(238.0217)\n",
      "-286.59757938707884 0.0 -286.59757938707884 0.34155824509790145 tensor(241.5754)\n",
      "-287.16521832690904 0.0 -287.16521832690904 0.37700649318755436 tensor(252.7337)\n",
      "-286.9855792969818 0.0 -286.9855792969818 0.22200731894879897 tensor(216.3356)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.84860390138556 0.0 -286.84860390138556 0.2426777714781034 tensor(214.0343)\n",
      "-287.09429707135615 0.0 -287.09429707135615 0.29540560376311575 tensor(186.0641)\n",
      "-286.84173965693384 0.0 -286.84173965693384 0.23631306334163177 tensor(154.3808)\n",
      "-286.9761126610591 0.0 -286.9761126610591 0.20743095891219507 tensor(158.6649)\n",
      "-286.7689565388434 0.0 -286.7689565388434 0.38610577742021407 tensor(193.9017)\n",
      "-286.645895121196 0.0 -286.645895121196 0.07751613582300144 tensor(180.2106)\n",
      "-286.8510723349157 0.0 -286.8510723349157 0.31194353916395207 tensor(180.0549)\n",
      "-287.28235879650794 0.0 -287.28235879650794 0.34750334377553804 tensor(270.9841)\n",
      "-286.70169200079624 0.0 -286.70169200079624 0.33136134894152947 tensor(185.7115)\n",
      "-286.8108508696748 0.0 -286.8108508696748 0.18504886480900037 tensor(194.3935)\n",
      "-287.0781953992822 0.0 -287.0781953992822 0.21115048757039762 tensor(213.3158)\n",
      "-286.57836898546276 0.0 -286.57836898546276 0.21001629706799327 tensor(180.1364)\n",
      "-286.86788700532054 0.0 -286.86788700532054 0.18578333693256535 tensor(232.3156)\n",
      "-286.7197604001693 0.0 -286.7197604001693 0.29935697276966833 tensor(190.2069)\n",
      "-287.0855845485386 0.0 -287.0855845485386 0.3035540076395745 tensor(179.3980)\n",
      "-286.82451461266845 0.0 -286.82451461266845 0.2759130543463437 tensor(174.8585)\n",
      "-286.88770589055326 0.0 -286.88770589055326 0.5143349838626423 tensor(196.3034)\n",
      "-286.7691147397607 0.0 -286.7691147397607 0.19705809267713367 tensor(198.7532)\n",
      "-287.10233940510557 0.0 -287.10233940510557 0.05840307045280316 tensor(175.4586)\n",
      "-286.53277835129944 0.0 -286.53277835129944 0.16789262834481158 tensor(166.2043)\n",
      "-287.1567881768823 0.0 -287.1567881768823 0.3197774582908106 tensor(200.5488)\n",
      "-287.01987549915475 0.0 -287.01987549915475 0.11309849766986557 tensor(187.1914)\n",
      "-286.9373872295188 0.0 -286.9373872295188 0.15278784611851645 tensor(206.7815)\n",
      "-286.64694623803837 0.0 -286.64694623803837 0.3195694719376067 tensor(189.1516)\n",
      "-286.742248533496 0.0 -286.742248533496 0.2203052946715093 tensor(177.7385)\n",
      "-287.34757545238995 0.0 -287.34757545238995 0.3945235429351088 tensor(244.9893)\n",
      "-286.86934032330646 0.0 -286.86934032330646 0.2508769994780658 tensor(186.9551)\n",
      "-287.04952608491953 0.0 -287.04952608491953 0.22050137904053382 tensor(215.5877)\n",
      "-286.95759622506625 0.0 -286.95759622506625 0.3877222603852211 tensor(211.5625)\n",
      "-286.58650028466025 0.0 -286.58650028466025 0.3810025879693283 tensor(217.0170)\n",
      "-286.81569975718844 0.0 -286.81569975718844 0.10978760450940964 tensor(239.8088)\n",
      "-286.55192858875694 0.0 -286.55192858875694 0.3098232776715827 tensor(290.6700)\n",
      "-287.03506900663945 0.0 -287.03506900663945 0.15762197956138235 tensor(187.6628)\n",
      "-286.89446096047243 0.0 -286.89446096047243 0.17912825964634702 tensor(199.4806)\n",
      "-287.0086851125934 0.0 -287.0086851125934 0.27401764930996253 tensor(219.3841)\n",
      "-286.7627999592525 0.0 -286.7627999592525 0.1401534574044518 tensor(180.5573)\n",
      "-287.2092282794099 0.0 -287.2092282794099 0.26150443282800445 tensor(556.2732)\n",
      "-286.8336523819007 0.0 -286.8336523819007 0.2877815627135159 tensor(165.0089)\n",
      "-287.00883421694004 0.0 -287.00883421694004 0.1605610785207144 tensor(271.0587)\n",
      "-287.13330506585214 0.0 -287.13330506585214 0.20327278448779415 tensor(158.3586)\n",
      "-287.0334179328895 0.0 -287.0334179328895 0.36369335402837727 tensor(170.9138)\n",
      "-286.9240268060592 0.0 -286.9240268060592 0.3127705460227621 tensor(239.1097)\n",
      "-286.68542146223035 0.0 -286.68542146223035 0.31730025004098356 tensor(190.6855)\n",
      "-286.7850552489481 0.0 -286.7850552489481 0.31200070160340676 tensor(162.7750)\n",
      "-286.6800365408724 0.0 -286.6800365408724 0.19189116988989208 tensor(234.7155)\n",
      "-286.7803812351614 0.0 -286.7803812351614 0.0934570826927536 tensor(221.5128)\n",
      "-286.7800590486212 0.0 -286.7800590486212 0.2315446723822382 tensor(191.8008)\n",
      "-287.1188350174058 0.0 -287.1188350174058 0.32268114265085357 tensor(164.0063)\n",
      "-286.87371632555767 0.0 -286.87371632555767 0.2612056600798587 tensor(206.4459)\n",
      "-286.83760497791116 0.0 -286.83760497791116 0.1589387880141227 tensor(282.0832)\n",
      "-286.97637051099565 0.0 -286.97637051099565 0.20237697334774948 tensor(215.5257)\n",
      "-286.7319599289591 0.0 -286.7319599289591 0.2575984819020607 tensor(214.2057)\n",
      "-286.69493042695854 0.0 -286.69493042695854 0.23428736938732783 tensor(167.6752)\n",
      "-287.05803844084994 0.0 -287.05803844084994 0.2524581470915985 tensor(219.2934)\n",
      "-287.19647047546914 0.0 -287.19647047546914 0.3996924991298408 tensor(409.8922)\n",
      "-287.1441142993241 0.0 -287.1441142993241 0.2097560955480427 tensor(200.1681)\n",
      "-287.25842974512653 0.0 -287.25842974512653 0.054724160931757346 tensor(306.1918)\n",
      "-287.0117111915374 0.0 -287.0117111915374 0.25657785218551404 tensor(185.0967)\n",
      "-287.0257249747809 0.0 -287.0257249747809 0.4911235412481266 tensor(235.9850)\n",
      "-286.65705119939435 0.0 -286.65705119939435 0.1043127347910823 tensor(226.6600)\n",
      "-287.073800161098 0.0 -287.073800161098 0.31362332175359764 tensor(207.1535)\n",
      "-287.26259428688496 0.0 -287.26259428688496 0.25573014438012504 tensor(227.8388)\n",
      "-287.05224681381947 0.0 -287.05224681381947 0.06801994402654382 tensor(192.4273)\n",
      "-286.8636523390807 0.0 -286.8636523390807 0.15147439500339585 tensor(195.8406)\n",
      "-286.90740879578675 0.0 -286.90740879578675 0.3671159600637247 tensor(311.6068)\n",
      "-286.75359108891644 0.0 -286.75359108891644 0.20766627697917084 tensor(241.5548)\n",
      "-286.558278867563 0.0 -286.558278867563 0.26673547612466036 tensor(297.8994)\n",
      "-286.8370621391773 0.0 -286.8370621391773 0.31667992076325 tensor(211.8308)\n",
      "-286.7708351780765 0.0 -286.7708351780765 0.36773880869624126 tensor(212.1364)\n",
      "-286.76441469810504 0.0 -286.76441469810504 0.26501252328545866 tensor(254.5784)\n",
      "-286.89039827936716 0.0 -286.89039827936716 0.2876095023772507 tensor(255.7781)\n",
      "-287.31391894473416 0.0 -287.31391894473416 0.16646581166475283 tensor(239.4993)\n",
      "-286.6781969783809 0.0 -286.6781969783809 0.19458144491278997 tensor(194.2659)\n",
      "-286.7095664438882 0.0 -286.7095664438882 0.31559743787448674 tensor(338.5140)\n",
      "-287.1102605496675 0.0 -287.1102605496675 0.29863291270989256 tensor(199.9790)\n",
      "-286.8529328886463 0.0 -286.8529328886463 0.21902255467069176 tensor(251.9831)\n",
      "-287.1829208274735 0.0 -287.1829208274735 0.34770439582691887 tensor(191.0400)\n",
      "-286.82404512800787 0.0 -286.82404512800787 0.16333842735751444 tensor(216.2856)\n",
      "-286.9561416180957 0.0 -286.9561416180957 0.3048879300365042 tensor(168.4267)\n",
      "-286.889456534278 0.0 -286.889456534278 0.2631105149996883 tensor(285.3514)\n",
      "-287.1176654945905 0.0 -287.1176654945905 0.2264482810960326 tensor(202.4685)\n",
      "-286.7934125025712 0.0 -286.7934125025712 0.25721642813739765 tensor(174.9846)\n",
      "-286.80891714384313 0.0 -286.80891714384313 0.166850295899626 tensor(175.5169)\n",
      "-286.8095104721106 0.0 -286.8095104721106 0.4901569225545101 tensor(174.1188)\n",
      "-287.11069967048456 0.0 -287.11069967048456 0.18595805255174463 tensor(221.4297)\n",
      "-286.90356881603316 0.0 -286.90356881603316 0.10182952614475799 tensor(198.6889)\n",
      "== Era 9 | Epoch 0 metrics ==\n",
      "\tloss -286.902\n",
      "\tforce 0\n",
      "\tdkl -286.902\n",
      "\tlogp 86.429\n",
      "\tlogq -200.473\n",
      "\tess 0.24926\n",
      "-286.95375823806694 0.0 -286.95375823806694 0.1940739433340476 tensor(188.7793)\n",
      "-286.70366628693387 0.0 -286.70366628693387 0.1666956659840868 tensor(262.6942)\n",
      "-286.85533060590046 0.0 -286.85533060590046 0.1042179955717242 tensor(194.2877)\n",
      "-286.95521033061345 0.0 -286.95521033061345 0.3255633897107702 tensor(176.5806)\n",
      "-287.1584730253569 0.0 -287.1584730253569 0.16337330849950174 tensor(246.2271)\n",
      "-286.637779632363 0.0 -286.637779632363 0.3333498445425381 tensor(259.7608)\n",
      "-287.17659213522717 0.0 -287.17659213522717 0.17192495116421097 tensor(195.1038)\n",
      "-286.85623851341074 0.0 -286.85623851341074 0.31784011878527696 tensor(181.1449)\n",
      "-286.75688584658116 0.0 -286.75688584658116 0.2808726035622772 tensor(209.8250)\n",
      "-286.95068916151297 0.0 -286.95068916151297 0.4131299033893543 tensor(194.5905)\n",
      "-287.17946308106417 0.0 -287.17946308106417 0.27341246466927677 tensor(240.1623)\n",
      "-287.11699322740515 0.0 -287.11699322740515 0.33623582385319667 tensor(364.1069)\n",
      "-286.87440592012393 0.0 -286.87440592012393 0.13880305693337927 tensor(193.5650)\n",
      "-286.9130623923595 0.0 -286.9130623923595 0.27581728761115 tensor(232.7451)\n",
      "-286.7554198775779 0.0 -286.7554198775779 0.1368394858946732 tensor(732.7920)\n",
      "-287.19537503003835 0.0 -287.19537503003835 0.2599688504228696 tensor(225.8479)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.9512631343992 0.0 -286.9512631343992 0.24617413740198177 tensor(154.4555)\n",
      "-287.1623977758407 0.0 -287.1623977758407 0.342355810479327 tensor(240.7198)\n",
      "-286.7930173669671 0.0 -286.7930173669671 0.20959228435346472 tensor(157.4699)\n",
      "-287.1452035330934 0.0 -287.1452035330934 0.16485865926838597 tensor(179.0042)\n",
      "-286.9528706600418 0.0 -286.9528706600418 0.20678226512173065 tensor(178.4780)\n",
      "-286.99343654530054 0.0 -286.99343654530054 0.3287748074047082 tensor(189.7015)\n",
      "-286.90179416538416 0.0 -286.90179416538416 0.27938592570630255 tensor(160.7295)\n",
      "-286.96891199218373 0.0 -286.96891199218373 0.3231368584825776 tensor(198.3310)\n",
      "-286.9843151577437 0.0 -286.9843151577437 0.23438945491508165 tensor(161.3125)\n",
      "-286.8496409542883 0.0 -286.8496409542883 0.30731494954182237 tensor(157.3821)\n",
      "-286.8520106378356 0.0 -286.8520106378356 0.33597407965101855 tensor(208.6632)\n",
      "-286.91932772932057 0.0 -286.91932772932057 0.25258550675454694 tensor(268.5872)\n",
      "-286.8904245355953 0.0 -286.8904245355953 0.2957357919102966 tensor(424.7750)\n",
      "-286.8317730971285 0.0 -286.8317730971285 0.17723232143274448 tensor(251.7508)\n",
      "-287.1829523323762 0.0 -287.1829523323762 0.2680500561538494 tensor(241.4253)\n",
      "-286.73093822847386 0.0 -286.73093822847386 0.31002347964441906 tensor(200.1369)\n",
      "-287.04897019610604 0.0 -287.04897019610604 0.363331524503111 tensor(180.4431)\n",
      "-286.71840049430614 0.0 -286.71840049430614 0.27803518775263214 tensor(185.9013)\n",
      "-286.94861766528 0.0 -286.94861766528 0.3174170963071807 tensor(182.4027)\n",
      "-287.00201675782387 0.0 -287.00201675782387 0.15509785178145533 tensor(181.0288)\n",
      "-286.64053508990764 0.0 -286.64053508990764 0.37437753310258826 tensor(321.1803)\n",
      "-287.174329030981 0.0 -287.174329030981 0.20103110734209723 tensor(274.5382)\n",
      "-286.7489516978724 0.0 -286.7489516978724 0.18286388761728953 tensor(218.9724)\n",
      "-287.1488414598192 0.0 -287.1488414598192 0.1777266328672361 tensor(261.5974)\n",
      "-287.1391504763218 0.0 -287.1391504763218 0.3930152510732271 tensor(172.5532)\n",
      "-286.785256595181 0.0 -286.785256595181 0.3885747069590969 tensor(197.6328)\n",
      "-287.1243552514143 0.0 -287.1243552514143 0.3592213411650424 tensor(213.5825)\n",
      "-287.0417735284742 0.0 -287.0417735284742 0.2714557772372436 tensor(178.3213)\n",
      "-286.97594002208274 0.0 -286.97594002208274 0.06380959930811098 tensor(179.3508)\n",
      "-286.7732289375083 0.0 -286.7732289375083 0.23191352252350192 tensor(193.7462)\n",
      "-286.9923171108418 0.0 -286.9923171108418 0.2538355520251568 tensor(177.8758)\n",
      "-286.7340290854341 0.0 -286.7340290854341 0.25857227552833356 tensor(211.0760)\n",
      "-287.0931691520906 0.0 -287.0931691520906 0.2128232434821998 tensor(205.9031)\n",
      "-286.91095287341466 0.0 -286.91095287341466 0.22354005665233728 tensor(188.8850)\n",
      "-286.75951806048636 0.0 -286.75951806048636 0.25258597323817333 tensor(210.7227)\n",
      "-286.95930189321035 0.0 -286.95930189321035 0.1656089587431872 tensor(209.0883)\n",
      "-286.95145167728833 0.0 -286.95145167728833 0.3511495687198409 tensor(192.0403)\n",
      "-286.67471239682976 0.0 -286.67471239682976 0.24763509192455135 tensor(181.9440)\n",
      "-286.7617799043959 0.0 -286.7617799043959 0.23299757447759106 tensor(218.9998)\n",
      "-286.7464018355622 0.0 -286.7464018355622 0.0953968656054318 tensor(240.4571)\n",
      "-286.9416258649743 0.0 -286.9416258649743 0.2372477749505365 tensor(198.5823)\n",
      "-286.76260319939746 0.0 -286.76260319939746 0.5239794372887334 tensor(225.1059)\n",
      "-286.8074708700202 0.0 -286.8074708700202 0.18012794999089882 tensor(191.5551)\n",
      "-287.00455685415767 0.0 -287.00455685415767 0.3279751493766123 tensor(247.6877)\n",
      "-286.92767234598557 0.0 -286.92767234598557 0.08168709804095407 tensor(187.1436)\n",
      "-286.97969200696923 0.0 -286.97969200696923 0.1717873081269275 tensor(175.7775)\n",
      "-286.7802799435841 0.0 -286.7802799435841 0.24231100079801277 tensor(202.9088)\n",
      "-286.91152541475606 0.0 -286.91152541475606 0.14551399963080985 tensor(199.5358)\n",
      "-287.1789949768492 0.0 -287.1789949768492 0.27435125152775514 tensor(183.9635)\n",
      "-286.5534806829205 0.0 -286.5534806829205 0.33338982029674086 tensor(245.1928)\n",
      "-287.04504347936165 0.0 -287.04504347936165 0.36952382313881044 tensor(216.6997)\n",
      "-287.16788746375613 0.0 -287.16788746375613 0.16932886278684675 tensor(207.0676)\n",
      "-287.1728999948749 0.0 -287.1728999948749 0.2809114981845519 tensor(186.0361)\n",
      "-286.95492522405164 0.0 -286.95492522405164 0.49817401901103014 tensor(212.5157)\n",
      "-287.06513641185205 0.0 -287.06513641185205 0.16659224584277665 tensor(215.6959)\n",
      "-287.04732340506473 0.0 -287.04732340506473 0.3564403010212563 tensor(210.0065)\n",
      "-287.3262316661934 0.0 -287.3262316661934 0.31859265598974346 tensor(206.1430)\n",
      "-287.3139153303763 0.0 -287.3139153303763 0.2961767446012404 tensor(191.1863)\n",
      "-286.9122450664431 0.0 -286.9122450664431 0.2915620310190827 tensor(165.1841)\n",
      "-287.10601875628214 0.0 -287.10601875628214 0.08714594294200713 tensor(178.1445)\n",
      "-287.057956882605 0.0 -287.057956882605 0.27096876575494133 tensor(245.4681)\n",
      "-286.92434171119316 0.0 -286.92434171119316 0.30265327932682545 tensor(184.1326)\n",
      "-287.54981819086345 0.0 -287.54981819086345 0.2540010843221418 tensor(219.2152)\n",
      "-287.03124780849447 0.0 -287.03124780849447 0.27701987727937816 tensor(208.5380)\n",
      "-286.94990645620396 0.0 -286.94990645620396 0.30573862867306606 tensor(261.0406)\n",
      "-286.61261204409413 0.0 -286.61261204409413 0.33821658051491116 tensor(347.0771)\n",
      "-286.9048657549396 0.0 -286.9048657549396 0.2015714233231112 tensor(157.9157)\n",
      "-287.3518387994616 0.0 -287.3518387994616 0.05134790160530708 tensor(163.2189)\n",
      "-286.8263877782321 0.0 -286.8263877782321 0.24362479661580017 tensor(213.7627)\n",
      "-287.26727556738695 0.0 -287.26727556738695 0.16104435150350502 tensor(178.5453)\n",
      "-286.7350128625406 0.0 -286.7350128625406 0.30937366747571193 tensor(175.9881)\n",
      "-286.9881399126089 0.0 -286.9881399126089 0.33831059039830247 tensor(191.2131)\n",
      "-286.75867021062413 0.0 -286.75867021062413 0.2347778489660164 tensor(150.8507)\n",
      "-287.0654998017816 0.0 -287.0654998017816 0.3619113865487419 tensor(197.7733)\n",
      "-287.2503998482407 0.0 -287.2503998482407 0.2593236438633501 tensor(138.5913)\n",
      "-287.2876050091837 0.0 -287.2876050091837 0.14126169155520776 tensor(211.0858)\n",
      "-287.1666037533643 0.0 -287.1666037533643 0.26670094968982727 tensor(171.8932)\n",
      "-287.1985667172522 0.0 -287.1985667172522 0.1335572068095466 tensor(200.0273)\n",
      "-286.65694769165555 0.0 -286.65694769165555 0.34742399755733355 tensor(236.7260)\n",
      "-287.2454942583187 0.0 -287.2454942583187 0.19005758665672132 tensor(194.6591)\n",
      "-286.97321066971915 0.0 -286.97321066971915 0.32804518825902296 tensor(172.4454)\n",
      "-287.16966754817 0.0 -287.16966754817 0.28892767199739466 tensor(167.1160)\n",
      "-286.7389849338693 0.0 -286.7389849338693 0.2654589297485995 tensor(245.4889)\n",
      "Accept rate: 0.349609375\n",
      "Topological susceptibility = 1.36 +/- 0.13\n",
      "... vs HMC estimate = 1.23 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "pre_flow_model, flow_act = flow_train(param)\n",
    "flow_eval(pre_flow_model,flow_act)\n",
    "pre_flow = pre_flow_model['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept rate: 0.2890625\n",
      "Topological susceptibility = 1.17 +/- 0.09\n",
      "... vs HMC estimate = 1.23 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "train_force = False\n",
    "flow_model = None\n",
    "if train_force:\n",
    "    flow_model, flow_act = flow_train(param, with_force=True, pre_model=pre_flow_model)\n",
    "else:\n",
    "    flow_model = pre_flow_model\n",
    "flow_eval(flow_model,flow_act)\n",
    "flow = flow_model['layers']\n",
    "# flow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.4054)\n",
      "tensor(12.3251)\n"
     ]
    }
   ],
   "source": [
    "def test_force(x = None):\n",
    "    model = flow_model\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    if x == None:\n",
    "        pre_model = pre_flow_model\n",
    "        pre_layers, pre_prior = pre_model['layers'], pre_model['prior']\n",
    "        pre_xi = pre_prior.sample_n(1)\n",
    "        x = ft_flow(pre_layers, pre_xi)\n",
    "    xi = ft_flow_inv(layers, x)\n",
    "    f = ft_force(param, layers, xi)\n",
    "    f_s = torch.linalg.norm(f)\n",
    "    print(f_s)\n",
    "\n",
    "test_force()\n",
    "test_force(torch.reshape(field,(1,)+field.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 2\n",
      "tau = 2\n",
      "steps = 8\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.662768641905577  topo: 0.0\n",
      "plaq(x) 0.662768641905577  force.norm 20.998065855859643\n",
      "Traj:    1  ACCEPT:  dH: -0.07537746   exp(-dH):  1.0782911    plaq:  0.64252562   topo: -2.0\n",
      "plaq(x) 0.6425256187449909  force.norm 20.872768644880818\n",
      "Traj:    2  ACCEPT:  dH:  0.1301976    exp(-dH):  0.87792193   plaq:  0.60711194   topo:  1.0\n",
      "plaq(x) 0.6071119405479931  force.norm 19.907921445212118\n",
      "Traj:    3  REJECT:  dH:  0.30832437   exp(-dH):  0.73467697   plaq:  0.60711194   topo:  1.0\n",
      "plaq(x) 0.6071119405479931  force.norm 21.31516480980625\n",
      "Traj:    4  ACCEPT:  dH:  0.55552569   exp(-dH):  0.57377056   plaq:  0.66056494   topo:  2.0\n",
      "plaq(x) 0.6605649410770968  force.norm 17.99000516761638\n",
      "Traj:    5  ACCEPT:  dH:  0.40886075   exp(-dH):  0.66440674   plaq:  0.67927934   topo:  2.0\n",
      "plaq(x) 0.6792793354321754  force.norm 17.976535887639457\n",
      "Traj:    6  ACCEPT:  dH: -0.41974358   exp(-dH):  1.5215713    plaq:  0.70891628   topo:  0.0\n",
      "plaq(x) 0.7089162820102802  force.norm 19.015342711960006\n",
      "Traj:    7  REJECT:  dH:  1.2231339    exp(-dH):  0.29430639   plaq:  0.70891628   topo:  0.0\n",
      "plaq(x) 0.7089162820102802  force.norm 20.349839059100024\n",
      "Traj:    8  ACCEPT:  dH: -0.11523443   exp(-dH):  1.1221365    plaq:  0.70742652   topo:  1.0\n",
      "Run times:  [0.01830169398454018, 0.019264473987277597, 0.01908413300407119, 0.018925440002931282]\n",
      "Per trajectory:  [0.00915084699227009, 0.009632236993638799, 0.009542066502035595, 0.009462720001465641]\n"
     ]
    }
   ],
   "source": [
    "field = run(param, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaq(field_run[0]) 0.6716397754361682\n",
      "tensor([31.7440], grad_fn=<AddBackward0>) tensor([-31.7440], grad_fn=<AddBackward0>)\n",
      "original_action tensor(5.0301, grad_fn=<AddBackward0>)\n",
      "eff_action tensor([1.7741], grad_fn=<AddBackward0>)\n",
      "plaq(x) -0.1024314687100247  logJ tensor([31.7440], grad_fn=<AddBackward0>)  force.norm 34.70517091343915\n",
      "plaq(y) 0.67163984638713\n",
      "plaq(x) 0.6716397754361682  force.norm 18.129125806140568\n"
     ]
    }
   ],
   "source": [
    "field_run = torch.reshape(field,(1,)+field.shape)\n",
    "flows = flow\n",
    "\n",
    "print(f'plaq(field_run[0]) {action(param, field_run[0]) / (-param.beta*param.volume)}')\n",
    "# field.requires_grad_(True)\n",
    "x = field_run\n",
    "logJ = 0.0\n",
    "for layer in reversed(flows):\n",
    "    x, lJ = layer.reverse(x)\n",
    "    logJ += lJ\n",
    "\n",
    "# x is the prior distribution now\n",
    "\n",
    "x.requires_grad_(True)\n",
    "    \n",
    "y = x\n",
    "logJy = 0.0\n",
    "for layer in flows:\n",
    "    y, lJ = layer.forward(y)\n",
    "    logJy += lJ\n",
    "    \n",
    "s = action(param, y[0]) - logJy\n",
    "\n",
    "print(logJ,logJy)\n",
    "\n",
    "\n",
    "# print(\"eff_action\", s + 136.3786)\n",
    "\n",
    "print(\"original_action\", action(param, y[0]) + 91)\n",
    "\n",
    "print(\"eff_action\", s + 56)\n",
    "\n",
    "s.backward()\n",
    "\n",
    "f = x.grad\n",
    "\n",
    "x.requires_grad_(False)\n",
    "\n",
    "print(f'plaq(x) {action(param, x[0]) / (-param.beta*param.volume)}  logJ {logJ}  force.norm {torch.linalg.norm(f)}')\n",
    "\n",
    "print(f'plaq(y) {action(param, y[0]) / (-param.beta*param.volume)}')\n",
    "\n",
    "print(f'plaq(x) {action(param, field_run[0]) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(force(param, field_run[0]))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 8, 8])\n",
      "tensor(34.7052)\n",
      "tensor(34.7052)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x = ft_flow_inv(flow, field_run)\n",
    "# x = field_run\n",
    "#for layer in reversed(flows):\n",
    "#    x, lJ = layer.reverse(x)\n",
    "ff = ft_force(param, flow, x)\n",
    "print(torch.linalg.norm(ff))\n",
    "fff = ft_force(param, flow, x)\n",
    "print(torch.linalg.norm(fff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-54.2259], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ft_flow_inv(flow, field_run)\n",
    "ft_action(param, flow, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattern(l):\n",
    "    return [x for y in l for x in y]\n",
    "\n",
    "def average(l):\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def sigma(l):\n",
    "    avg = average(l)\n",
    "    sq_avg = average([ np.square(v - avg) for v in l ])\n",
    "    return sq_avg / np.sqrt(len(l) - 1)\n",
    "\n",
    "def sub_avg(l):\n",
    "    avg = average(l)\n",
    "    return np.array([x - avg for x in l])\n",
    "\n",
    "n_block = 16 # ADJUST ME\n",
    "\n",
    "def block_list(l):\n",
    "    n_block_local = n_block\n",
    "    size_block = len(l) // n_block_local\n",
    "    if size_block < 1:\n",
    "        size_block = 1\n",
    "        n_block_local = len(l)\n",
    "    if n_block_local == 0:\n",
    "        return []\n",
    "    start = len(l) - n_block_local * size_block\n",
    "    return [ l[ start + i * size_block : start + (i+1) * size_block] for i in range(n_block_local) ]\n",
    "\n",
    "def change_sqr(l, lp):\n",
    "    size = min(len(l), len(lp))\n",
    "    if size == 0:\n",
    "        return []\n",
    "    vs = [ np.square(l[i] - lp[i]) for i in range(size) ]\n",
    "    vs = list(map(average, block_list(vs)))\n",
    "    avg = average(vs)\n",
    "    sig = sigma(vs)\n",
    "    return [ avg, sig ]\n",
    "\n",
    "def change_sqr_vs_dt(l, dt_range = 10):\n",
    "    return [ [ i ] + change_sqr(l, l[i:]) for i in range(1, dt_range + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_hmc_info_list = []\n",
    "def ft_leapfrog(param, flow, x, p):\n",
    "    mom_norm = torch.sum(p*p)\n",
    "    info_list = []\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = ft_force(param, flow, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    info = np.array((float(torch.linalg.norm(f)),\n",
    "                     float(ft_action(param, flow, x_).detach()),\n",
    "                     float(torch.sum(p*p_)/np.sqrt(mom_norm*torch.sum(p_*p_)))))\n",
    "    info_list.append(info)\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        f = ft_force(param, flow, x_)\n",
    "        info = np.array((float(torch.linalg.norm(f)),\n",
    "                        float(ft_action(param, flow, x_).detach()),\n",
    "                        float(torch.sum(p*p_)/np.sqrt(mom_norm*torch.sum(p_*p_)))))\n",
    "        info_list.append(info)\n",
    "        p_ = p_ + (-dt)*f\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    print(np.sqrt(average([l[0]**2 for l in info_list])),\n",
    "          (info_list[0][1], info_list[-1][1]),\n",
    "          info_list[-1][2])\n",
    "    ft_hmc_info_list.append(info_list)\n",
    "    return (x_, p_)\n",
    "\n",
    "def ft_hmc(param, flow, field):\n",
    "    x = ft_flow_inv(flow, field)\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = ft_action(param, flow, x).detach() + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = ft_leapfrog(param, flow, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = ft_action(param, flow, xr).detach() + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    # ADJUST ME\n",
    "    newx = xr if acc else x\n",
    "    # newx = xr\n",
    "    newfield = ft_flow(flow, newx)\n",
    "    return (float(dH), float(exp_mdH), acc, newfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_run(param, flow, field = None):\n",
    "    if field == None:\n",
    "        field = param.initializer()\n",
    "    ft_hmc_info_list = []\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo} {field.shape}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                field_run = torch.reshape(field,(1,)+field.shape)\n",
    "                dH, exp_mdH, acc, field_run = ft_hmc(param, flow, field_run)\n",
    "                field = field_run[0]\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.7887135679736341  topo: 1.0 torch.Size([2, 8, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-9bfc4b1acf09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# field = ft_run(param, pre_flow)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# field = ft_run(param, pre_flow, field)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-92cf93af01c1>\u001b[0m in \u001b[0;36mft_run\u001b[0;34m(param, flow, field)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mfield_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mdH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_mdH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_hmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mplaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-4105e60d2aa8>\u001b[0m in \u001b[0;36mft_hmc\u001b[0;34m(param, flow, field)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mact0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_leapfrog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-4105e60d2aa8>\u001b[0m in \u001b[0;36mft_leapfrog\u001b[0;34m(param, flow, x, p)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnstep\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_force\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         info = np.array((float(torch.linalg.norm(f)),\n\u001b[1;32m     17\u001b[0m                         \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0657b5d4ab2c>\u001b[0m in \u001b[0;36mft_force\u001b[0;34m(param, flow, field, create_graph)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# f.grad = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0657b5d4ab2c>\u001b[0m in \u001b[0;36mft_action\u001b[0;34m(param, flow, f)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlogJy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlogJy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU1GaugeAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/application/Public/FTHMC/github/ipynb/field_transformation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mplaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_u1_plaq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mnew_plaq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaq_coupling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mdelta_plaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_plaq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mplaq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mdelta_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_plaq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mdelta_plaq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# signs for U vs Udagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/application/Public/FTHMC/github/ipynb/field_transformation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frozen'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_cos_sin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNN must output n_mix (s_i) + 1 (t) channels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'zeros'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n\u001b[0m\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 0.5, # 0.3\n",
    "    nstep = 64, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    nprint = 4,\n",
    "    #\n",
    "    seed = 1331)\n",
    "\n",
    "# field = ft_run(param, pre_flow)\n",
    "# field = ft_run(param, pre_flow, field)\n",
    "field = ft_run(param, flow, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.7887135679736341  topo: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-9bfc4b1acf09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# field = ft_run(param, pre_flow)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# field = ft_run(param, pre_flow, field)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-0e8f9d55b0a4>\u001b[0m in \u001b[0;36mft_run\u001b[0;34m(param, flow, field)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mfield_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mdH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_mdH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_hmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_run\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mplaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-4105e60d2aa8>\u001b[0m in \u001b[0;36mft_hmc\u001b[0;34m(param, flow, field)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mact0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_leapfrog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-4105e60d2aa8>\u001b[0m in \u001b[0;36mft_leapfrog\u001b[0;34m(param, flow, x, p)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_force\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         info = np.array((float(torch.linalg.norm(f)),\n\u001b[0;32m---> 17\u001b[0;31m                         \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                         float(torch.sum(p*p_)/np.sqrt(mom_norm*torch.sum(p_*p_)))))\n\u001b[1;32m     19\u001b[0m         \u001b[0minfo_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0657b5d4ab2c>\u001b[0m in \u001b[0;36mft_action\u001b[0;34m(param, flow, f)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlogJy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlogJy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU1GaugeAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/application/Public/FTHMC/github/ipynb/field_transformation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mplaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_u1_plaq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mnew_plaq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaq_coupling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mdelta_plaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_plaq\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mplaq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mdelta_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_plaq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mdelta_plaq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# signs for U vs Udagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/application/Public/FTHMC/github/ipynb/field_transformation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frozen'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_cos_sin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNN must output n_mix (s_i) + 1 (t) channels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/application/Public/FTHMC/github/ipynb/field_transformation.py\u001b[0m in \u001b[0;36mstack_cos_sin\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmid_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack_cos_sin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNCPPlaqCouplingLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     def __init__(self, net, *, mask_shape, mask_mu, mask_off,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 0.5, # 0.3\n",
    "    nstep = 64, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    nprint = 4,\n",
    "    #\n",
    "    seed = 1331)\n",
    "\n",
    "# field = ft_run(param, pre_flow)\n",
    "# field = ft_run(param, pre_flow, field)\n",
    "field = ft_run(param, flow, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8744031477616621"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_list = np.array([l[1] for l in flattern(ft_hmc_info_list)])\n",
    "action_list = sub_avg(action_list)\n",
    "np.sqrt(average(action_list**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.58414880207859"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_list = np.array([l[0] for l in flattern(ft_hmc_info_list)])\n",
    "np.sqrt(average(force_list**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.09316701  26.38977078  68.00603671  31.22982216  29.12227097\n",
      "  21.62493451  18.86294066  16.22557307  13.16750373  10.03210621\n",
      "   7.26475395   5.25456284   4.48932086   5.28910149   7.1604243\n",
      "   9.41633164  11.56022469  13.34897174  15.0347588   17.06472245\n",
      "  13.63004187  13.67601611  23.44102517  15.40261377  13.8023178\n",
      "  13.10502926   9.80238718   9.96926622  12.14492081  11.3295407\n",
      "   9.74145174   8.83835725   8.45431986   8.16819299   7.74117561\n",
      "   7.01310237   5.88640763   5.09706541   7.34167767  11.13599049\n",
      "  11.48665577   8.42736975   6.56570125   6.74110212   7.24219414\n",
      "   7.34891711   6.94702375   6.30927502   5.95707362   6.37242431\n",
      "   8.58184857  14.59622813  21.46877935  21.35009509  14.97622465\n",
      "  10.38055866   9.55276204  10.11013628  11.13003905  12.4687411\n",
      "  14.04994638  15.84468009  17.9148159   20.32374896  21.45948374\n",
      "  20.98446039  20.24634092  19.33385882  18.35541384  17.40966759\n",
      "  16.56214508  15.83911804  15.23833978  14.74681446  14.354167\n",
      "  14.05499849  13.84057964  13.68593788  13.54062623  13.32954698\n",
      "  12.96605141  12.37562472  11.52466345  10.44436307   9.23756731\n",
      "   8.06087347   7.08268658   6.42481259   6.11079176   6.06545174\n",
      "   6.17414963   6.34383661   6.52771069   6.72799048   6.99736015\n",
      "   7.43591693   8.1480001    9.11844789  10.0597549   10.41581458\n",
      "   9.69978523   8.10552296   6.90094777   7.17964762   7.92215354\n",
      "   8.08087148   8.10880808   9.21887789  12.34212954  18.07967304\n",
      "  25.5810841   29.06969071  28.66098311  35.36729676  32.25800075\n",
      "  26.17858062  23.05047191  19.81355863  16.22342543  13.51183319\n",
      "  12.92224123  15.66180783  26.21573667  59.54632047  94.42847055\n",
      "  95.27213509  79.32085015  39.74508977  31.90677193  32.57720051\n",
      "  33.07161128  33.6212482   34.66086613  35.99198272  40.384848\n",
      "  57.93015108  74.84169075  78.71714521  74.79996951  50.45443701\n",
      "  18.70598999   8.1077732    8.68973489  10.06191789  12.76605787\n",
      "  16.53354321  19.52654177  20.49882476  19.73472463  18.15745473\n",
      "  16.53731654  15.41114038  15.10411429  15.52352871  16.16938716\n",
      "  16.60069964  16.82383194  17.2501277   18.28344579  19.90431396\n",
      "  22.11380627  27.29224746  38.1747208   23.40673481  30.25971739\n",
      "  34.11590211  27.86120259  21.50226568  16.24526185  12.51081428\n",
      "   9.89669702  11.5674501   58.85183874 112.63527542 114.78446305\n",
      "  72.43939216  14.50057963   6.85498518   7.13273086   7.37238715\n",
      "   9.70582967  14.10411407  18.81285795  22.39825498  24.54681162\n",
      "  25.65048157  26.19460307  26.54898859  26.99527178  27.719621\n",
      "  28.7422634   29.84046449  29.93543183  28.99228804  28.03715794\n",
      "  27.28812611  26.78487191  26.41743435  25.90314796  24.77626882\n",
      "  22.51057764  18.89880312  14.54560864  10.84924375   8.94633358\n",
      "   7.48579064  17.3521546   95.97703983 119.73151391 116.9751738\n",
      "  45.22232151 128.48281058  26.5517386   14.4769289   17.73086215\n",
      "  20.84804765  22.23274971  18.67007458  13.23139047  12.34849949\n",
      "  11.25733629   9.92422647  13.28177059  11.79125812   8.62538928\n",
      "   8.62855011   8.53479263   8.51255936   8.40980943   8.24186913\n",
      "   8.05577428   7.87705123   7.72802      7.6498317    7.69977086\n",
      "   7.88064795   8.00843473   7.91780985   9.17760593  14.4593458\n",
      "  19.44715769  18.15547273  14.09005439  12.76624536  13.32130166\n",
      "  14.30878279  15.53330054  17.87591109  22.16584113  25.36063617\n",
      "  25.93247046  25.68735776  25.47584892  25.43140872  25.34790949\n",
      "  24.92880443  24.09072044  23.22452493  22.41402063  21.60822916\n",
      "  22.42102798 379.98781674  15.22729134  14.44044496  14.04494353\n",
      "  13.91504852  13.94350326  13.89219468  12.98443362   9.76684802\n",
      "   9.51753746  21.73463357  23.46636634  12.07282621  10.14676349\n",
      "  13.03207835  11.65654379   9.14263492   7.39255237   6.49967334\n",
      "   6.57155203   8.35932129  12.53869334  18.89580044  25.26190203\n",
      "  27.10883763  22.55205081  15.77811158  10.56454751   7.35888174\n",
      "   5.69300571   5.26369594   5.71900305   6.8057653    8.53133809\n",
      "  10.95557921  13.96475331  17.23459194  20.37327817  23.09265236]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(force_list[0:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0 torch.Size([2, 12, 12])\n",
      "plaq(x) 1.0  force.norm 0.39715422715538007\n",
      "Traj:    1  ACCEPT:  dH: -0.0062864964  exp(-dH):  1.0063063    plaq:  0.81235594   topo:  0.0\n",
      "plaq(x) 0.812355938734624  force.norm 27.202019368757615\n",
      "Traj:    2  ACCEPT:  dH: -0.0015405631  exp(-dH):  1.0015418    plaq:  0.7492163    topo:  0.0\n",
      "plaq(x) 0.7492163023596704  force.norm 29.485665083133863\n",
      "Traj:    3  ACCEPT:  dH:  0.00037426607  exp(-dH):  0.9996258    plaq:  0.72375702   topo:  0.0\n",
      "plaq(x) 0.7237570215411304  force.norm 28.303420580973683\n",
      "Traj:    4  ACCEPT:  dH: -0.0013935697  exp(-dH):  1.0013945    plaq:  0.67309378   topo:  0.0\n",
      "plaq(x) 0.6730937834511482  force.norm 29.926790016437316\n",
      "Traj:    5  ACCEPT:  dH:  0.00020590437  exp(-dH):  0.99979412   plaq:  0.67381348   topo:  0.0\n",
      "plaq(x) 0.6738134751318049  force.norm 29.254445461455482\n",
      "Traj:    6  ACCEPT:  dH: -0.0010210109  exp(-dH):  1.0010215    plaq:  0.63424319   topo:  0.0\n",
      "plaq(x) 0.6342431936955772  force.norm 30.810502093683475\n",
      "Traj:    7  ACCEPT:  dH:  0.0017600222  exp(-dH):  0.99824153   plaq:  0.70351444   topo:  2.0\n",
      "plaq(x) 0.70351444445981  force.norm 28.09674494949839\n",
      "Traj:    8  ACCEPT:  dH: -0.00014578087  exp(-dH):  1.0001458    plaq:  0.69008336   topo:  1.0\n",
      "plaq(x) 0.6900833552793605  force.norm 28.731337250731077\n",
      "Traj:    9  ACCEPT:  dH: -6.891858e-05  exp(-dH):  1.0000689    plaq:  0.67354262   topo:  1.0\n",
      "plaq(x) 0.6735426189660738  force.norm 28.428738762489594\n",
      "Traj:   10  ACCEPT:  dH:  0.00069516431  exp(-dH):  0.99930508   plaq:  0.69156271   topo:  1.0\n",
      "plaq(x) 0.6915627099062224  force.norm 27.194316041901697\n",
      "Traj:   11  ACCEPT:  dH: -0.000784183  exp(-dH):  1.0007845    plaq:  0.69946047   topo:  1.0\n",
      "plaq(x) 0.6994604723126163  force.norm 29.515955049741972\n",
      "Traj:   12  ACCEPT:  dH: -0.00032878553  exp(-dH):  1.0003288    plaq:  0.67413458   topo: -1.0\n",
      "plaq(x) 0.6741345807610624  force.norm 28.98205375100225\n",
      "Traj:   13  ACCEPT:  dH:  0.0004953974  exp(-dH):  0.99950473   plaq:  0.6939326    topo:  1.0\n",
      "plaq(x) 0.6939326003188925  force.norm 28.037216835456476\n",
      "Traj:   14  ACCEPT:  dH:  0.00074450643  exp(-dH):  0.99925577   plaq:  0.74674402   topo:  0.0\n",
      "plaq(x) 0.7467440192862353  force.norm 27.003561692540437\n",
      "Traj:   15  ACCEPT:  dH: -0.00059242672  exp(-dH):  1.0005926    plaq:  0.72912235   topo:  0.0\n",
      "plaq(x) 0.7291223545981094  force.norm 27.841438782648805\n",
      "Traj:   16  ACCEPT:  dH: -0.0011345629  exp(-dH):  1.0011352    plaq:  0.66816103   topo:  0.0\n",
      "Run times:  [0.17318592400988564, 0.1469544539868366, 0.14312102901749313, 0.13111080101225525]\n",
      "Per trajectory:  [0.04329648100247141, 0.03673861349670915, 0.03578025725437328, 0.03277770025306381]\n"
     ]
    }
   ],
   "source": [
    "param_new = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (12, 12),\n",
    "    tau = 0.5, # 0.3\n",
    "    nstep = 64, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    nprint = 4,\n",
    "    #\n",
    "    seed = 1331)\n",
    "\n",
    "field_new = run(param_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nets(layers):\n",
    "    nets = []\n",
    "    for l in layers:\n",
    "        nets.append(l.plaq_coupling.net)\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_u1_equiv_layers_net(*, lattice_shape, nets):\n",
    "    n_layers = len(nets)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        # periodically loop through all arrangements of maskings\n",
    "        mu = i % 2\n",
    "        off = (i//2) % 4\n",
    "        net = nets[i]\n",
    "        plaq_coupling = NCPPlaqCouplingLayer(\n",
    "            net, mask_shape=lattice_shape, mask_mu=mu, mask_off=off)\n",
    "        link_coupling = GaugeEquivCouplingLayer(\n",
    "            lattice_shape=lattice_shape, mask_mu=mu, mask_off=off, \n",
    "            plaq_coupling=plaq_coupling)\n",
    "        layers.append(link_coupling)\n",
    "    return torch.nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_nets(flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(get_nets(flow)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(flow, f\"flow_8x8_beta={param.beta}.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_load = torch.load(f\"flow_8x8_beta={param.beta}.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0 torch.Size([2, 8, 8])\n",
      "2.130422682286812 (-53.222385355919386, -53.429393766012076) 0.9978707399156546\n",
      "Traj:    1  ACCEPT:  dH: -0.0042142763  exp(-dH):  1.0042232    plaq:  0.90614763   topo:  1.0\n",
      "4.029733486080523 (-53.417886748435585, -54.24536203857387) 0.9946731949446221\n",
      "Traj:    2  ACCEPT:  dH:  1.1748347e-05  exp(-dH):  0.99998825   plaq:  0.87431725   topo:  0.0\n",
      "18.183895995414918 (-54.23038681455611, -54.38308753681161) 0.9545260703723759\n",
      "Traj:    3  ACCEPT:  dH:  0.0027780899  exp(-dH):  0.99722577   plaq:  0.80137963   topo:  0.0\n",
      "16.457871831575794 (-54.31892098117961, -53.913041180964) 0.927469245598059\n",
      "Traj:    4  ACCEPT:  dH: -0.0010801517  exp(-dH):  1.0010807    plaq:  0.77648579   topo: -2.0\n",
      "23.428317208817752 (-53.88039348814588, -55.737052236478576) 0.8380530816381841\n",
      "Traj:    5  ACCEPT:  dH: -0.011471572  exp(-dH):  1.0115376    plaq:  0.71429      topo:  1.0\n",
      "26.041669827299454 (-55.676816307041044, -56.007745996089405) 0.8560342095589245\n",
      "Traj:    6  REJECT:  dH:  0.63318235   exp(-dH):  0.5308996    plaq:  0.71428996   topo:  1.0\n",
      "14.384428194709 (-55.84714542538101, -54.01970268560733) 0.9817901094814632\n",
      "Traj:    7  ACCEPT:  dH:  0.0017229297  exp(-dH):  0.99827855   plaq:  0.76299302   topo:  0.0\n",
      "13.367513132042133 (-53.93749043909432, -51.4530761810442) 0.9793672023516626\n",
      "Traj:    8  ACCEPT:  dH: -0.17195395   exp(-dH):  1.1876231    plaq:  0.72689869   topo:  2.0\n",
      "28.626026728069455 (-51.33373453073922, -52.54047759618333) 0.9527591841310407\n",
      "Traj:    9  ACCEPT:  dH:  0.040663787  exp(-dH):  0.96015189   plaq:  0.69912499   topo:  1.0\n",
      "24.71123444953893 (-52.462413465154945, -52.92497040112319) 0.9300076242848005\n",
      "Traj:   10  REJECT:  dH:  0.28888943   exp(-dH):  0.74909503   plaq:  0.69912497   topo:  1.0\n",
      "21.559768810632303 (-52.55352817445676, -51.315267750045514) 0.9274258702796708\n",
      "Traj:   11  REJECT:  dH:  1.2333579    exp(-dH):  0.29131274   plaq:  0.69912501   topo:  1.0\n",
      "16.108842121282223 (-52.419614385512595, -52.86399306827553) 0.9424625838977652\n",
      "Traj:   12  ACCEPT:  dH: -0.0071106829  exp(-dH):  1.007136     plaq:  0.70583495   topo:  0.0\n",
      "18.68092111186873 (-52.78781258114179, -52.78104117790631) 0.9147975902129266\n",
      "Traj:   13  ACCEPT:  dH:  0.010737852  exp(-dH):  0.98931959   plaq:  0.71345941   topo: -1.0\n",
      "19.83246672848734 (-52.82474799245566, -53.374151945486474) 0.9820945061022079\n",
      "Traj:   14  ACCEPT:  dH:  0.00082361776  exp(-dH):  0.99917672   plaq:  0.703809     topo: -1.0\n",
      "14.03376252266432 (-53.475485123117814, -51.85973076048177) 0.9380116852948601\n",
      "Traj:   15  ACCEPT:  dH: -0.00065869991  exp(-dH):  1.0006589    plaq:  0.68234504   topo: -1.0\n",
      "24.66526512008481 (-51.75609637550189, -53.2716748565242) 0.9394176125907822\n",
      "Traj:   16  REJECT:  dH:  0.40122056   exp(-dH):  0.66950238   plaq:  0.68234505   topo: -1.0\n",
      "Run times:  [47.113823952997336, 46.901855227013584, 47.161451328982366, 46.758459962991765]\n",
      "Per trajectory:  [11.778455988249334, 11.725463806753396, 11.790362832245592, 11.689614990747941]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6534, 5.4021, 0.4575, 4.8465, 2.8914, 1.5222, 4.7147, 0.3549],\n",
       "         [1.6744, 1.3655, 4.9173, 5.0352, 1.6899, 1.1627, 0.1941, 1.0700],\n",
       "         [4.2327, 6.2283, 0.1999, 0.8074, 0.2348, 3.1774, 3.9142, 3.4625],\n",
       "         [2.6332, 3.2829, 5.9404, 1.4562, 1.1028, 2.6821, 5.4223, 3.7265],\n",
       "         [1.5549, 4.5315, 3.9697, 3.7680, 4.5322, 5.7528, 0.5089, 4.0359],\n",
       "         [1.0642, 4.0047, 3.9317, 0.2294, 1.9237, 1.4102, 4.9743, 2.5347],\n",
       "         [4.9772, 2.9403, 0.8832, 2.8796, 6.2385, 0.0933, 2.3724, 5.5047],\n",
       "         [0.5177, 2.0975, 2.1847, 4.3087, 2.8215, 0.7616, 5.9679, 5.9345]],\n",
       "\n",
       "        [[2.5493, 5.9101, 6.0390, 1.0026, 5.9397, 4.5870, 0.4594, 0.5701],\n",
       "         [0.9451, 1.3213, 4.4304, 6.0886, 4.6240, 1.1671, 1.5940, 0.6317],\n",
       "         [0.2708, 5.4701, 6.0213, 3.3811, 3.2066, 5.7253, 1.9331, 5.7300],\n",
       "         [1.8646, 5.7644, 5.8269, 1.3982, 6.1795, 0.2834, 1.7275, 0.1077],\n",
       "         [1.7839, 0.3296, 1.7925, 0.6204, 0.3223, 0.9301, 5.8468, 4.2750],\n",
       "         [5.2899, 0.1411, 2.7742, 1.4842, 2.3801, 0.9954, 4.5551, 1.9205],\n",
       "         [1.6103, 0.9675, 4.0165, 4.0223, 1.8282, 3.3419, 1.8666, 6.2453],\n",
       "         [0.7987, 5.9839, 0.2923, 0.9748, 2.0390, 0.1014, 5.0684, 6.0693]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_run(param, flow_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_new = make_u1_equiv_layers_net(lattice_shape = param_new.lat, nets = get_nets(flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0 torch.Size([2, 12, 12])\n",
      "1.9879761523473762 (-119.74963665923889, -118.76456592618388) 0.9988032924479914\n",
      "Traj:    1  ACCEPT:  dH:  0.0011222239  exp(-dH):  0.99887841   plaq:  0.93142052   topo:  0.0\n",
      "12.42193092625858 (-118.75965918089585, -118.93633117431767) 0.9910858034285454\n",
      "Traj:    2  ACCEPT:  dH: -0.0095857972  exp(-dH):  1.0096319    plaq:  0.844902     topo:  0.0\n",
      "15.478099801405712 (-118.87205347789688, -119.94980202022144) 0.9819081817417009\n",
      "Traj:    3  ACCEPT:  dH:  0.0056008684  exp(-dH):  0.99441479   plaq:  0.80854077   topo: -1.0\n",
      "17.494684446804182 (-119.8917686966347, -117.0291676196318) 0.9750739933170334\n",
      "Traj:    4  ACCEPT:  dH: -0.0034243272  exp(-dH):  1.0034302    plaq:  0.71301701   topo:  1.0\n",
      "25.98443996296935 (-117.08010794260822, -116.4797207140743) 0.9456847457252119\n",
      "Traj:    5  ACCEPT:  dH:  0.013018336  exp(-dH):  0.98706604   plaq:  0.74261923   topo:  0.0\n",
      "31.558932086620857 (-116.4636225607436, -121.09884990955004) 0.9263357056112453\n",
      "Traj:    6  REJECT:  dH:  0.82583608   exp(-dH):  0.43786874   plaq:  0.74261922   topo:  0.0\n",
      "32.670292514606835 (-116.40355948085673, -117.81373467633608) 0.9876844223314731\n",
      "Traj:    7  ACCEPT:  dH: -0.20551732   exp(-dH):  1.2281602    plaq:  0.72527966   topo:  0.0\n",
      "28.7132327814876 (-117.70277974611791, -119.84197929442055) 0.9388446326815996\n",
      "Traj:    8  ACCEPT:  dH:  0.092025131  exp(-dH):  0.91208223   plaq:  0.70775039   topo:  4.0\n",
      "29.343324296285036 (-119.79750532365998, -120.69295472566915) 0.9243833654276712\n",
      "Traj:    9  ACCEPT:  dH: -0.19609194   exp(-dH):  1.2166388    plaq:  0.73160175   topo:  1.0\n",
      "22.10966001724717 (-120.67296133583059, -120.40639517680121) 0.9596658419960157\n",
      "Traj:   10  ACCEPT:  dH: -0.34119592   exp(-dH):  1.4066288    plaq:  0.75286473   topo: -1.0\n",
      "21.76798527977922 (-120.1488626060403, -120.92892683946307) 0.9800464329784173\n",
      "Traj:   11  ACCEPT:  dH:  0.0039711172  exp(-dH):  0.99603676   plaq:  0.78678811   topo: -1.0\n",
      "20.36418205148782 (-120.74654447056923, -120.16032607378824) 0.9639823834649363\n",
      "Traj:   12  ACCEPT:  dH:  0.15448508   exp(-dH):  0.85685627   plaq:  0.72877147   topo:  3.0\n",
      "29.72414458907271 (-120.19027370698258, -119.4261336513294) 0.9689681983668214\n",
      "Traj:   13  ACCEPT:  dH:  0.05372897   exp(-dH):  0.94768892   plaq:  0.71100387   topo: -1.0\n",
      "25.570502850463008 (-119.60095635215472, -122.19148686851835) 0.9700452619696625\n",
      "Traj:   14  ACCEPT:  dH: -0.077125083  exp(-dH):  1.0801772    plaq:  0.73712136   topo:  0.0\n",
      "34.88021601442102 (-122.31009217682187, -124.16554732226616) 0.9351530618346771\n",
      "Traj:   15  ACCEPT:  dH:  0.17614107   exp(-dH):  0.83849968   plaq:  0.71057242   topo: -1.0\n",
      "39.284623171789825 (-124.17736836755948, -119.76807999467466) 0.9379221318084326\n",
      "Traj:   16  ACCEPT:  dH: -0.0105426    exp(-dH):  1.0105984    plaq:  0.7165462    topo:  1.0\n",
      "Run times:  [51.49649445101386, 51.50944151097792, 54.49907330199494, 49.76490359698073]\n",
      "Per trajectory:  [12.874123612753465, 12.87736037774448, 13.624768325498735, 12.441225899245183]\n"
     ]
    }
   ],
   "source": [
    "field_new = ft_run(param_new, flow_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.7165461986580746  topo: 1.0 torch.Size([2, 12, 12])\n",
      "33.205227499877566 (-119.69422351167141, -120.6127662698634) 0.9372124560816845\n",
      "Traj:    1  ACCEPT:  dH: -0.11566231   exp(-dH):  1.1226167    plaq:  0.70306027   topo:  1.0\n",
      "44.41642108291006 (-120.64624274028078, -121.32630719456144) 0.8803180973693999\n",
      "Traj:    2  ACCEPT:  dH: -0.03176109   exp(-dH):  1.0322709    plaq:  0.69111901   topo:  0.0\n",
      "37.033858167006905 (-121.26552062339381, -120.25376386065368) 0.9415684068308903\n",
      "Traj:    3  ACCEPT:  dH: -0.024179267  exp(-dH):  1.024474     plaq:  0.69309801   topo: -1.0\n",
      "32.78631415560145 (-120.32965204440877, -124.04080016816137) 0.9446849064872301\n",
      "Traj:    4  ACCEPT:  dH: -0.005851262  exp(-dH):  1.0058684    plaq:  0.69533788   topo:  0.0\n",
      "94.55484287198264 (-124.03655090387468, -119.75372667268061) 0.8925964152188036\n",
      "Traj:    5  REJECT:  dH:  14.360501    exp(-dH):  5.7984724e-07  plaq:  0.69533769   topo:  0.0\n",
      "30.22908038913341 (-123.95993080355423, -119.56816350696539) 0.9211626804152137\n",
      "Traj:    6  ACCEPT:  dH:  0.0016005495  exp(-dH):  0.99840073   plaq:  0.71598732   topo: -2.0\n",
      "36.03924407526037 (-119.67255308973843, -119.90863371831055) 0.9680088430386425\n",
      "Traj:    7  REJECT:  dH:  0.85995003   exp(-dH):  0.42318323   plaq:  0.71598726   topo: -2.0\n",
      "32.01314323041125 (-119.8526803715439, -121.08720712330579) 0.9393610421487225\n",
      "Traj:    8  ACCEPT:  dH:  0.65858789   exp(-dH):  0.5175817    plaq:  0.72518279   topo:  0.0\n",
      "43.42076890313657 (-121.0787077398429, -119.64379492112214) 0.9131935633738714\n",
      "Traj:    9  ACCEPT:  dH:  0.67577517   exp(-dH):  0.50876189   plaq:  0.69276674   topo:  1.0\n",
      "70.36167307078036 (-119.63341392692324, -120.68625560580203) 0.9712105337186091\n",
      "Traj:   10  REJECT:  dH:  5.5509295    exp(-dH):  0.0038838455  plaq:  0.69276674   topo:  1.0\n",
      "46.32827388254183 (-119.589768257599, -117.88039733704255) 0.9286727976444048\n",
      "Traj:   11  ACCEPT:  dH:  0.76011703   exp(-dH):  0.4676117    plaq:  0.68067812   topo: -3.0\n",
      "37.82249302452652 (-118.00799257816092, -121.50077579659583) 0.9602560236793249\n",
      "Traj:   12  REJECT:  dH:  0.74114928   exp(-dH):  0.47656589   plaq:  0.68067819   topo: -3.0\n",
      "35.82253164765983 (-118.09629892771338, -120.8583981646894) 0.9490029338463626\n",
      "Traj:   13  ACCEPT:  dH:  0.057694852  exp(-dH):  0.94393794   plaq:  0.67175112   topo:  1.0\n",
      "45.5484890167177 (-120.82360158762734, -119.21713523614113) 0.9393332414878042\n",
      "Traj:   14  ACCEPT:  dH:  1.2150792    exp(-dH):  0.29668653   plaq:  0.67047475   topo: -1.0\n",
      "33.77034402250193 (-119.27135605744579, -119.39808163495793) 0.9409229058428826\n",
      "Traj:   15  REJECT:  dH:  1.5050094    exp(-dH):  0.2220152    plaq:  0.67047475   topo: -1.0\n",
      "34.940075223085444 (-119.24935640380467, -120.9303303184922) 0.9116157576655861\n",
      "Traj:   16  ACCEPT:  dH: -0.11053899   exp(-dH):  1.1168799    plaq:  0.69494791   topo:  1.0\n",
      "Run times:  [50.31788488899474, 49.748376683011884, 49.79296660597902, 51.672306317981565]\n",
      "Per trajectory:  [12.579471222248685, 12.437094170752971, 12.448241651494754, 12.918076579495391]\n"
     ]
    }
   ],
   "source": [
    "field_new = ft_run(param_new, flow_new, field_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-100.9553)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action(param, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-200.1450)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action(param_new, field_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small volume, original\n",
      "v6/out_b5.0_l16.16_n128_t1.0_s32.out.topo_history\n",
      "[1, 0.01488095238095238, 0.0001989734216270108]\n",
      "[2, 0.02976190476190476, 0.0007227080601624759]\n",
      "[3, 0.04464285714285714, 0.0012784614102241273]\n",
      "[4, 0.05952380952380952, 0.002085790350848665]\n",
      "[5, 0.0744047619047619, 0.0031446948820360895]\n",
      "[6, 0.08928571428571429, 0.004089246872058566]\n",
      "[7, 0.109375, 0.00711811978277574]\n",
      "[8, 0.125, 0.00903696114115064]\n",
      "[9, 0.140625, 0.010749041669845194]\n",
      "[10, 0.15625, 0.01281917144018132]\n",
      "Small volume, Field Transformation\n",
      "v6/out_b5.0_l16.16_n128_t1.0_s64_ft.out.topo_history\n",
      "[1, 0.13690476190476192, 0.0025523487188016565]\n",
      "[2, 0.24702380952380948, 0.006309973221481873]\n",
      "[3, 0.36011904761904756, 0.01309794006503323]\n",
      "[4, 0.47321428571428564, 0.025942017488680285]\n",
      "[5, 0.5744047619047619, 0.03457792139745722]\n",
      "[6, 0.6785714285714286, 0.044240711125895386]\n",
      "[7, 0.765625, 0.05573212949298342]\n",
      "[8, 0.8250000000000001, 0.059385744641847066]\n",
      "[9, 0.8968750000000001, 0.07830436055759851]\n",
      "[10, 1.0, 0.11150964550955521]\n",
      "Large volume, original\n",
      "v6/out_b5.0_l32.32_n128_t1.0_s32.out.topo_history\n",
      "[1, 0.10714285714285714, 0.0017930478454663964]\n",
      "[2, 0.1845238095238095, 0.0047479175091686724]\n",
      "[3, 0.25595238095238093, 0.009212240716248273]\n",
      "[4, 0.3244047619047619, 0.01589729027275118]\n",
      "[5, 0.3928571428571429, 0.0247733345179745]\n",
      "[6, 0.4672619047619047, 0.04034128947217063]\n",
      "[7, 0.5375, 0.1266384867056779]\n",
      "[8, 0.571875, 0.12845142617567998]\n",
      "[9, 0.60625, 0.13070310204036953]\n",
      "[10, 0.634375, 0.13230423773362587]\n",
      "Large volume, same field Transformation\n",
      "v6/out_b5.0_l32.32_n128_t1.0_s64_ft.out.topo_history\n",
      "[1, 0.4196428571428571, 0.016629146536206847]\n",
      "[2, 0.7529761904761904, 0.048464893996528605]\n",
      "[3, 1.1339285714285714, 0.08974158725542851]\n",
      "[4, 1.5357142857142856, 0.19185611946490447]\n",
      "[5, 2.0654761904761907, 0.4538881063919147]\n",
      "[6, 2.520833333333334, 0.6519718652486624]\n",
      "[7, 2.909375, 1.0726726629080912]\n",
      "[8, 3.3375, 1.5222841729421297]\n",
      "[9, 3.6781249999999996, 1.7908488403821676]\n",
      "[10, 4.062499999999999, 2.169153078911814]\n"
     ]
    }
   ],
   "source": [
    "def print_topo_change_sqr(topo_history):\n",
    "    size_hist = len(topo_history)\n",
    "    drop_len = size_hist // 3 # ADJUST ME\n",
    "    dt = change_sqr_vs_dt(topo_history[drop_len:])\n",
    "    for l in dt:\n",
    "        print(l)\n",
    "\n",
    "def non_empty(str):\n",
    "    return not(str == \"\" or str == \"\\n\")\n",
    "        \n",
    "fns = {\n",
    "    \"v6/out_b5.0_l16.16_n128_t1.0_s32.out.topo_history\":\"Small volume, original\",\n",
    "    \"v6/out_b5.0_l16.16_n128_t1.0_s64_ft.out.topo_history\":\"Small volume, Field Transformation\",\n",
    "    \"v6/out_b5.0_l32.32_n128_t1.0_s32.out.topo_history\":\"Large volume, original\",\n",
    "    \"v6/out_b5.0_l32.32_n128_t1.0_s64_ft.out.topo_history\":\"Large volume, same field Transformation\",\n",
    "}\n",
    "\n",
    "for (fn, description) in fns.items():\n",
    "    print(description)\n",
    "    print(fn)\n",
    "    with open(fn, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = list(filter(non_empty, lines))\n",
    "    topo_history = list(map(float, lines))\n",
    "    print_topo_change_sqr(topo_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
