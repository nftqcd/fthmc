{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Xiao-Yong\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from functools import reduce\n",
    "\n",
    "class Param:\n",
    "    def __init__(self, beta = 6.0, lat = [64, 64], tau = 2.0, nstep = 50, ntraj = 256, nrun = 4, nprint = 256, seed = 11*13, randinit = False, nth = int(os.environ.get('OMP_NUM_THREADS', '2')), nth_interop = 2):\n",
    "        self.beta = beta\n",
    "        self.lat = lat\n",
    "        self.nd = len(lat)\n",
    "        self.volume = reduce(lambda x,y:x*y, lat)\n",
    "        self.tau = tau\n",
    "        self.nstep = nstep\n",
    "        self.dt = self.tau / self.nstep\n",
    "        self.ntraj = ntraj\n",
    "        self.nrun = nrun\n",
    "        self.nprint = nprint\n",
    "        self.seed = seed\n",
    "        self.randinit = randinit\n",
    "        self.nth = nth\n",
    "        self.nth_interop = nth_interop\n",
    "    def initializer(self):\n",
    "        if self.randinit:\n",
    "            return torch.empty((param.nd,) + param.lat).uniform_(-math.pi, math.pi)\n",
    "        else:\n",
    "            return torch.zeros((param.nd,) + param.lat)\n",
    "    def summary(self):\n",
    "        return f\"\"\"latsize = {self.lat}\n",
    "volume = {self.volume}\n",
    "beta = {self.beta}\n",
    "trajs = {self.ntraj}\n",
    "tau = {self.tau}\n",
    "steps = {self.nstep}\n",
    "seed = {self.seed}\n",
    "nth = {self.nth}\n",
    "nth_interop = {self.nth_interop}\n",
    "\"\"\"\n",
    "    def uniquestr(self):\n",
    "        lat = \".\".join(str(x) for x in self.lat)\n",
    "        return f\"out_l{lat}_b{param.beta}_n{param.ntraj}_t{param.tau}_s{param.nstep}.out\"\n",
    "\n",
    "def action(param, f):\n",
    "    return (-param.beta)*torch.sum(torch.cos(plaqphase(f)))\n",
    "\n",
    "def force(param, f):\n",
    "    f.requires_grad_(True)\n",
    "    s = action(param, f)\n",
    "    s.backward()\n",
    "    ff = f.grad\n",
    "    f.requires_grad_(False)\n",
    "    return ff\n",
    "\n",
    "plaqphase = lambda f: f[0,:] - f[1,:] - torch.roll(f[0,:], shifts=-1, dims=1) + torch.roll(f[1,:], shifts=-1, dims=0)\n",
    "topocharge = lambda f: torch.floor(0.1 + torch.sum(regularize(plaqphase(f))) / (2*math.pi))\n",
    "def regularize(f):\n",
    "    p2 = 2*math.pi\n",
    "    f_ = (f - math.pi) / p2\n",
    "    return p2*(f_ - torch.floor(f_) - 0.5)\n",
    "\n",
    "def leapfrog(param, x, p):\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = force(param, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    print(f'plaq(x) {action(param, x) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(f)}')\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        p_ = p_ + (-dt)*force(param, x_)\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    return (x_, p_)\n",
    "def hmc(param, x):\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = action(param, x) + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = leapfrog(param, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = action(param, xr) + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    newx = xr if acc else x\n",
    "    return (dH, exp_mdH, acc, newx)\n",
    "\n",
    "put = lambda s: sys.stdout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ BEGIN CODE FROM https://arxiv.org/abs/2101.08176\n",
    "# Introduction to Normalizing Flows for Lattice Field Theory\n",
    "# Michael S. Albergo, Denis Boyda, Daniel C. Hackett, Gurtej Kanwar, Kyle Cranmer, Sébastien Racanière, Danilo Jimenez Rezende, Phiala E. Shanahan\n",
    "# License: CC BY 4.0\n",
    "# With slight modifications by Xiao-Yong Jin to reduce global variables\n",
    "\n",
    "import numpy as np\n",
    "import packaging.version\n",
    "torch_device = 'cpu'\n",
    "float_dtype = np.float64\n",
    "def torch_mod(x):\n",
    "    return torch.remainder(x, 2*np.pi)\n",
    "def torch_wrap(x):\n",
    "    return torch_mod(x+np.pi) - np.pi\n",
    "def grab(var):\n",
    "    return var.detach().cpu().numpy()\n",
    "def compute_ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*torch.logsumexp(logw, dim=0) - torch.logsumexp(2*logw, dim=0)\n",
    "    ess_per_cfg = torch.exp(log_ess) / len(logw)\n",
    "    return ess_per_cfg\n",
    "def bootstrap(x, *, Nboot, binsize):\n",
    "    boots = []\n",
    "    x = x.reshape(-1, binsize, *x.shape[1:])\n",
    "    for i in range(Nboot):\n",
    "        boots.append(np.mean(x[np.random.randint(len(x), size=len(x))], axis=(0,1)))\n",
    "    return np.mean(boots), np.std(boots)\n",
    "def print_metrics(history, avg_last_N_epochs, era, epoch):\n",
    "    print(f'== Era {era} | Epoch {epoch} metrics ==')\n",
    "    for key, val in history.items():\n",
    "        avgd = np.mean(val[-avg_last_N_epochs:])\n",
    "        print(f'\\t{key} {avgd:g}')\n",
    "def serial_sample_generator(model, action, batch_size, N_samples):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    layers.eval()\n",
    "    x, logq, logp = None, None, None\n",
    "    for i in range(N_samples):\n",
    "        batch_i = i % batch_size\n",
    "        if batch_i == 0:\n",
    "            # we're out of samples to propose, generate a new batch\n",
    "            x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size)\n",
    "            logp = -action(x)\n",
    "        yield x[batch_i], logq[batch_i], logp[batch_i]\n",
    "def make_mcmc_ensemble(model, action, batch_size, N_samples):\n",
    "    history = {\n",
    "        'x' : [],\n",
    "        'logq' : [],\n",
    "        'logp' : [],\n",
    "        'accepted' : []\n",
    "    }\n",
    "\n",
    "    # build Markov chain\n",
    "    sample_gen = serial_sample_generator(model, action, batch_size, N_samples)\n",
    "    for new_x, new_logq, new_logp in sample_gen:\n",
    "        if len(history['logp']) == 0:\n",
    "            # always accept first proposal, Markov chain must start somewhere\n",
    "            accepted = True\n",
    "        else: \n",
    "            # Metropolis acceptance condition\n",
    "            last_logp = history['logp'][-1]\n",
    "            last_logq = history['logq'][-1]\n",
    "            p_accept = torch.exp((new_logp - new_logq) - (last_logp - last_logq))\n",
    "            p_accept = min(1, p_accept)\n",
    "            draw = torch.rand(1) # ~ [0,1]\n",
    "            if draw < p_accept:\n",
    "                accepted = True\n",
    "            else:\n",
    "                accepted = False\n",
    "                new_x = history['x'][-1]\n",
    "                new_logp = last_logp\n",
    "                new_logq = last_logq\n",
    "        # Update Markov chain\n",
    "        history['logp'].append(new_logp)\n",
    "        history['logq'].append(new_logq)\n",
    "        history['x'].append(new_x)\n",
    "        history['accepted'].append(accepted)\n",
    "    return history\n",
    "def make_conv_net(*, hidden_sizes, kernel_size, in_channels, out_channels, use_final_tanh):\n",
    "    sizes = [in_channels] + hidden_sizes + [out_channels]\n",
    "    assert packaging.version.parse(torch.__version__) >= packaging.version.parse('1.5.0')\n",
    "    assert kernel_size % 2 == 1, 'kernel size must be odd for PyTorch >= 1.5.0'\n",
    "    padding_size = (kernel_size // 2)\n",
    "    net = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        net.append(torch.nn.Conv2d(\n",
    "            sizes[i], sizes[i+1], kernel_size, padding=padding_size,\n",
    "            stride=1, padding_mode='circular'))\n",
    "        if i != len(sizes) - 2:\n",
    "            net.append(torch.nn.SiLU())\n",
    "        else:\n",
    "            if use_final_tanh:\n",
    "                net.append(torch.nn.Tanh())\n",
    "    return torch.nn.Sequential(*net)\n",
    "def set_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight is not None:\n",
    "        torch.nn.init.normal_(m.weight, mean=1, std=2)\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        m.bias.data.fill_(-1)\n",
    "def calc_dkl(logp, logq):\n",
    "    return (logq - logp).mean()  # reverse KL, assuming samples from q\n",
    "def apply_flow_to_prior(prior, coupling_layers, *, batch_size):\n",
    "    x = prior.sample_n(batch_size)\n",
    "    logq = prior.log_prob(x)\n",
    "    for layer in coupling_layers:\n",
    "        x, logJ = layer.forward(x)\n",
    "        logq = logq - logJ\n",
    "    return x, logq\n",
    "def train_step(model, action, optimizer, metrics, batch_size):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size)\n",
    "    logp = -action(x)\n",
    "    loss = calc_dkl(logp, logq)\n",
    "    loss.backward()\n",
    "    \n",
    "    # minimization target\n",
    "    # loss mini\n",
    "    # -> (logq - logp) mini\n",
    "    # -> (action - logJ) mini\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    metrics['loss'].append(grab(loss))\n",
    "    metrics['logp'].append(grab(logp))\n",
    "    metrics['logq'].append(grab(logq))\n",
    "    metrics['ess'].append(grab( compute_ess(logp, logq) ))\n",
    "def compute_u1_plaq(links, mu, nu):\n",
    "    \"\"\"Compute U(1) plaqs in the (mu,nu) plane given `links` = arg(U)\"\"\"\n",
    "    return (links[:,mu] + torch.roll(links[:,nu], -1, mu+1)\n",
    "            - torch.roll(links[:,mu], -1, nu+1) - links[:,nu])\n",
    "class U1GaugeAction:\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "    def __call__(self, cfgs):\n",
    "        Nd = cfgs.shape[1]\n",
    "        action_density = 0\n",
    "        for mu in range(Nd):\n",
    "            for nu in range(mu+1,Nd):\n",
    "                action_density = action_density + torch.cos(\n",
    "                    compute_u1_plaq(cfgs, mu, nu))\n",
    "        return -self.beta * torch.sum(action_density, dim=tuple(range(1,Nd+1)))\n",
    "def gauge_transform(links, alpha):\n",
    "    for mu in range(len(links.shape[2:])):\n",
    "        links[:,mu] = alpha + links[:,mu] - torch.roll(alpha, -1, mu+1)\n",
    "    return links\n",
    "def random_gauge_transform(x):\n",
    "    Nconf, VolShape = x.shape[0], x.shape[2:]\n",
    "    return gauge_transform(x, 2*np.pi*torch.rand((Nconf,) + VolShape))\n",
    "def topo_charge(x):\n",
    "    P01 = torch_wrap(compute_u1_plaq(x, mu=0, nu=1))\n",
    "    axes = tuple(range(1, len(P01.shape)))\n",
    "    return torch.sum(P01, dim=axes) / (2*np.pi)\n",
    "class MultivariateUniform(torch.nn.Module):\n",
    "    \"\"\"Uniformly draw samples from [a,b]\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        super().__init__()\n",
    "        self.dist = torch.distributions.uniform.Uniform(a, b)\n",
    "    def log_prob(self, x):\n",
    "        axes = range(1, len(x.shape))\n",
    "        return torch.sum(self.dist.log_prob(x), dim=tuple(axes))\n",
    "    def sample_n(self, batch_size):\n",
    "        return self.dist.sample((batch_size,))\n",
    "class GaugeEquivCouplingLayer(torch.nn.Module):\n",
    "    \"\"\"U(1) gauge equiv coupling layer defined by `plaq_coupling` acting on plaquettes.\"\"\"\n",
    "    def __init__(self, *, lattice_shape, mask_mu, mask_off, plaq_coupling):\n",
    "        super().__init__()\n",
    "        link_mask_shape = (len(lattice_shape),) + lattice_shape\n",
    "        self.active_mask = make_2d_link_active_stripes(link_mask_shape, mask_mu, mask_off)\n",
    "        self.plaq_coupling = plaq_coupling\n",
    "\n",
    "    def forward(self, x):\n",
    "        plaq = compute_u1_plaq(x, mu=0, nu=1)\n",
    "        new_plaq, logJ = self.plaq_coupling(plaq)\n",
    "        delta_plaq = new_plaq - plaq\n",
    "        delta_links = torch.stack((delta_plaq, -delta_plaq), dim=1) # signs for U vs Udagger\n",
    "        fx = self.active_mask * torch_mod(delta_links + x) + (1-self.active_mask) * x\n",
    "        return fx, logJ\n",
    "\n",
    "    def reverse(self, fx):\n",
    "        new_plaq = compute_u1_plaq(fx, mu=0, nu=1)\n",
    "        plaq, logJ = self.plaq_coupling.reverse(new_plaq)\n",
    "        delta_plaq = plaq - new_plaq\n",
    "        delta_links = torch.stack((delta_plaq, -delta_plaq), dim=1) # signs for U vs Udagger\n",
    "        x = self.active_mask * torch_mod(delta_links + fx) + (1-self.active_mask) * fx\n",
    "        return x, logJ\n",
    "def make_2d_link_active_stripes(shape, mu, off):\n",
    "    \"\"\"\n",
    "    Stripes mask looks like in the `mu` channel (mu-oriented links)::\n",
    "\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "\n",
    "    where vertical is the `mu` direction, and the pattern is offset in the nu\n",
    "    direction by `off` (mod 4). The other channel is identically 0.\n",
    "    \"\"\"\n",
    "    assert len(shape) == 2+1, 'need to pass shape suitable for 2D gauge theory'\n",
    "    assert shape[0] == len(shape[1:]), 'first dim of shape must be Nd'\n",
    "    assert mu in (0,1), 'mu must be 0 or 1'\n",
    "\n",
    "    mask = np.zeros(shape).astype(np.uint8)\n",
    "    if mu == 0:\n",
    "        mask[mu,:,0::4] = 1\n",
    "    elif mu == 1:\n",
    "        mask[mu,0::4] = 1\n",
    "    nu = 1-mu\n",
    "    mask = np.roll(mask, off, axis=nu+1)\n",
    "    return torch.from_numpy(mask.astype(float_dtype)).to(torch_device)\n",
    "def make_single_stripes(shape, mu, off):\n",
    "    \"\"\"\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "      1 0 0 0 1 0 0 0 1 0 0\n",
    "\n",
    "    where vertical is the `mu` direction. Vector of 1 is repeated every 4.\n",
    "    The pattern is offset in perpendicular to the mu direction by `off` (mod 4).\n",
    "    \"\"\"\n",
    "    assert len(shape) == 2, 'need to pass 2D shape'\n",
    "    assert mu in (0,1), 'mu must be 0 or 1'\n",
    "\n",
    "    mask = np.zeros(shape).astype(np.uint8)\n",
    "    if mu == 0:\n",
    "        mask[:,0::4] = 1\n",
    "    elif mu == 1:\n",
    "        mask[0::4] = 1\n",
    "    mask = np.roll(mask, off, axis=1-mu)\n",
    "    return torch.from_numpy(mask).to(torch_device)\n",
    "def make_double_stripes(shape, mu, off):\n",
    "    \"\"\"\n",
    "    Double stripes mask looks like::\n",
    "\n",
    "      1 1 0 0 1 1 0 0\n",
    "      1 1 0 0 1 1 0 0\n",
    "      1 1 0 0 1 1 0 0\n",
    "      1 1 0 0 1 1 0 0\n",
    "\n",
    "    where vertical is the `mu` direction. The pattern is offset in perpendicular\n",
    "    to the mu direction by `off` (mod 4).\n",
    "    \"\"\"\n",
    "    assert len(shape) == 2, 'need to pass 2D shape'\n",
    "    assert mu in (0,1), 'mu must be 0 or 1'\n",
    "\n",
    "    mask = np.zeros(shape).astype(np.uint8)\n",
    "    if mu == 0:\n",
    "        mask[:,0::4] = 1\n",
    "        mask[:,1::4] = 1\n",
    "    elif mu == 1:\n",
    "        mask[0::4] = 1\n",
    "        mask[1::4] = 1\n",
    "    mask = np.roll(mask, off, axis=1-mu)\n",
    "    return torch.from_numpy(mask).to(torch_device)\n",
    "def make_plaq_masks(mask_shape, mask_mu, mask_off):\n",
    "    mask = {}\n",
    "    mask['frozen'] = make_double_stripes(mask_shape, mask_mu, mask_off+1)\n",
    "    mask['active'] = make_single_stripes(mask_shape, mask_mu, mask_off)\n",
    "    mask['passive'] = 1 - mask['frozen'] - mask['active']\n",
    "    return mask\n",
    "def tan_transform(x, s):\n",
    "    return torch_mod(2*torch.atan(torch.exp(s)*torch.tan(x/2)))\n",
    "\n",
    "def tan_transform_logJ(x, s):\n",
    "    return -torch.log(torch.exp(-s)*torch.cos(x/2)**2 + torch.exp(s)*torch.sin(x/2)**2)\n",
    "def mixture_tan_transform(x, s):\n",
    "    assert len(x.shape) == len(s.shape), \\\n",
    "        f'Dimension mismatch between x and s {x.shape} vs {s.shape}'\n",
    "    return torch.mean(tan_transform(x, s), dim=1, keepdim=True)\n",
    "\n",
    "def mixture_tan_transform_logJ(x, s):\n",
    "    assert len(x.shape) == len(s.shape), \\\n",
    "        f'Dimension mismatch between x and s {x.shape} vs {s.shape}'\n",
    "    return torch.logsumexp(tan_transform_logJ(x, s), dim=1) - np.log(s.shape[1])\n",
    "def invert_transform_bisect(y, *, f, tol, max_iter, a=0, b=2*np.pi):\n",
    "    min_x = a*torch.ones_like(y)\n",
    "    max_x = b*torch.ones_like(y)\n",
    "    min_val = f(min_x)\n",
    "    max_val = f(max_x)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_iter):\n",
    "            mid_x = (min_x + max_x) / 2\n",
    "            mid_val = f(mid_x)\n",
    "            greater_mask = (y > mid_val).int()\n",
    "            greater_mask = greater_mask.float()\n",
    "            err = torch.max(torch.abs(y - mid_val))\n",
    "            if err < tol: return mid_x\n",
    "            if torch.all((mid_x == min_x) + (mid_x == max_x)):\n",
    "                print('WARNING: Reached floating point precision before tolerance '\n",
    "                      f'(iter {i}, err {err})')\n",
    "                return mid_x\n",
    "            min_x = greater_mask*mid_x + (1-greater_mask)*min_x\n",
    "            min_val = greater_mask*mid_val + (1-greater_mask)*min_val\n",
    "            max_x = (1-greater_mask)*mid_x + greater_mask*max_x\n",
    "            max_val = (1-greater_mask)*mid_val + greater_mask*max_val\n",
    "        print(f'WARNING: Did not converge to tol {tol} in {max_iter} iters! Error was {err}')\n",
    "        return mid_x\n",
    "def stack_cos_sin(x):\n",
    "    return torch.stack((torch.cos(x), torch.sin(x)), dim=1)\n",
    "class NCPPlaqCouplingLayer(torch.nn.Module):\n",
    "    def __init__(self, net, *, mask_shape, mask_mu, mask_off,\n",
    "                 inv_prec=1e-6, inv_max_iter=1000):\n",
    "        super().__init__()\n",
    "        assert len(mask_shape) == 2, (\n",
    "            f'NCPPlaqCouplingLayer is implemented only in 2D, '\n",
    "            f'mask shape {mask_shape} is invalid')\n",
    "        self.mask = make_plaq_masks(mask_shape, mask_mu, mask_off)\n",
    "        self.net = net\n",
    "        self.inv_prec = inv_prec\n",
    "        self.inv_max_iter = inv_max_iter\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = self.mask['frozen'] * x\n",
    "        net_out = self.net(stack_cos_sin(x2))\n",
    "        assert net_out.shape[1] >= 2, 'CNN must output n_mix (s_i) + 1 (t) channels'\n",
    "        s, t = net_out[:,:-1], net_out[:,-1]\n",
    "\n",
    "        x1 = self.mask['active'] * x\n",
    "        x1 = x1.unsqueeze(1)\n",
    "        local_logJ = self.mask['active'] * mixture_tan_transform_logJ(x1, s)\n",
    "        axes = tuple(range(1, len(local_logJ.shape)))\n",
    "        logJ = torch.sum(local_logJ, dim=axes)\n",
    "        fx1 = self.mask['active'] * mixture_tan_transform(x1, s).squeeze(1)\n",
    "\n",
    "        fx = (\n",
    "            self.mask['active'] * torch_mod(fx1 + t) +\n",
    "            self.mask['passive'] * x +\n",
    "            self.mask['frozen'] * x)\n",
    "        return fx, logJ\n",
    "\n",
    "    def reverse(self, fx):\n",
    "        fx2 = self.mask['frozen'] * fx\n",
    "        net_out = self.net(stack_cos_sin(fx2))\n",
    "        assert net_out.shape[1] >= 2, 'CNN must output n_mix (s_i) + 1 (t) channels'\n",
    "        s, t = net_out[:,:-1], net_out[:,-1]\n",
    "\n",
    "        x1 = torch_mod(self.mask['active'] * (fx - t).unsqueeze(1))\n",
    "        transform = lambda x: self.mask['active'] * mixture_tan_transform(x, s)\n",
    "        x1 = invert_transform_bisect(\n",
    "            x1, f=transform, tol=self.inv_prec, max_iter=self.inv_max_iter)\n",
    "        local_logJ = self.mask['active'] * mixture_tan_transform_logJ(x1, s)\n",
    "        axes = tuple(range(1, len(local_logJ.shape)))\n",
    "        logJ = -torch.sum(local_logJ, dim=axes)\n",
    "        x1 = x1.squeeze(1)\n",
    "\n",
    "        x = (\n",
    "            self.mask['active'] * x1 +\n",
    "            self.mask['passive'] * fx +\n",
    "            self.mask['frozen'] * fx2)\n",
    "        return x, logJ\n",
    "def make_u1_equiv_layers(*, n_layers, n_mixture_comps, lattice_shape, hidden_sizes, kernel_size):\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        # periodically loop through all arrangements of maskings\n",
    "        mu = i % 2\n",
    "        off = (i//2) % 4\n",
    "        in_channels = 2 # x - > (cos(x), sin(x))\n",
    "        out_channels = n_mixture_comps + 1 # for mixture s and t, respectively\n",
    "        net = make_conv_net(in_channels=in_channels, out_channels=out_channels,\n",
    "            hidden_sizes=hidden_sizes, kernel_size=kernel_size,\n",
    "            use_final_tanh=False)\n",
    "        plaq_coupling = NCPPlaqCouplingLayer(\n",
    "            net, mask_shape=lattice_shape, mask_mu=mu, mask_off=off)\n",
    "        link_coupling = GaugeEquivCouplingLayer(\n",
    "            lattice_shape=lattice_shape, mask_mu=mu, mask_off=off, \n",
    "            plaq_coupling=plaq_coupling)\n",
    "        layers.append(link_coupling)\n",
    "    return torch.nn.ModuleList(layers)\n",
    "\n",
    "def flow_train(param):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    # Theory\n",
    "    lattice_shape = param.lat\n",
    "    link_shape = (2,*param.lat)\n",
    "    beta = param.beta\n",
    "    u1_action = U1GaugeAction(beta)\n",
    "\n",
    "    # Model\n",
    "    prior = MultivariateUniform(torch.zeros(link_shape), 2*np.pi*torch.ones(link_shape))\n",
    "\n",
    "    n_layers = 16\n",
    "    n_s_nets = 2\n",
    "    hidden_sizes = [8,8]\n",
    "    kernel_size = 3\n",
    "    layers = make_u1_equiv_layers(lattice_shape=lattice_shape, n_layers=n_layers, n_mixture_comps=n_s_nets,\n",
    "                                  hidden_sizes=hidden_sizes, kernel_size=kernel_size)\n",
    "    set_weights(layers)\n",
    "    model = {'layers': layers, 'prior': prior}\n",
    "\n",
    "    # Training\n",
    "    base_lr = .001\n",
    "    optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)\n",
    "\n",
    "    # ADJUST ME\n",
    "    N_era = 10\n",
    "    #\n",
    "    N_epoch = 100\n",
    "    batch_size = 64\n",
    "    print_freq = N_epoch # epochs\n",
    "    plot_freq = 1 # epochs\n",
    "\n",
    "    history = {\n",
    "        'loss' : [],\n",
    "        'logp' : [],\n",
    "        'logq' : [],\n",
    "        'ess' : []\n",
    "    }\n",
    "\n",
    "    for era in range(N_era):\n",
    "        for epoch in range(N_epoch):\n",
    "            train_step(model, u1_action, optimizer, history, batch_size)\n",
    "            if epoch % print_freq == 0:\n",
    "                print_metrics(history, print_freq, era, epoch)\n",
    "    return model,u1_action\n",
    "\n",
    "def flow_eval(model, u1_action):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    ensemble_size = 1024\n",
    "    u1_ens = make_mcmc_ensemble(model, u1_action, 64, ensemble_size)\n",
    "    print(\"Accept rate:\", np.mean(u1_ens['accepted']))\n",
    "    Q = grab(topo_charge(torch.stack(u1_ens['x'], axis=0)))\n",
    "    X_mean, X_err = bootstrap(Q**2, Nboot=100, binsize=16)\n",
    "    print(f'Topological susceptibility = {X_mean:.2f} +/- {X_err:.2f}')\n",
    "    print(f'... vs HMC estimate = 1.23 +/- 0.02')\n",
    "\n",
    "# ------ END CODE FROM https://arxiv.org/abs/2101.08176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 2, # 0.3\n",
    "    nstep = 8, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 2, # 2**16 # 2**10 # 2**15\n",
    "    #\n",
    "    nprint = 2,\n",
    "    seed = 1331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(param.seed)\n",
    "\n",
    "torch.set_num_threads(param.nth)\n",
    "torch.set_num_interop_threads(param.nth_interop)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(param.nth)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"0\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 2\n",
      "tau = 2\n",
      "steps = 8\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0\n",
      "plaq(x) 1.0  force.norm 9.246826593536493\n",
      "Traj:    1  ACCEPT:  dH: -2.1286122    exp(-dH):  8.4031965    plaq:  0.87165922   topo:  0.0\n",
      "plaq(x) 0.8716592163190823  force.norm 13.60065719819702\n",
      "Traj:    2  ACCEPT:  dH: -0.43065065   exp(-dH):  1.5382581    plaq:  0.85113562   topo:  0.0\n",
      "plaq(x) 0.851135619433136  force.norm 17.54309727438331\n",
      "Traj:    3  ACCEPT:  dH: -0.52286113   exp(-dH):  1.686847     plaq:  0.81229714   topo:  0.0\n",
      "plaq(x) 0.8122971355705815  force.norm 17.226420992986522\n",
      "Traj:    4  ACCEPT:  dH:  0.039753441  exp(-dH):  0.96102636   plaq:  0.78167924   topo:  0.0\n",
      "plaq(x) 0.7816792404187667  force.norm 17.04668749206334\n",
      "Traj:    5  ACCEPT:  dH:  0.42993024   exp(-dH):  0.65055447   plaq:  0.80392524   topo:  0.0\n",
      "plaq(x) 0.803925239737245  force.norm 17.257422008841356\n",
      "Traj:    6  ACCEPT:  dH: -0.96340329   exp(-dH):  2.6206       plaq:  0.78202049   topo:  0.0\n",
      "plaq(x) 0.7820204923371585  force.norm 18.801543612929564\n",
      "Traj:    7  ACCEPT:  dH: -0.74395609   exp(-dH):  2.1042436    plaq:  0.69283802   topo:  0.0\n",
      "plaq(x) 0.6928380205992224  force.norm 20.552012705600852\n",
      "Traj:    8  ACCEPT:  dH: -0.49108779   exp(-dH):  1.6340928    plaq:  0.66276864   topo:  0.0\n",
      "Run times:  [0.026500010979361832, 0.019916662015020847, 0.01629374804906547, 0.013835720950737596]\n",
      "Per trajectory:  [0.013250005489680916, 0.009958331007510424, 0.008146874024532735, 0.006917860475368798]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "def run(param, field = param.initializer()):\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                dH, exp_mdH, acc, field = hmc(param, field)\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field\n",
    "field = run(param)\n",
    "field = torch.reshape(field,(1,)+field.shape)\n",
    "field_run = field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Era 0 | Epoch 0 metrics ==\n",
      "\tloss -235.345\n",
      "\tlogp 0.326149\n",
      "\tlogq -235.019\n",
      "\tess 0.0449843\n",
      "== Era 1 | Epoch 0 metrics ==\n",
      "\tloss -276.304\n",
      "\tlogp 64.6121\n",
      "\tlogq -211.692\n",
      "\tess 0.0456754\n",
      "== Era 2 | Epoch 0 metrics ==\n",
      "\tloss -283.352\n",
      "\tlogp 77.9836\n",
      "\tlogq -205.368\n",
      "\tess 0.06757\n",
      "== Era 3 | Epoch 0 metrics ==\n",
      "\tloss -285.1\n",
      "\tlogp 82.214\n",
      "\tlogq -202.886\n",
      "\tess 0.111624\n",
      "== Era 4 | Epoch 0 metrics ==\n",
      "\tloss -285.424\n",
      "\tlogp 83.0325\n",
      "\tlogq -202.391\n",
      "\tess 0.114429\n",
      "== Era 5 | Epoch 0 metrics ==\n",
      "\tloss -285.852\n",
      "\tlogp 83.8965\n",
      "\tlogq -201.955\n",
      "\tess 0.132878\n",
      "== Era 6 | Epoch 0 metrics ==\n",
      "\tloss -286.162\n",
      "\tlogp 84.5777\n",
      "\tlogq -201.584\n",
      "\tess 0.152924\n",
      "== Era 7 | Epoch 0 metrics ==\n",
      "\tloss -286.305\n",
      "\tlogp 84.8143\n",
      "\tlogq -201.491\n",
      "\tess 0.177722\n",
      "== Era 8 | Epoch 0 metrics ==\n",
      "\tloss -286.367\n",
      "\tlogp 84.8838\n",
      "\tlogq -201.483\n",
      "\tess 0.170423\n",
      "== Era 9 | Epoch 0 metrics ==\n",
      "\tloss -286.457\n",
      "\tlogp 85.0582\n",
      "\tlogq -201.399\n",
      "\tess 0.195936\n",
      "Accept rate: 0.2861328125\n",
      "Topological susceptibility = 1.23 +/- 0.10\n",
      "... vs HMC estimate = 1.23 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "flow_model,flow_act = flow_train(param)\n",
    "flow_eval(flow_model,flow_act)\n",
    "flow = flow_model['layers']\n",
    "flow.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 2\n",
      "tau = 2\n",
      "steps = 8\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.6627686419055754  topo: 0.0\n",
      "plaq(x) 0.6627686419055754  force.norm 20.929551567391623\n",
      "Traj:    1  ACCEPT:  dH:  0.016237397  exp(-dH):  0.98389372   plaq:  0.67254684   topo:  1.0\n",
      "plaq(x) 0.6725468370615701  force.norm 20.88726225547214\n",
      "Traj:    2  REJECT:  dH:  0.84646424   exp(-dH):  0.42892884   plaq:  0.67254684   topo:  1.0\n",
      "plaq(x) 0.6725468370615701  force.norm 21.800052664051226\n",
      "Traj:    3  ACCEPT:  dH: -0.41449009   exp(-dH):  1.5135987    plaq:  0.65561512   topo:  2.0\n",
      "plaq(x) 0.6556151150106684  force.norm 20.891489602965283\n",
      "Traj:    4  ACCEPT:  dH: -0.28651909   exp(-dH):  1.3317836    plaq:  0.67368747   topo:  0.0\n",
      "plaq(x) 0.673687468122236  force.norm 23.08817340551291\n",
      "Traj:    5  ACCEPT:  dH:  0.082586681  exp(-dH):  0.92073162   plaq:  0.64250383   topo:  0.0\n",
      "plaq(x) 0.6425038251174207  force.norm 20.203348778536572\n",
      "Traj:    6  REJECT:  dH:  0.28453241   exp(-dH):  0.75236597   plaq:  0.64250383   topo:  0.0\n",
      "plaq(x) 0.6425038251174207  force.norm 20.07458903839735\n",
      "Traj:    7  ACCEPT:  dH: -0.36879065   exp(-dH):  1.4459849    plaq:  0.59343367   topo:  0.0\n",
      "plaq(x) 0.5934336728805107  force.norm 21.198988484278136\n",
      "Traj:    8  ACCEPT:  dH:  0.066065349  exp(-dH):  0.93606969   plaq:  0.64192323   topo:  1.0\n",
      "Run times:  [0.01633048802614212, 0.013957190094515681, 0.0126242641126737, 0.014662801986560225]\n",
      "Per trajectory:  [0.00816524401307106, 0.006978595047257841, 0.00631213205633685, 0.007331400993280113]\n"
     ]
    }
   ],
   "source": [
    "field_run = run(param, field_run[0])\n",
    "field_run = torch.reshape(field_run,(1,)+field_run.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaq(field) 0.6419232312298483\n",
      "tensor([28.5181], grad_fn=<AddBackward0>) tensor([-28.5181], grad_fn=<AddBackward0>)\n",
      "original_action tensor(8.8338, grad_fn=<AddBackward0>)\n",
      "eff_action tensor([2.3519], grad_fn=<AddBackward0>)\n",
      "plaq(x) -0.18872859092322408  logJ tensor([28.5181], grad_fn=<AddBackward0>)  force.norm 25.86894041618198\n",
      "plaq(y) 0.6419231019220838\n",
      "plaq(x) 0.6419232312298483  force.norm 21.046726320887668\n"
     ]
    }
   ],
   "source": [
    "flow = flow_model['layers']\n",
    "flow.eval()\n",
    "\n",
    "flows = flow\n",
    "\n",
    "field = torch.clone(field_run)\n",
    "\n",
    "print(f'plaq(field) {action(param, field[0]) / (-param.beta*param.volume)}')\n",
    "# field.requires_grad_(True)\n",
    "x = field\n",
    "logJ = 0.0\n",
    "for layer in reversed(flows):\n",
    "    x, lJ = layer.reverse(x)\n",
    "    logJ += lJ\n",
    "\n",
    "# x is the prior distribution now\n",
    "    \n",
    "x.requires_grad_(True)\n",
    "    \n",
    "y = x\n",
    "logJy = 0.0\n",
    "for layer in flows:\n",
    "    y, lJ = layer.forward(y)\n",
    "    logJy += lJ\n",
    "    \n",
    "s = action(param, y[0]) - logJy\n",
    "\n",
    "print(logJ,logJy)\n",
    "\n",
    "\n",
    "# print(\"eff_action\", s + 136.3786)\n",
    "\n",
    "print(\"original_action\", action(param, y[0]) + 91)\n",
    "\n",
    "print(\"eff_action\", s + 56)\n",
    "\n",
    "s.backward()\n",
    "\n",
    "f = x.grad\n",
    "\n",
    "x.requires_grad_(False)\n",
    "\n",
    "print(f'plaq(x) {action(param, x[0]) / (-param.beta*param.volume)}  logJ {logJ}  force.norm {torch.linalg.norm(f)}')\n",
    "\n",
    "print(f'plaq(y) {action(param, y[0]) / (-param.beta*param.volume)}')\n",
    "\n",
    "print(f'plaq(x) {action(param, field_run[0]) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(force(param, field_run[0]))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_flow(flow, f):\n",
    "    for layer in flow:\n",
    "        f, lJ = layer.forward(f)\n",
    "    return f.detach()\n",
    "\n",
    "def ft_flow_inv(flow, f):\n",
    "    for layer in reversed(flow):\n",
    "        f, lJ = layer.reverse(f)\n",
    "    return f.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_action(param, flow, f):\n",
    "    y = f\n",
    "    logJy = 0.0\n",
    "    for layer in flow:\n",
    "        y, lJ = layer.forward(y)\n",
    "        logJy += lJ\n",
    "    s = action(param, y[0]) - logJy\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_force(param, flow, field):\n",
    "    # f is the field follows the transformed distribution (close to prior distribution)\n",
    "    f = torch.clone(field)\n",
    "    # f = field\n",
    "    f.requires_grad_(True)\n",
    "    s = ft_action(param, flow, f)\n",
    "    s.backward()\n",
    "    ff = f.grad\n",
    "    f.requires_grad_(False)\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.8689)\n",
      "tensor(25.8689)\n"
     ]
    }
   ],
   "source": [
    "x = ft_flow_inv(flow, field_run)\n",
    "# x = field_run\n",
    "#for layer in reversed(flows):\n",
    "#    x, lJ = layer.reverse(x)\n",
    "ff = ft_force(param, flow, x)\n",
    "print(torch.linalg.norm(ff))\n",
    "fff = ft_force(param, flow, x)\n",
    "print(torch.linalg.norm(fff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-53.6481], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ft_flow_inv(flow, field_run)\n",
    "ft_action(param, flow, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_leapfrog(param, flow, x, p):\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = ft_force(param, flow, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    print(f'force.norm {torch.linalg.norm(f)} ft_action {float(ft_action(param, flow, x_).detach())} pp_action {0.5*torch.sum(p_*p_)}')\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        f = ft_force(param, flow, x_)\n",
    "        # print(f'force.norm {torch.linalg.norm(f)} ft_action {float(ft_action(param, flow, x_).detach())} pp_action {0.5*torch.sum(p_*p_)}')\n",
    "        p_ = p_ + (-dt)*f\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    return (x_, p_)\n",
    "\n",
    "def ft_hmc(param, flow, field):\n",
    "    x = ft_flow_inv(flow, field)\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = ft_action(param, flow, x).detach() + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = ft_leapfrog(param, flow, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = ft_action(param, flow, xr).detach() + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    # ADJUST ME\n",
    "    newx = xr if acc else x\n",
    "    # newx = xr\n",
    "    newfield = ft_flow(flow, newx)\n",
    "    return (float(dH), float(exp_mdH), acc, newfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_run(param, flow, field = param.initializer()):\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                field_run = torch.reshape(field,(1,)+field.shape)\n",
    "                dH, exp_mdH, acc, field_run = ft_hmc(param, flow, field_run)\n",
    "                field = field_run[0]\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.3\n",
      "steps = 8\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.6419232312298483  topo: 1.0\n",
      "force.norm 24.154966914652306 ft_action -53.352883274040444 pp_action 55.050053169344004\n",
      "Traj:    1  ACCEPT:  dH: -0.70435216   exp(-dH):  2.022536     plaq:  0.62325645   topo:  1.0\n",
      "force.norm 18.242967829067414 ft_action -54.499625696556606 pp_action 71.96845802176573\n",
      "Traj:    2  REJECT:  dH:  4.2423313    exp(-dH):  0.014374042  plaq:  0.62325635   topo:  1.0\n",
      "force.norm 17.349559154822405 ft_action -53.927681862619245 pp_action 66.43826386681144\n",
      "Traj:    3  ACCEPT:  dH:  0.033335148  exp(-dH):  0.96721435   plaq:  0.69312676   topo:  0.0\n",
      "force.norm 14.484198905786556 ft_action -54.59804685013093 pp_action 57.391131054797995\n",
      "Traj:    4  REJECT:  dH:  5.6908244    exp(-dH):  0.003376808  plaq:  0.6931268    topo:  0.0\n",
      "force.norm 11.154557497865682 ft_action -54.82740411517329 pp_action 71.54274439663864\n",
      "Traj:    5  ACCEPT:  dH: -0.0018431065  exp(-dH):  1.0018448    plaq:  0.71599595   topo:  0.0\n",
      "force.norm 6.919196719327459 ft_action -55.374065235231086 pp_action 60.92887730847431\n",
      "Traj:    6  ACCEPT:  dH:  0.3031197    exp(-dH):  0.73851069   plaq:  0.69444129   topo:  0.0\n",
      "force.norm 20.395974188081652 ft_action -52.93279517899996 pp_action 58.31378039388959\n",
      "Traj:    7  ACCEPT:  dH:  0.17960017   exp(-dH):  0.83560424   plaq:  0.68635171   topo:  1.0\n",
      "force.norm 7.530759265572256 ft_action -53.414957970321026 pp_action 69.5738207549228\n",
      "Traj:    8  REJECT:  dH:  1.575912     exp(-dH):  0.20681886   plaq:  0.68635175   topo:  1.0\n",
      "force.norm 16.74073014082459 ft_action -53.525958492442015 pp_action 83.46353069967743\n",
      "Traj:    9  ACCEPT:  dH: -1.0664961    exp(-dH):  2.9051823    plaq:  0.70431469   topo:  2.0\n",
      "force.norm 8.08091499966775 ft_action -52.71424304898512 pp_action 72.92927152135252\n",
      "Traj:   10  REJECT:  dH:  1.0768665    exp(-dH):  0.3406613    plaq:  0.70431465   topo:  2.0\n",
      "force.norm 54.48800586037825 ft_action -53.43644064756634 pp_action 66.36865828889812\n",
      "Traj:   11  ACCEPT:  dH:  0.50110209   exp(-dH):  0.60586257   plaq:  0.70643143   topo:  0.0\n",
      "force.norm 10.557818526420718 ft_action -55.40739089609463 pp_action 63.01864982385786\n",
      "Traj:   12  REJECT:  dH:  0.60389574   exp(-dH):  0.54667777   plaq:  0.70643156   topo:  0.0\n",
      "force.norm 14.077192157933673 ft_action -55.22368774613128 pp_action 58.861418451804084\n",
      "Traj:   13  ACCEPT:  dH: -2.0977539    exp(-dH):  8.1478482    plaq:  0.75884126   topo:  0.0\n",
      "force.norm 5.892640091464471 ft_action -55.656363649144986 pp_action 64.13135356625027\n",
      "Traj:   14  ACCEPT:  dH:  0.053359679  exp(-dH):  0.94803896   plaq:  0.77192519   topo:  0.0\n",
      "force.norm 6.269315663384655 ft_action -56.12622274317328 pp_action 83.02922341730037\n",
      "Traj:   15  ACCEPT:  dH:  1.8969561    exp(-dH):  0.15002458   plaq:  0.70217418   topo:  0.0\n",
      "force.norm 22.995745053607312 ft_action -53.871149552151735 pp_action 57.929530750747325\n",
      "Traj:   16  ACCEPT:  dH:  0.40369176   exp(-dH):  0.66784994   plaq:  0.77088174   topo:  0.0\n",
      "Run times:  [4.544604241033085, 4.384430482983589, 5.608457591035403, 5.182971931993961]\n",
      "Per trajectory:  [1.1361510602582712, 1.0961076207458973, 1.4021143977588508, 1.2957429829984903]\n"
     ]
    }
   ],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 0.3, # 0.3\n",
    "    nstep = 8, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    #\n",
    "    nprint = 4,\n",
    "    seed = 1331)\n",
    "\n",
    "field_run = ft_run(param, flow, field_run[0])\n",
    "field_run = torch.reshape(field_run,(1,)+field_run.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
