{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "\n",
    "# FT-HMC implemented for 8x8 2D QED (using SiLU as activation function).\n",
    "\n",
    "# Try to minimize size of the force in training. No significant improvements.\n",
    "\n",
    "# Some test on ergodicity\n",
    "# (calculate the probablity of generating the configs obtained via conventional HMC).\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Plot the force size distribution\n",
    "# Is the large force from the original action or Field-Transformation the determinant?\n",
    "# If from the determinant, then the fermion force won't cause problem for HMC\n",
    "\n",
    "# Use the same Field-Transformation for larger system (say 16x16, 32x32, 64x64, etc)\n",
    "# Study how the delta H depends on the system size ( perhaps delta H ~ sqrt(volume) )\n",
    "\n",
    "# Study the auto-correlation for observables, topo, plaq, flowed plaq, etc.\n",
    "\n",
    "# Improving the Field-Transformation to reduce force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from functools import reduce\n",
    "from field_transformation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Xiao-Yong\n",
    "\n",
    "class Param:\n",
    "    def __init__(self, beta = 6.0, lat = [64, 64], tau = 2.0, nstep = 50, ntraj = 256, nrun = 4, nprint = 256, seed = 11*13, randinit = False, nth = int(os.environ.get('OMP_NUM_THREADS', '2')), nth_interop = 2):\n",
    "        self.beta = beta\n",
    "        self.lat = lat\n",
    "        self.nd = len(lat)\n",
    "        self.volume = reduce(lambda x,y:x*y, lat)\n",
    "        self.tau = tau\n",
    "        self.nstep = nstep\n",
    "        self.dt = self.tau / self.nstep\n",
    "        self.ntraj = ntraj\n",
    "        self.nrun = nrun\n",
    "        self.nprint = nprint\n",
    "        self.seed = seed\n",
    "        self.randinit = randinit\n",
    "        self.nth = nth\n",
    "        self.nth_interop = nth_interop\n",
    "    def initializer(self):\n",
    "        if self.randinit:\n",
    "            return torch.empty((param.nd,) + param.lat).uniform_(-math.pi, math.pi)\n",
    "        else:\n",
    "            return torch.zeros((param.nd,) + param.lat)\n",
    "    def summary(self):\n",
    "        return f\"\"\"latsize = {self.lat}\n",
    "volume = {self.volume}\n",
    "beta = {self.beta}\n",
    "trajs = {self.ntraj}\n",
    "tau = {self.tau}\n",
    "steps = {self.nstep}\n",
    "seed = {self.seed}\n",
    "nth = {self.nth}\n",
    "nth_interop = {self.nth_interop}\n",
    "\"\"\"\n",
    "    def uniquestr(self):\n",
    "        lat = \".\".join(str(x) for x in self.lat)\n",
    "        return f\"out_l{lat}_b{param.beta}_n{param.ntraj}_t{param.tau}_s{param.nstep}.out\"\n",
    "\n",
    "def action(param, f):\n",
    "    return (-param.beta)*torch.sum(torch.cos(plaqphase(f)))\n",
    "\n",
    "def force(param, f):\n",
    "    f.requires_grad_(True)\n",
    "    s = action(param, f)\n",
    "    f.grad = None\n",
    "    s.backward()\n",
    "    ff = f.grad\n",
    "    f.requires_grad_(False)\n",
    "    return ff\n",
    "\n",
    "plaqphase = lambda f: f[0,:] - f[1,:] - torch.roll(f[0,:], shifts=-1, dims=1) + torch.roll(f[1,:], shifts=-1, dims=0)\n",
    "topocharge = lambda f: torch.floor(0.1 + torch.sum(regularize(plaqphase(f))) / (2*math.pi))\n",
    "def regularize(f):\n",
    "    p2 = 2*math.pi\n",
    "    f_ = (f - math.pi) / p2\n",
    "    return p2*(f_ - torch.floor(f_) - 0.5)\n",
    "\n",
    "def leapfrog(param, x, p):\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = force(param, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    print(f'plaq(x) {action(param, x) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(f)}')\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        p_ = p_ + (-dt)*force(param, x_)\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    return (x_, p_)\n",
    "def hmc(param, x):\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = action(param, x) + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = leapfrog(param, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = action(param, xr) + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    newx = xr if acc else x\n",
    "    return (dH, exp_mdH, acc, newx)\n",
    "\n",
    "put = lambda s: sys.stdout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 2, # 0.3\n",
    "    nstep = 8, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 2, # 2**16 # 2**10 # 2**15\n",
    "    #\n",
    "    nprint = 2,\n",
    "    seed = 1331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(param.seed)\n",
    "\n",
    "torch.set_num_threads(param.nth)\n",
    "torch.set_num_interop_threads(param.nth_interop)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(param.nth)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"0\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 1.0  topo: 0.0\n",
      "plaq(x) 1.0  force.norm 0.3902628323006594\n",
      "Traj:    1  ACCEPT:  dH: -0.0061020683  exp(-dH):  1.0061207    plaq:  0.80160241   topo:  0.0\n",
      "plaq(x) 0.8016024084919849  force.norm 26.353286360548907\n",
      "Traj:    2  ACCEPT:  dH: -0.00098832182  exp(-dH):  1.0009888    plaq:  0.75800764   topo:  0.0\n",
      "plaq(x) 0.7580076383603013  force.norm 28.28470315611182\n",
      "Traj:    3  ACCEPT:  dH: -0.00045810993  exp(-dH):  1.0004582    plaq:  0.74109523   topo:  0.0\n",
      "plaq(x) 0.7410952313494887  force.norm 28.779010749455527\n",
      "Traj:    4  ACCEPT:  dH:  0.00021325972  exp(-dH):  0.99978676   plaq:  0.74094164   topo:  0.0\n",
      "plaq(x) 0.7409416399750001  force.norm 28.464612960452314\n",
      "Traj:    5  ACCEPT:  dH: -0.00017601601  exp(-dH):  1.000176     plaq:  0.72011859   topo:  0.0\n",
      "plaq(x) 0.7201185944633461  force.norm 27.835833225543123\n",
      "Traj:    6  ACCEPT:  dH: -0.00027155741  exp(-dH):  1.0002716    plaq:  0.73939683   topo:  0.0\n",
      "plaq(x) 0.7393968322935627  force.norm 28.96019557545712\n",
      "Traj:    7  ACCEPT:  dH:  0.0012727982  exp(-dH):  0.99872801   plaq:  0.76353599   topo:  0.0\n",
      "plaq(x) 0.7635359944277044  force.norm 26.527636154840334\n",
      "Traj:    8  ACCEPT:  dH: -0.001000918  exp(-dH):  1.0010014    plaq:  0.74099477   topo: -1.0\n",
      "plaq(x) 0.7409947677031383  force.norm 28.697987725424145\n",
      "Traj:    9  ACCEPT:  dH:  0.00057563778  exp(-dH):  0.99942453   plaq:  0.75896281   topo:  0.0\n",
      "plaq(x) 0.7589628142324673  force.norm 27.879660792098036\n",
      "Traj:   10  ACCEPT:  dH: -0.0003360139  exp(-dH):  1.0003361    plaq:  0.72549854   topo:  0.0\n",
      "plaq(x) 0.7254985447170057  force.norm 27.99131011599841\n",
      "Traj:   11  ACCEPT:  dH:  0.00070289786  exp(-dH):  0.99929735   plaq:  0.7538829    topo:  1.0\n",
      "plaq(x) 0.7538828957381701  force.norm 26.76388529434804\n",
      "Traj:   12  ACCEPT:  dH: -0.00075628116  exp(-dH):  1.0007566    plaq:  0.72616945   topo:  1.0\n",
      "plaq(x) 0.7261694524430305  force.norm 27.52852462137567\n",
      "Traj:   13  ACCEPT:  dH:  0.00016085898  exp(-dH):  0.99983915   plaq:  0.7404225    topo:  1.0\n",
      "plaq(x) 0.7404225012722063  force.norm 27.99723742740619\n",
      "Traj:   14  ACCEPT:  dH: -0.00092120759  exp(-dH):  1.0009216    plaq:  0.69324415   topo: -1.0\n",
      "plaq(x) 0.6932441546491368  force.norm 29.904137524959637\n",
      "Traj:   15  ACCEPT:  dH: -0.00087324391  exp(-dH):  1.0008736    plaq:  0.64553856   topo: -1.0\n",
      "plaq(x) 0.6455385584470208  force.norm 31.06011068234206\n",
      "Traj:   16  ACCEPT:  dH:  0.0011118819  exp(-dH):  0.99888874   plaq:  0.64361543   topo: -1.0\n",
      "Run times:  [0.18376669503049925, 0.1568578949663788, 0.16596208501141518, 0.18774260499048978]\n",
      "Per trajectory:  [0.04594167375762481, 0.0392144737415947, 0.041490521252853796, 0.046935651247622445]\n"
     ]
    }
   ],
   "source": [
    "def run(param, field = None):\n",
    "    if field is None:\n",
    "        field = param.initializer()\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                dH, exp_mdH, acc, field = hmc(param, field)\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field\n",
    "field = run(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_flow(flow, f):\n",
    "    for layer in flow:\n",
    "        f, lJ = layer.forward(f)\n",
    "    return f.detach()\n",
    "\n",
    "def ft_flow_inv(flow, f):\n",
    "    for layer in reversed(flow):\n",
    "        f, lJ = layer.reverse(f)\n",
    "    return f.detach()\n",
    "\n",
    "def ft_action(param, flow, f):\n",
    "    y = f\n",
    "    logJy = 0.0\n",
    "    for layer in flow:\n",
    "        y, lJ = layer.forward(y)\n",
    "        logJy += lJ\n",
    "    action = U1GaugeAction(param.beta)\n",
    "    s = action(y) - logJy\n",
    "    return s\n",
    "\n",
    "def ft_force(param, flow, field, create_graph = False):\n",
    "    # f is the field follows the transformed distribution (close to prior distribution)\n",
    "    f = field\n",
    "    f.requires_grad_(True)\n",
    "    s = ft_action(param, flow, f)\n",
    "    ss = torch.sum(s)\n",
    "    # f.grad = None\n",
    "    ff, = torch.autograd.grad(ss, f, create_graph = create_graph)\n",
    "    f.requires_grad_(False)\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, action, optimizer, metrics, batch_size, with_force = False, pre_model = None):\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    optimizer.zero_grad()\n",
    "    #\n",
    "    xi = None\n",
    "    if pre_model != None:\n",
    "        pre_layers, pre_prior = pre_model['layers'], pre_model['prior']\n",
    "        pre_xi = pre_prior.sample_n(batch_size)\n",
    "        x = ft_flow(pre_layers, pre_xi)\n",
    "        xi = ft_flow_inv(layers, x)\n",
    "    #\n",
    "    xi, x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size, xi=xi)\n",
    "    logp = -action(x)\n",
    "    #\n",
    "    force_size = torch.tensor(0.0)\n",
    "    dkl = calc_dkl(logp, logq)\n",
    "    loss = torch.tensor(0.0)\n",
    "    if with_force:\n",
    "        assert pre_model != None\n",
    "        force = ft_force(param, layers, xi, True)\n",
    "        force_size = torch.sum(torch.square(force))\n",
    "        loss = force_size\n",
    "    else:\n",
    "        loss = dkl\n",
    "    #\n",
    "    loss.backward()\n",
    "    #\n",
    "    # minimization target\n",
    "    # loss mini\n",
    "    # -> (logq - logp) mini\n",
    "    # -> (action - logJ) mini\n",
    "    #\n",
    "    optimizer.step()\n",
    "    ess = compute_ess(logp, logq)\n",
    "    #\n",
    "    print(grab(loss),\n",
    "          grab(force_size),\n",
    "          grab(dkl),\n",
    "          grab(ess),\n",
    "          torch.linalg.norm(ft_force(param, layers, xi)))\n",
    "    #\n",
    "    metrics['loss'].append(grab(loss))\n",
    "    metrics['force'].append(grab(force_size))\n",
    "    metrics['dkl'].append(grab(dkl))\n",
    "    metrics['logp'].append(grab(logp))\n",
    "    metrics['logq'].append(grab(logq))\n",
    "    metrics['ess'].append(grab(ess))\n",
    "\n",
    "def flow_train(param, with_force = False, pre_model = None):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    # Theory\n",
    "    lattice_shape = param.lat\n",
    "    link_shape = (2,*param.lat)\n",
    "    beta = param.beta\n",
    "    u1_action = U1GaugeAction(beta)\n",
    "    # Model\n",
    "    prior = MultivariateUniform(torch.zeros(link_shape), 2*np.pi*torch.ones(link_shape))\n",
    "    #\n",
    "    n_layers = 24\n",
    "    n_s_nets = 2\n",
    "    hidden_sizes = [8,8]\n",
    "    kernel_size = 3\n",
    "    layers = make_u1_equiv_layers(lattice_shape=lattice_shape, n_layers=n_layers, n_mixture_comps=n_s_nets,\n",
    "                                  hidden_sizes=hidden_sizes, kernel_size=kernel_size)\n",
    "    set_weights(layers)\n",
    "    model = {'layers': layers, 'prior': prior}\n",
    "    # Training\n",
    "    base_lr = .001\n",
    "    optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr)\n",
    "    optimizer_wf = torch.optim.Adam(model['layers'].parameters(), lr=base_lr / 100.0)\n",
    "    #\n",
    "    # ADJUST ME\n",
    "    N_era = 10\n",
    "    N_epoch = 100\n",
    "    #\n",
    "    batch_size = 64\n",
    "    print_freq = N_epoch # epochs\n",
    "    plot_freq = 1 # epochs\n",
    "    history = {\n",
    "        'loss' : [],\n",
    "        'force' : [],\n",
    "        'dkl' : [],\n",
    "        'logp' : [],\n",
    "        'logq' : [],\n",
    "        'ess' : []\n",
    "    }\n",
    "    for era in range(N_era):\n",
    "        for epoch in range(N_epoch):\n",
    "            train_step(model, u1_action, optimizer, history, batch_size)\n",
    "            if with_force:\n",
    "                train_step(model, u1_action, optimizer_wf, history, batch_size,\n",
    "                           with_force = with_force, pre_model = pre_model)\n",
    "            if epoch % print_freq == 0:\n",
    "                print_metrics(history, print_freq, era, epoch)\n",
    "    return model,u1_action\n",
    "\n",
    "def flow_eval(model, u1_action):  # packaged from original ipynb by Xiao-Yong Jin\n",
    "    ensemble_size = 1024\n",
    "    u1_ens = make_mcmc_ensemble(model, u1_action, 64, ensemble_size)\n",
    "    print(\"Accept rate:\", np.mean(u1_ens['accepted']))\n",
    "    Q = grab(topo_charge(torch.stack(u1_ens['x'], axis=0)))\n",
    "    X_mean, X_err = bootstrap(Q**2, Nboot=100, binsize=16)\n",
    "    print(f'Topological susceptibility = {X_mean:.2f} +/- {X_err:.2f}')\n",
    "    print(f'... vs HMC estimate = 1.23 +/- 0.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-235.80379178382825 0.0 -235.80379178382825 0.015742986900554163 tensor(173.9276)\n",
      "== Era 0 | Epoch 0 metrics ==\n",
      "\tloss -235.804\n",
      "\tforce 0\n",
      "\tdkl -235.804\n",
      "\tlogp 0.693854\n",
      "\tlogq -235.11\n",
      "\tess 0.015743\n",
      "-240.22592204900425 0.0 -240.22592204900425 0.01726504117436026 tensor(169.3064)\n",
      "-241.73488535507386 0.0 -241.73488535507386 0.016400911777968756 tensor(167.0606)\n",
      "-247.46169379805445 0.0 -247.46169379805445 0.015810153849540865 tensor(160.3539)\n",
      "-253.25240939507336 0.0 -253.25240939507336 0.01941417680419557 tensor(158.7390)\n",
      "-256.7516621657513 0.0 -256.7516621657513 0.015804690643426364 tensor(154.5671)\n",
      "-257.21331835493976 0.0 -257.21331835493976 0.01562887239859159 tensor(157.1432)\n",
      "-261.58546505634735 0.0 -261.58546505634735 0.020297799438260253 tensor(151.7423)\n",
      "-265.36695333901866 0.0 -265.36695333901866 0.0156412599546812 tensor(151.7542)\n",
      "-266.2862499543668 0.0 -266.2862499543668 0.016146700456890064 tensor(155.4196)\n",
      "-270.4000967148002 0.0 -270.4000967148002 0.01831122343147412 tensor(156.1291)\n",
      "-272.4753249417529 0.0 -272.4753249417529 0.04926610112492897 tensor(161.7628)\n",
      "-274.0188336018449 0.0 -274.0188336018449 0.01720192443603824 tensor(164.9150)\n",
      "-276.6255801641357 0.0 -276.6255801641357 0.02571803697602018 tensor(171.3087)\n",
      "-278.0437584060316 0.0 -278.0437584060316 0.0465059199441165 tensor(167.8843)\n",
      "-278.5597071189384 0.0 -278.5597071189384 0.1287209159279449 tensor(172.7423)\n",
      "-279.621035979776 0.0 -279.621035979776 0.05532086467648965 tensor(190.1240)\n",
      "-279.01972318243645 0.0 -279.01972318243645 0.03938253604333165 tensor(200.9539)\n",
      "-279.76487981304757 0.0 -279.76487981304757 0.054519163219636006 tensor(197.8722)\n",
      "-278.52186411795196 0.0 -278.52186411795196 0.04685051049393984 tensor(190.9811)\n",
      "-279.3359481493468 0.0 -279.3359481493468 0.04544745289418525 tensor(200.5659)\n",
      "-278.2460912674644 0.0 -278.2460912674644 0.06238963263133862 tensor(220.7544)\n",
      "-278.34046858181995 0.0 -278.34046858181995 0.05001545440329313 tensor(206.7338)\n",
      "-278.8382760601778 0.0 -278.8382760601778 0.04484455707296428 tensor(204.9199)\n",
      "-277.82948138271104 0.0 -277.82948138271104 0.018040034645773903 tensor(209.8352)\n",
      "-277.5916656840476 0.0 -277.5916656840476 0.01636556036707938 tensor(206.6545)\n",
      "-278.1507596029115 0.0 -278.1507596029115 0.06118730824765313 tensor(207.9465)\n",
      "-278.8101103706663 0.0 -278.8101103706663 0.034171797405028785 tensor(197.2281)\n",
      "-279.4655671051157 0.0 -279.4655671051157 0.045986787380270414 tensor(184.8082)\n",
      "-280.6271984343321 0.0 -280.6271984343321 0.01813896051119748 tensor(195.5126)\n",
      "-280.49219102778613 0.0 -280.49219102778613 0.05813158193591589 tensor(188.1255)\n",
      "-280.20689888466694 0.0 -280.20689888466694 0.021029946327521885 tensor(190.1216)\n",
      "-280.0755728496201 0.0 -280.0755728496201 0.03678629248046407 tensor(194.5040)\n",
      "-280.27826695532247 0.0 -280.27826695532247 0.028742392685797303 tensor(192.5850)\n",
      "-280.1873088901689 0.0 -280.1873088901689 0.08962251720645786 tensor(180.5103)\n",
      "-280.89798536616973 0.0 -280.89798536616973 0.02286624294173069 tensor(176.2874)\n",
      "-279.70993882292083 0.0 -279.70993882292083 0.0788166434846866 tensor(178.6955)\n",
      "-280.85005605236887 0.0 -280.85005605236887 0.055791318173413366 tensor(172.9682)\n",
      "-281.228642916131 0.0 -281.228642916131 0.02247591586331085 tensor(172.0726)\n",
      "-280.76011951779475 0.0 -280.76011951779475 0.03457317148080947 tensor(171.1435)\n",
      "-280.7015899077327 0.0 -280.7015899077327 0.041598648103628816 tensor(175.0371)\n",
      "-280.0759987440519 0.0 -280.0759987440519 0.01571257750061264 tensor(179.4415)\n",
      "-280.15991748188526 0.0 -280.15991748188526 0.08907547182949271 tensor(172.4989)\n",
      "-280.51062148805613 0.0 -280.51062148805613 0.08400647854928746 tensor(173.4484)\n",
      "-281.0749944569544 0.0 -281.0749944569544 0.019619205321069054 tensor(173.8982)\n",
      "-280.32840611227647 0.0 -280.32840611227647 0.039978622586709346 tensor(165.3508)\n",
      "-280.55509908945055 0.0 -280.55509908945055 0.06077742684745242 tensor(172.4607)\n",
      "-281.0819986780939 0.0 -281.0819986780939 0.04230964744577129 tensor(176.0983)\n",
      "-280.0948994967971 0.0 -280.0948994967971 0.07837078262497556 tensor(183.6973)\n",
      "-280.80247837907297 0.0 -280.80247837907297 0.032545641534498564 tensor(182.5193)\n",
      "-281.4956722444581 0.0 -281.4956722444581 0.0275810425537696 tensor(176.5550)\n",
      "-280.40079033652864 0.0 -280.40079033652864 0.034283307159465856 tensor(172.2658)\n",
      "-281.93450647976016 0.0 -281.93450647976016 0.09477756280786527 tensor(180.6309)\n",
      "-280.8650609714495 0.0 -280.8650609714495 0.02059988318219647 tensor(181.0129)\n",
      "-280.89788182581094 0.0 -280.89788182581094 0.022247776021795965 tensor(187.7786)\n",
      "-281.1891778348007 0.0 -281.1891778348007 0.061989714949322615 tensor(186.5429)\n",
      "-280.31052060625746 0.0 -280.31052060625746 0.12670764213761346 tensor(177.8267)\n",
      "-282.51824385883754 0.0 -282.51824385883754 0.04110197507992237 tensor(179.2902)\n",
      "-280.88979613636826 0.0 -280.88979613636826 0.03578817012174448 tensor(180.6850)\n",
      "-281.67261027289476 0.0 -281.67261027289476 0.020884132175671248 tensor(187.8300)\n",
      "-281.2433286605999 0.0 -281.2433286605999 0.07685460093425121 tensor(182.5710)\n",
      "-281.3994165151803 0.0 -281.3994165151803 0.01967491947136179 tensor(184.8858)\n",
      "-281.44447396798336 0.0 -281.44447396798336 0.015986711230498888 tensor(182.5059)\n",
      "-282.1103157083836 0.0 -282.1103157083836 0.045043580083314445 tensor(183.4524)\n",
      "-280.9787463953471 0.0 -280.9787463953471 0.03672576751273208 tensor(168.4463)\n",
      "-280.53249089993466 0.0 -280.53249089993466 0.056355509864126865 tensor(182.7690)\n",
      "-282.19144397426135 0.0 -282.19144397426135 0.04076380493125131 tensor(179.7477)\n",
      "-281.71195185616403 0.0 -281.71195185616403 0.0583052023596927 tensor(176.5427)\n",
      "-281.6959079076538 0.0 -281.6959079076538 0.024023842314468972 tensor(166.3750)\n",
      "-280.2845728767015 0.0 -280.2845728767015 0.03698126790042735 tensor(180.1940)\n",
      "-281.0296800962542 0.0 -281.0296800962542 0.0549875835180861 tensor(175.1306)\n",
      "-281.3336376168702 0.0 -281.3336376168702 0.0273282222066487 tensor(173.1741)\n",
      "-281.2295763241233 0.0 -281.2295763241233 0.03630821176004081 tensor(179.3191)\n",
      "-281.1784295910469 0.0 -281.1784295910469 0.03828596828691298 tensor(181.6012)\n",
      "-281.6338148299032 0.0 -281.6338148299032 0.10277893799147371 tensor(174.1253)\n",
      "-281.6642892115454 0.0 -281.6642892115454 0.042474873501435306 tensor(175.1895)\n",
      "-282.0442685956595 0.0 -282.0442685956595 0.11302439180849502 tensor(181.0515)\n",
      "-282.217124653517 0.0 -282.217124653517 0.040262488754816264 tensor(181.2859)\n",
      "-282.10812986663086 0.0 -282.10812986663086 0.07920310769711404 tensor(176.3626)\n",
      "-281.860965762946 0.0 -281.860965762946 0.15869952263176476 tensor(185.5843)\n",
      "-282.21128894428523 0.0 -282.21128894428523 0.028988375934742277 tensor(179.8114)\n",
      "-280.85091289770736 0.0 -280.85091289770736 0.08700997061604492 tensor(186.5723)\n",
      "-281.1877732527943 0.0 -281.1877732527943 0.023828303839758713 tensor(177.4217)\n",
      "-281.761208043354 0.0 -281.761208043354 0.043515496622139105 tensor(177.1457)\n",
      "-281.6323167069575 0.0 -281.6323167069575 0.020703850250401615 tensor(196.2096)\n",
      "-281.7073711926607 0.0 -281.7073711926607 0.030385222654674224 tensor(191.7956)\n",
      "-282.26686509613796 0.0 -282.26686509613796 0.018121128150640247 tensor(178.5193)\n",
      "-282.32893941857486 0.0 -282.32893941857486 0.015834773442371275 tensor(185.0120)\n",
      "-282.56461468454836 0.0 -282.56461468454836 0.03721718187262966 tensor(182.8940)\n",
      "-281.6663090522586 0.0 -281.6663090522586 0.09016990979063415 tensor(180.6176)\n",
      "-281.8691022646892 0.0 -281.8691022646892 0.032426043228763066 tensor(182.4766)\n",
      "-281.8105358433904 0.0 -281.8105358433904 0.03087916641312881 tensor(191.2861)\n",
      "-282.21598126391734 0.0 -282.21598126391734 0.054216245681964995 tensor(178.4135)\n",
      "-282.85015366684627 0.0 -282.85015366684627 0.10421619782065272 tensor(174.2103)\n",
      "-281.99705545539797 0.0 -281.99705545539797 0.04501972828116775 tensor(175.0787)\n",
      "-282.94012552043495 0.0 -282.94012552043495 0.021274293550750265 tensor(172.8371)\n",
      "-281.89170141835814 0.0 -281.89170141835814 0.03897666733373204 tensor(172.9879)\n",
      "-283.15599364900083 0.0 -283.15599364900083 0.01696672651246978 tensor(169.7644)\n",
      "-282.47809447276256 0.0 -282.47809447276256 0.05757713509931378 tensor(170.1345)\n",
      "-282.6446707239398 0.0 -282.6446707239398 0.0561849518070143 tensor(173.9761)\n",
      "-282.00670714588443 0.0 -282.00670714588443 0.04757257023645178 tensor(170.8849)\n",
      "== Era 1 | Epoch 0 metrics ==\n",
      "\tloss -278.203\n",
      "\tforce 0\n",
      "\tdkl -278.203\n",
      "\tlogp 68.451\n",
      "\tlogq -209.752\n",
      "\tess 0.0447474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-282.30318004954523 0.0 -282.30318004954523 0.03519414026309184 tensor(173.6366)\n",
      "-281.71068933998566 0.0 -281.71068933998566 0.044429059801842655 tensor(189.9906)\n",
      "-282.02463668393204 0.0 -282.02463668393204 0.028696246313953674 tensor(171.9115)\n",
      "-283.132869239756 0.0 -283.132869239756 0.019255968703855045 tensor(165.2447)\n",
      "-282.0692816718184 0.0 -282.0692816718184 0.023057457037706616 tensor(170.5575)\n",
      "-282.6374101397821 0.0 -282.6374101397821 0.03463303707863622 tensor(174.0301)\n",
      "-282.6942481521188 0.0 -282.6942481521188 0.07775488816440008 tensor(182.1365)\n",
      "-282.8336707077077 0.0 -282.8336707077077 0.11739860957901044 tensor(173.4950)\n",
      "-282.4292215109003 0.0 -282.4292215109003 0.04353524233107781 tensor(174.8651)\n",
      "-282.6392043268243 0.0 -282.6392043268243 0.057467884980354454 tensor(174.7497)\n",
      "-282.52266144131096 0.0 -282.52266144131096 0.1869519973917699 tensor(175.9306)\n",
      "-282.3960594537949 0.0 -282.3960594537949 0.10639430865819953 tensor(175.4130)\n",
      "-283.18745889396143 0.0 -283.18745889396143 0.07316614556037243 tensor(185.5502)\n",
      "-283.1071742096166 0.0 -283.1071742096166 0.11033916417667007 tensor(172.6518)\n",
      "-284.2231862243465 0.0 -284.2231862243465 0.07341238050430209 tensor(178.3674)\n",
      "-282.6782644014969 0.0 -282.6782644014969 0.04964608190177719 tensor(188.2040)\n",
      "-282.58670020715914 0.0 -282.58670020715914 0.01979867941971493 tensor(176.1086)\n",
      "-282.9601264612375 0.0 -282.9601264612375 0.06513096496416332 tensor(176.9925)\n",
      "-283.507229866559 0.0 -283.507229866559 0.01945150541737811 tensor(185.1243)\n",
      "-282.7754210874775 0.0 -282.7754210874775 0.07204193091620131 tensor(181.8959)\n",
      "-283.2024171454899 0.0 -283.2024171454899 0.058742829418482 tensor(170.0647)\n",
      "-282.7496708762467 0.0 -282.7496708762467 0.06744229921808483 tensor(189.1918)\n",
      "-282.19984885633585 0.0 -282.19984885633585 0.08623079508400139 tensor(183.4364)\n",
      "-283.46424653000787 0.0 -283.46424653000787 0.04030354199547901 tensor(169.0150)\n",
      "-283.0612352735593 0.0 -283.0612352735593 0.11139450370861136 tensor(176.9349)\n",
      "-282.29700158833884 0.0 -282.29700158833884 0.029870035508316312 tensor(173.2359)\n",
      "-283.04789136564364 0.0 -283.04789136564364 0.07469371942879573 tensor(180.8702)\n",
      "-283.2999767570986 0.0 -283.2999767570986 0.1033338614856936 tensor(184.7027)\n",
      "-283.2584641423234 0.0 -283.2584641423234 0.042256543041594916 tensor(164.9398)\n",
      "-283.5549297190919 0.0 -283.5549297190919 0.05284754294273788 tensor(176.7548)\n",
      "-283.44679563845085 0.0 -283.44679563845085 0.03132916591299293 tensor(169.0613)\n",
      "-283.29576600649983 0.0 -283.29576600649983 0.1090892861495012 tensor(168.3540)\n",
      "-283.36231638432264 0.0 -283.36231638432264 0.13317080561339126 tensor(178.3065)\n",
      "-283.33356307004675 0.0 -283.33356307004675 0.037717101793621026 tensor(169.9870)\n",
      "-283.68519867946276 0.0 -283.68519867946276 0.022691849782946284 tensor(162.7710)\n",
      "-283.8246997662301 0.0 -283.8246997662301 0.1108849459247168 tensor(173.6911)\n",
      "-283.26320330302593 0.0 -283.26320330302593 0.04599732008526972 tensor(166.0525)\n",
      "-283.9682544420002 0.0 -283.9682544420002 0.06766684517055477 tensor(176.9502)\n",
      "-283.17363556577624 0.0 -283.17363556577624 0.0746285181154122 tensor(185.8444)\n",
      "-283.46881889312465 0.0 -283.46881889312465 0.11987147049099285 tensor(180.1229)\n",
      "-283.40641675598147 0.0 -283.40641675598147 0.04601947783379247 tensor(182.7823)\n",
      "-283.5793508013917 0.0 -283.5793508013917 0.07835659860062531 tensor(175.5434)\n",
      "-283.93621492741386 0.0 -283.93621492741386 0.06598119556474961 tensor(178.8233)\n",
      "-283.91012809830215 0.0 -283.91012809830215 0.04993670394996056 tensor(182.1567)\n",
      "-284.3697748446327 0.0 -284.3697748446327 0.12103288366830631 tensor(160.9178)\n",
      "-283.9412174668042 0.0 -283.9412174668042 0.1767342221840089 tensor(169.1831)\n",
      "-284.2950279147256 0.0 -284.2950279147256 0.04776530373027298 tensor(171.9146)\n",
      "-283.96319330502206 0.0 -283.96319330502206 0.0741857367761963 tensor(165.7994)\n",
      "-284.2201830671403 0.0 -284.2201830671403 0.06989594801602658 tensor(172.9130)\n",
      "-284.45051366883473 0.0 -284.45051366883473 0.060882074213737696 tensor(175.0090)\n",
      "-283.8741287280267 0.0 -283.8741287280267 0.06459306405637323 tensor(179.6951)\n",
      "-283.6781404872687 0.0 -283.6781404872687 0.04960306698565907 tensor(178.4403)\n",
      "-283.82481370678715 0.0 -283.82481370678715 0.016166408115057963 tensor(169.3131)\n",
      "-284.11021619211493 0.0 -284.11021619211493 0.10069734602275496 tensor(182.6901)\n",
      "-284.08864210160567 0.0 -284.08864210160567 0.1197664454721053 tensor(179.7751)\n",
      "-283.60113745842614 0.0 -283.60113745842614 0.04771807037186648 tensor(175.5429)\n",
      "-284.53002920665483 0.0 -284.53002920665483 0.09251990685106692 tensor(168.7565)\n",
      "-284.0222837211801 0.0 -284.0222837211801 0.10871693142448109 tensor(194.3816)\n",
      "-284.6389806430451 0.0 -284.6389806430451 0.12263617010084052 tensor(162.3994)\n",
      "-283.85040911298563 0.0 -283.85040911298563 0.04448038210276195 tensor(185.6694)\n",
      "-283.64363095979536 0.0 -283.64363095979536 0.15892952567228277 tensor(177.9785)\n",
      "-284.085284121786 0.0 -284.085284121786 0.12204017675400022 tensor(164.1414)\n",
      "-284.52010441809915 0.0 -284.52010441809915 0.17213783609115507 tensor(174.7066)\n",
      "-284.09742051473444 0.0 -284.09742051473444 0.11808280917733889 tensor(171.3153)\n",
      "-284.1038455659367 0.0 -284.1038455659367 0.11329840995753751 tensor(157.0927)\n",
      "-284.48818801213673 0.0 -284.48818801213673 0.043779521579852 tensor(175.0124)\n",
      "-284.6844803917569 0.0 -284.6844803917569 0.14009778345404436 tensor(172.9611)\n",
      "-284.82795057946186 0.0 -284.82795057946186 0.06381134877238784 tensor(181.9822)\n",
      "-283.84731463673705 0.0 -283.84731463673705 0.14575420698428013 tensor(170.0130)\n",
      "-284.5270323027262 0.0 -284.5270323027262 0.16989467527463406 tensor(160.4349)\n",
      "-284.3056092565243 0.0 -284.3056092565243 0.11429048397444173 tensor(170.5609)\n",
      "-284.6048803190806 0.0 -284.6048803190806 0.04444173957319538 tensor(165.0588)\n",
      "-284.84107993179305 0.0 -284.84107993179305 0.042264704696756375 tensor(156.7691)\n",
      "-285.09822921006116 0.0 -285.09822921006116 0.07452433132458808 tensor(169.0757)\n",
      "-284.56791321813745 0.0 -284.56791321813745 0.04548152510590806 tensor(166.8612)\n",
      "-284.28800473966487 0.0 -284.28800473966487 0.05860942150288278 tensor(177.2119)\n",
      "-285.10734248210156 0.0 -285.10734248210156 0.027581707935021547 tensor(182.1247)\n",
      "-284.7711944624117 0.0 -284.7711944624117 0.16990258486279147 tensor(161.8611)\n",
      "-284.9485453738905 0.0 -284.9485453738905 0.04237198967617256 tensor(179.6818)\n",
      "-284.54075318600223 0.0 -284.54075318600223 0.10447810921394249 tensor(168.9022)\n",
      "-284.9902015430231 0.0 -284.9902015430231 0.05931719238077771 tensor(178.7228)\n",
      "-284.492945606835 0.0 -284.492945606835 0.1465350296105903 tensor(198.6983)\n",
      "-284.70163936834473 0.0 -284.70163936834473 0.09047054893545678 tensor(169.7947)\n",
      "-285.2429312925678 0.0 -285.2429312925678 0.024129017240744517 tensor(172.3599)\n",
      "-284.0130427928664 0.0 -284.0130427928664 0.1524910641239055 tensor(169.2721)\n",
      "-284.75996831295873 0.0 -284.75996831295873 0.02632855942286506 tensor(179.1257)\n",
      "-285.75495444126886 0.0 -285.75495444126886 0.10864935249242606 tensor(153.9854)\n",
      "-284.81064142098523 0.0 -284.81064142098523 0.10521914401772113 tensor(172.1167)\n",
      "-285.54486035399714 0.0 -285.54486035399714 0.1855202512248685 tensor(186.2401)\n",
      "-285.13386407846565 0.0 -285.13386407846565 0.0577332757845257 tensor(175.2438)\n",
      "-285.27303873318783 0.0 -285.27303873318783 0.10084277279357752 tensor(155.3600)\n",
      "-285.169421877806 0.0 -285.169421877806 0.16481802801902484 tensor(162.0158)\n",
      "-285.48126022013054 0.0 -285.48126022013054 0.21647479998460994 tensor(150.0655)\n",
      "-285.1864804541423 0.0 -285.1864804541423 0.20412787577288885 tensor(166.8762)\n",
      "-284.7439428094399 0.0 -284.7439428094399 0.05471426533879927 tensor(170.6931)\n",
      "-285.08235824509353 0.0 -285.08235824509353 0.20725302838207318 tensor(173.3536)\n",
      "-284.9361512862854 0.0 -284.9361512862854 0.06251251654264883 tensor(178.2972)\n",
      "-285.4491674382099 0.0 -285.4491674382099 0.13403303249778092 tensor(163.1847)\n",
      "-285.5311706336511 0.0 -285.5311706336511 0.0524799363568913 tensor(170.9179)\n",
      "-285.14919858896826 0.0 -285.14919858896826 0.055671792326049005 tensor(155.7343)\n",
      "== Era 2 | Epoch 0 metrics ==\n",
      "\tloss -283.91\n",
      "\tforce 0\n",
      "\tdkl -283.91\n",
      "\tlogp 79.3769\n",
      "\tlogq -204.533\n",
      "\tess 0.084167\n",
      "-285.2184109390457 0.0 -285.2184109390457 0.05831390274229265 tensor(174.0637)\n",
      "-285.50803701946836 0.0 -285.50803701946836 0.1166042724279074 tensor(158.2435)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-285.4745453428999 0.0 -285.4745453428999 0.08551370972478504 tensor(159.2134)\n",
      "-285.74109108264855 0.0 -285.74109108264855 0.09694266245844649 tensor(166.2664)\n",
      "-285.18165772424607 0.0 -285.18165772424607 0.18581720592138307 tensor(160.6161)\n",
      "-284.9456329769272 0.0 -284.9456329769272 0.037887675923891334 tensor(165.5799)\n",
      "-285.13679041170724 0.0 -285.13679041170724 0.0709500809313619 tensor(161.8093)\n",
      "-284.8899231454957 0.0 -284.8899231454957 0.1292122816474257 tensor(175.4481)\n",
      "-285.48417859246416 0.0 -285.48417859246416 0.09883028991631716 tensor(170.3434)\n",
      "-285.2071173702128 0.0 -285.2071173702128 0.16868826186902278 tensor(159.7044)\n",
      "-285.5761087567977 0.0 -285.5761087567977 0.10562976216060008 tensor(153.3455)\n",
      "-284.95738302568657 0.0 -284.95738302568657 0.1613954717331739 tensor(166.5366)\n",
      "-285.52831036667357 0.0 -285.52831036667357 0.06433101101357216 tensor(157.1581)\n",
      "-285.53802653925254 0.0 -285.53802653925254 0.08475295738354054 tensor(168.5715)\n",
      "-285.5215177825453 0.0 -285.5215177825453 0.04622443999974189 tensor(145.8244)\n",
      "-284.73798765022883 0.0 -284.73798765022883 0.12556293282681696 tensor(177.4749)\n",
      "-285.09763247129774 0.0 -285.09763247129774 0.051669530337650406 tensor(171.8913)\n",
      "-285.3677441486632 0.0 -285.3677441486632 0.07960932582645106 tensor(160.3034)\n",
      "-285.172117130299 0.0 -285.172117130299 0.14246042925814437 tensor(178.2159)\n",
      "-284.95505389019525 0.0 -284.95505389019525 0.09333795996034315 tensor(171.3762)\n",
      "-285.210791449917 0.0 -285.210791449917 0.02345102202011016 tensor(163.6698)\n",
      "-285.7729029572439 0.0 -285.7729029572439 0.1590933247135578 tensor(177.1358)\n",
      "-285.7009747862055 0.0 -285.7009747862055 0.07724134735040727 tensor(149.6147)\n",
      "-285.39137360890896 0.0 -285.39137360890896 0.06450193671957691 tensor(176.4151)\n",
      "-285.40595957764197 0.0 -285.40595957764197 0.20089973349584755 tensor(161.9324)\n",
      "-285.0249742498494 0.0 -285.0249742498494 0.16469151906215282 tensor(168.7602)\n",
      "-285.34496399192614 0.0 -285.34496399192614 0.08451825216578815 tensor(180.1230)\n",
      "-285.4542273420165 0.0 -285.4542273420165 0.1512933289333432 tensor(160.0746)\n",
      "-285.7394136091922 0.0 -285.7394136091922 0.07095784232521866 tensor(143.8819)\n",
      "-285.3525736052087 0.0 -285.3525736052087 0.22523899715807372 tensor(157.8147)\n",
      "-285.6514614256912 0.0 -285.6514614256912 0.12542073232142328 tensor(170.8566)\n",
      "-285.3086019845266 0.0 -285.3086019845266 0.24665216109748223 tensor(163.0398)\n",
      "-285.2200104747799 0.0 -285.2200104747799 0.19890735548454128 tensor(162.9307)\n",
      "-286.0346141044008 0.0 -286.0346141044008 0.06072443752146208 tensor(154.9434)\n",
      "-284.8816161975323 0.0 -284.8816161975323 0.1456399103051855 tensor(163.2169)\n",
      "-285.4859866499263 0.0 -285.4859866499263 0.15903325152067285 tensor(170.9442)\n",
      "-285.1185583000024 0.0 -285.1185583000024 0.10976198201359776 tensor(174.2226)\n",
      "-285.4987788689051 0.0 -285.4987788689051 0.2624752126350842 tensor(170.8950)\n",
      "-285.3732465873661 0.0 -285.3732465873661 0.07718100543474717 tensor(187.6936)\n",
      "-285.40862417951007 0.0 -285.40862417951007 0.03999043262133996 tensor(178.2936)\n",
      "-284.806348543121 0.0 -284.806348543121 0.13733116813623214 tensor(170.3104)\n",
      "-285.6282098060283 0.0 -285.6282098060283 0.10152011862808323 tensor(159.3864)\n",
      "-285.1068810633505 0.0 -285.1068810633505 0.10477600508487718 tensor(162.4142)\n",
      "-285.27508423020356 0.0 -285.27508423020356 0.10034933840115375 tensor(156.2405)\n",
      "-285.3810020436001 0.0 -285.3810020436001 0.07143800805349106 tensor(161.7828)\n",
      "-285.18245914653824 0.0 -285.18245914653824 0.12577538158485227 tensor(161.4112)\n",
      "-285.64785782581953 0.0 -285.64785782581953 0.20742398966620018 tensor(170.1285)\n",
      "-285.6591782869963 0.0 -285.6591782869963 0.08653047755323123 tensor(175.8828)\n",
      "-285.20873855958456 0.0 -285.20873855958456 0.15381965053538188 tensor(168.5506)\n",
      "-286.12585610110995 0.0 -286.12585610110995 0.07900627174344212 tensor(163.5564)\n",
      "-285.1630109045656 0.0 -285.1630109045656 0.10736049842534362 tensor(170.3251)\n",
      "-285.26426660961425 0.0 -285.26426660961425 0.05824219574694272 tensor(159.8768)\n",
      "-285.24075330162725 0.0 -285.24075330162725 0.02513388317778653 tensor(149.0011)\n",
      "-285.3878000143175 0.0 -285.3878000143175 0.1375428989483743 tensor(167.0263)\n",
      "-285.74013543106435 0.0 -285.74013543106435 0.2709526156846631 tensor(161.0943)\n",
      "-285.3269827196391 0.0 -285.3269827196391 0.1476589401836637 tensor(146.9850)\n",
      "-285.7492749946028 0.0 -285.7492749946028 0.04864819677573033 tensor(149.4949)\n",
      "-284.84690469649837 0.0 -284.84690469649837 0.046979299405575096 tensor(175.6567)\n",
      "-285.42050268531966 0.0 -285.42050268531966 0.18875815928513134 tensor(165.2185)\n",
      "-285.50508590616903 0.0 -285.50508590616903 0.2267452268441885 tensor(158.1366)\n",
      "-285.11426066992465 0.0 -285.11426066992465 0.11170305946447684 tensor(171.1510)\n",
      "-285.4200717483932 0.0 -285.4200717483932 0.08255416918836905 tensor(174.0107)\n",
      "-285.6588242093638 0.0 -285.6588242093638 0.2340175471610539 tensor(152.2888)\n",
      "-285.4040448912074 0.0 -285.4040448912074 0.12415036562649723 tensor(171.3159)\n",
      "-285.8326220236139 0.0 -285.8326220236139 0.132049420221111 tensor(159.6966)\n",
      "-286.12827467087465 0.0 -286.12827467087465 0.08635103738068722 tensor(160.6796)\n",
      "-285.40239419303754 0.0 -285.40239419303754 0.2114754887098206 tensor(149.3049)\n",
      "-285.9532353569974 0.0 -285.9532353569974 0.09001105402510966 tensor(161.5633)\n",
      "-285.7232486283 0.0 -285.7232486283 0.08756597541290156 tensor(162.3246)\n",
      "-285.4911551790798 0.0 -285.4911551790798 0.2631365142210279 tensor(159.4343)\n",
      "-285.9975754643381 0.0 -285.9975754643381 0.11244521943021936 tensor(143.8802)\n",
      "-286.167799137468 0.0 -286.167799137468 0.13057303432116943 tensor(193.3245)\n",
      "-285.9101314902405 0.0 -285.9101314902405 0.16758019302613836 tensor(179.1590)\n",
      "-286.0283852404322 0.0 -286.0283852404322 0.10425423095815238 tensor(175.1772)\n",
      "-285.28565590155114 0.0 -285.28565590155114 0.24177755632872341 tensor(178.5364)\n",
      "-285.55992223407884 0.0 -285.55992223407884 0.16093645744025906 tensor(174.1965)\n",
      "-285.5090475273259 0.0 -285.5090475273259 0.19199648176172993 tensor(165.6718)\n",
      "-285.5643496720979 0.0 -285.5643496720979 0.025165735053617914 tensor(149.2694)\n",
      "-285.65946266979324 0.0 -285.65946266979324 0.23745579630994743 tensor(171.5936)\n",
      "-285.97686956721475 0.0 -285.97686956721475 0.2389963151982018 tensor(171.9225)\n",
      "-285.780326560224 0.0 -285.780326560224 0.18065905708737146 tensor(164.8595)\n",
      "-286.1399537964311 0.0 -286.1399537964311 0.28431228718302154 tensor(149.7332)\n",
      "-285.0679787753785 0.0 -285.0679787753785 0.06653915860296575 tensor(189.3691)\n",
      "-285.95221940619604 0.0 -285.95221940619604 0.16713016144299656 tensor(147.3088)\n",
      "-285.102728405786 0.0 -285.102728405786 0.16943164752547002 tensor(154.0403)\n",
      "-285.9449710904536 0.0 -285.9449710904536 0.1905866090189777 tensor(156.4954)\n",
      "-285.3359436672664 0.0 -285.3359436672664 0.05919862536075672 tensor(147.4999)\n",
      "-285.8493522440597 0.0 -285.8493522440597 0.02916542139084924 tensor(172.8778)\n",
      "-285.6722934403898 0.0 -285.6722934403898 0.07606397676176742 tensor(149.7635)\n",
      "-286.0483398525318 0.0 -286.0483398525318 0.16844934143533194 tensor(143.6541)\n",
      "-285.3165836052095 0.0 -285.3165836052095 0.15058063873564667 tensor(184.5201)\n",
      "-286.04855617132927 0.0 -286.04855617132927 0.1369129194398237 tensor(161.3928)\n",
      "-285.85490224589705 0.0 -285.85490224589705 0.14551214859476544 tensor(159.7537)\n",
      "-286.0995853766601 0.0 -286.0995853766601 0.06316112282927659 tensor(168.0187)\n",
      "-285.54512764712405 0.0 -285.54512764712405 0.20625890614219056 tensor(178.3941)\n",
      "-285.8807443244534 0.0 -285.8807443244534 0.09429638143330366 tensor(187.6910)\n",
      "-285.3657718193308 0.0 -285.3657718193308 0.1348426685020567 tensor(160.1588)\n",
      "-285.55937227388847 0.0 -285.55937227388847 0.1751961648359966 tensor(162.6509)\n",
      "-285.5763420966449 0.0 -285.5763420966449 0.1801899603856416 tensor(147.5945)\n",
      "-285.70283333807157 0.0 -285.70283333807157 0.17589751229650613 tensor(148.2741)\n",
      "== Era 3 | Epoch 0 metrics ==\n",
      "\tloss -285.489\n",
      "\tforce 0\n",
      "\tdkl -285.489\n",
      "\tlogp 83.2097\n",
      "\tlogq -202.279\n",
      "\tess 0.12966\n",
      "-285.76680265178265 0.0 -285.76680265178265 0.10178840738930414 tensor(150.5435)\n",
      "-285.4680570493844 0.0 -285.4680570493844 0.05770085807668592 tensor(171.2564)\n",
      "-285.43206279194635 0.0 -285.43206279194635 0.1302547484754149 tensor(177.0882)\n",
      "-285.9759109310212 0.0 -285.9759109310212 0.15499422698248255 tensor(146.2332)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-285.21536683697167 0.0 -285.21536683697167 0.2189449581050638 tensor(171.8966)\n",
      "-285.48660306425217 0.0 -285.48660306425217 0.11321807829973943 tensor(187.9403)\n",
      "-285.5833949046988 0.0 -285.5833949046988 0.06373153825501945 tensor(158.9213)\n",
      "-285.48727051687985 0.0 -285.48727051687985 0.08399532584933865 tensor(182.5070)\n",
      "-285.54272284288584 0.0 -285.54272284288584 0.049358031771688855 tensor(175.0742)\n",
      "-285.1236794662193 0.0 -285.1236794662193 0.1780201144879282 tensor(176.3286)\n",
      "-285.84530449928945 0.0 -285.84530449928945 0.1956267839023526 tensor(155.9885)\n",
      "-286.1826945515368 0.0 -286.1826945515368 0.20304134682894795 tensor(165.7703)\n",
      "-285.68631223265453 0.0 -285.68631223265453 0.2116240683831535 tensor(173.1717)\n",
      "-285.77136434638396 0.0 -285.77136434638396 0.23513026087984623 tensor(168.5351)\n",
      "-285.317134575164 0.0 -285.317134575164 0.09205607013542556 tensor(185.0386)\n",
      "-285.3653766845781 0.0 -285.3653766845781 0.07107531753230847 tensor(166.2538)\n",
      "-285.29057236143143 0.0 -285.29057236143143 0.15304830221564364 tensor(146.5159)\n",
      "-286.194993757234 0.0 -286.194993757234 0.09924179134502198 tensor(140.5681)\n",
      "-285.59927213349647 0.0 -285.59927213349647 0.138491585194817 tensor(143.8572)\n",
      "-285.59891436134194 0.0 -285.59891436134194 0.06521984104092854 tensor(156.7595)\n",
      "-285.4790959430529 0.0 -285.4790959430529 0.08939270539170388 tensor(161.1441)\n",
      "-285.7907303176325 0.0 -285.7907303176325 0.23053553118396541 tensor(159.3303)\n",
      "-285.82516958254695 0.0 -285.82516958254695 0.14869992292426606 tensor(160.4531)\n",
      "-285.960931029814 0.0 -285.960931029814 0.043799591386091324 tensor(168.2859)\n",
      "-286.4194444673995 0.0 -286.4194444673995 0.20034135140201725 tensor(164.1387)\n",
      "-285.9451479834608 0.0 -285.9451479834608 0.13872383396379284 tensor(175.2018)\n",
      "-286.32251168264213 0.0 -286.32251168264213 0.03848087795001208 tensor(153.5619)\n",
      "-285.42136722124803 0.0 -285.42136722124803 0.17850418260060957 tensor(205.6805)\n",
      "-285.7594490444576 0.0 -285.7594490444576 0.05138424468062407 tensor(169.3042)\n",
      "-286.0376518776557 0.0 -286.0376518776557 0.026644427522124915 tensor(171.5180)\n",
      "-285.893226269323 0.0 -285.893226269323 0.17379707100369884 tensor(171.0951)\n",
      "-285.57799451698935 0.0 -285.57799451698935 0.22343545947788113 tensor(191.7971)\n",
      "-285.97831641676964 0.0 -285.97831641676964 0.12836732978515467 tensor(160.6582)\n",
      "-285.976227651725 0.0 -285.976227651725 0.13150862877770228 tensor(150.0928)\n",
      "-285.47954380100407 0.0 -285.47954380100407 0.1372598823552702 tensor(149.2705)\n",
      "-285.7372194510653 0.0 -285.7372194510653 0.22963611398457554 tensor(176.6230)\n",
      "-285.7004468420615 0.0 -285.7004468420615 0.1808495174883172 tensor(157.5890)\n",
      "-285.8348683267615 0.0 -285.8348683267615 0.04183876779620874 tensor(174.3954)\n",
      "-285.9213032885972 0.0 -285.9213032885972 0.10114782101154014 tensor(154.7946)\n",
      "-286.07203455042077 0.0 -286.07203455042077 0.024293322666988148 tensor(191.2808)\n",
      "-285.75147052969476 0.0 -285.75147052969476 0.2420243180111561 tensor(177.6644)\n",
      "-285.8471073049054 0.0 -285.8471073049054 0.20185830771964675 tensor(164.8439)\n",
      "-286.5346025128837 0.0 -286.5346025128837 0.1417978584258321 tensor(167.3637)\n",
      "-286.26594642885857 0.0 -286.26594642885857 0.24229393931184007 tensor(155.5533)\n",
      "-286.46954412497007 0.0 -286.46954412497007 0.1674040488495974 tensor(157.0791)\n",
      "-286.1955829134787 0.0 -286.1955829134787 0.1241252157557497 tensor(166.4006)\n",
      "-286.0118338041941 0.0 -286.0118338041941 0.13708992634023412 tensor(203.2334)\n",
      "-285.8520907490885 0.0 -285.8520907490885 0.1355173860068636 tensor(166.1003)\n",
      "-285.29339214728486 0.0 -285.29339214728486 0.236216330869415 tensor(181.9316)\n",
      "-285.50445627492036 0.0 -285.50445627492036 0.08912614334847203 tensor(202.8846)\n",
      "-285.9996268283801 0.0 -285.9996268283801 0.12103188366228283 tensor(185.2486)\n",
      "-286.33014126016315 0.0 -286.33014126016315 0.1345635379588023 tensor(170.2457)\n",
      "-286.1475059152067 0.0 -286.1475059152067 0.03190149422646425 tensor(174.7110)\n",
      "-286.02588765326664 0.0 -286.02588765326664 0.05249145428279456 tensor(161.2350)\n",
      "-285.5365893689632 0.0 -285.5365893689632 0.25670770516902813 tensor(223.7720)\n",
      "-285.95947827717254 0.0 -285.95947827717254 0.15799818594205406 tensor(202.8918)\n",
      "-285.6400397146826 0.0 -285.6400397146826 0.2400711275459497 tensor(174.7470)\n",
      "-286.4455366786875 0.0 -286.4455366786875 0.08333277784476924 tensor(164.8188)\n",
      "-285.8878082309569 0.0 -285.8878082309569 0.1609887125131096 tensor(185.6120)\n",
      "-285.8289919808018 0.0 -285.8289919808018 0.24177051142090017 tensor(154.5666)\n",
      "-286.3316744815624 0.0 -286.3316744815624 0.06472993476063993 tensor(173.8204)\n",
      "-285.92098997700356 0.0 -285.92098997700356 0.19738540597560233 tensor(163.4354)\n",
      "-286.0823241301757 0.0 -286.0823241301757 0.2121773610852956 tensor(187.3811)\n",
      "-286.30812725754396 0.0 -286.30812725754396 0.1853159004480566 tensor(161.9840)\n",
      "-285.80295934297976 0.0 -285.80295934297976 0.08005205322248768 tensor(182.2111)\n",
      "-286.3197841969145 0.0 -286.3197841969145 0.08261502238088958 tensor(135.4652)\n",
      "-285.87384796394616 0.0 -285.87384796394616 0.17381075075802488 tensor(146.1151)\n",
      "-285.9876876208972 0.0 -285.9876876208972 0.12174536353852483 tensor(172.5823)\n",
      "-286.55975521689857 0.0 -286.55975521689857 0.21983654002947806 tensor(166.0589)\n",
      "-285.9220225161214 0.0 -285.9220225161214 0.1876677241841585 tensor(167.5606)\n",
      "-286.0765987989716 0.0 -286.0765987989716 0.1885098960839824 tensor(181.0600)\n",
      "-286.1543955642343 0.0 -286.1543955642343 0.0901973308925188 tensor(152.5548)\n",
      "-285.7425184634802 0.0 -285.7425184634802 0.2645679241177861 tensor(155.5382)\n",
      "-285.97452982100253 0.0 -285.97452982100253 0.038548553330468416 tensor(182.2066)\n",
      "-285.4426810196418 0.0 -285.4426810196418 0.09961071888925913 tensor(205.5914)\n",
      "-286.4502022307532 0.0 -286.4502022307532 0.12253955978193826 tensor(209.6046)\n",
      "-285.87415506382865 0.0 -285.87415506382865 0.1806752779461716 tensor(176.3223)\n",
      "-285.633172957895 0.0 -285.633172957895 0.10505634224936243 tensor(170.0886)\n",
      "-286.2982599840825 0.0 -286.2982599840825 0.11071059692260705 tensor(173.9313)\n",
      "-286.3767390940999 0.0 -286.3767390940999 0.29578966140107604 tensor(154.7378)\n",
      "-285.87487732959596 0.0 -285.87487732959596 0.1507526796076606 tensor(199.8584)\n",
      "-286.5401057141871 0.0 -286.5401057141871 0.17198996883375975 tensor(163.6161)\n",
      "-285.68995636126783 0.0 -285.68995636126783 0.13228359304580573 tensor(173.1019)\n",
      "-285.61924434351533 0.0 -285.61924434351533 0.23296359856791202 tensor(155.4756)\n",
      "-286.16892160524014 0.0 -286.16892160524014 0.06991240478150683 tensor(187.4093)\n",
      "-286.469665153297 0.0 -286.469665153297 0.2715143102312879 tensor(165.6034)\n",
      "-286.2173028679772 0.0 -286.2173028679772 0.16109965486891273 tensor(140.0564)\n",
      "-286.5079164410247 0.0 -286.5079164410247 0.17137915789121885 tensor(193.0755)\n",
      "-286.3678080219322 0.0 -286.3678080219322 0.15875643250664329 tensor(164.9792)\n",
      "-285.8241610477585 0.0 -285.8241610477585 0.11775348729408863 tensor(175.3829)\n",
      "-286.16674065728085 0.0 -286.16674065728085 0.020939657942266734 tensor(173.2131)\n",
      "-286.3545227268155 0.0 -286.3545227268155 0.17067485404800142 tensor(167.2548)\n",
      "-285.9563202192909 0.0 -285.9563202192909 0.22817950998001785 tensor(185.0857)\n",
      "-286.09767824203163 0.0 -286.09767824203163 0.27414884142181106 tensor(172.3105)\n",
      "-285.90224733860464 0.0 -285.90224733860464 0.1813558481153591 tensor(185.0857)\n",
      "-286.5479033295622 0.0 -286.5479033295622 0.1859041591528892 tensor(158.8732)\n",
      "-286.4339286364053 0.0 -286.4339286364053 0.08786954500253773 tensor(152.5512)\n",
      "-285.8722643214861 0.0 -285.8722643214861 0.09119058925608116 tensor(194.9570)\n",
      "-286.2734946360102 0.0 -286.2734946360102 0.12517089549062624 tensor(177.3496)\n",
      "-285.7866118709935 0.0 -285.7866118709935 0.10672450011157118 tensor(189.8093)\n",
      "== Era 4 | Epoch 0 metrics ==\n",
      "\tloss -285.924\n",
      "\tforce 0\n",
      "\tdkl -285.924\n",
      "\tlogp 84.217\n",
      "\tlogq -201.707\n",
      "\tess 0.144656\n",
      "-286.42959024970804 0.0 -286.42959024970804 0.24351044375936184 tensor(170.9347)\n",
      "-286.0740382370601 0.0 -286.0740382370601 0.12732676696086598 tensor(180.7658)\n",
      "-286.1718182842369 0.0 -286.1718182842369 0.116711414046523 tensor(164.3912)\n",
      "-286.3227836646729 0.0 -286.3227836646729 0.06557855207650502 tensor(159.9655)\n",
      "-286.299079295947 0.0 -286.299079295947 0.24660392220856947 tensor(178.4996)\n",
      "-286.70345074852196 0.0 -286.70345074852196 0.1302508449775322 tensor(180.4461)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.2298298684607 0.0 -286.2298298684607 0.20246256551853137 tensor(171.2551)\n",
      "-286.18202929008567 0.0 -286.18202929008567 0.15544772488882455 tensor(171.7185)\n",
      "-286.30195926420214 0.0 -286.30195926420214 0.20824601214829314 tensor(164.9257)\n",
      "-286.8255649181762 0.0 -286.8255649181762 0.29026847375326403 tensor(173.1734)\n",
      "-286.41775810069305 0.0 -286.41775810069305 0.10261035785947044 tensor(182.5192)\n",
      "-286.356919937346 0.0 -286.356919937346 0.057770890006515106 tensor(164.2266)\n",
      "-286.17847129446477 0.0 -286.17847129446477 0.09900306937627477 tensor(172.7846)\n",
      "-286.4338164251212 0.0 -286.4338164251212 0.0916416689717375 tensor(175.3758)\n",
      "-286.51255808334076 0.0 -286.51255808334076 0.08394243379648351 tensor(184.6040)\n",
      "-286.49338604286265 0.0 -286.49338604286265 0.15191005068858007 tensor(171.0245)\n",
      "-286.1197153753396 0.0 -286.1197153753396 0.0958604712796014 tensor(221.3763)\n",
      "-286.27289834545667 0.0 -286.27289834545667 0.1455096301059904 tensor(202.0609)\n",
      "-286.68317381021313 0.0 -286.68317381021313 0.12920912774993898 tensor(201.8453)\n",
      "-286.39462466446287 0.0 -286.39462466446287 0.19459712125247008 tensor(164.9536)\n",
      "-286.3604874535927 0.0 -286.3604874535927 0.1296597048494416 tensor(178.8615)\n",
      "-286.36705685915075 0.0 -286.36705685915075 0.2742197354200487 tensor(155.8174)\n",
      "-286.20462568386733 0.0 -286.20462568386733 0.22485183099814623 tensor(185.4790)\n",
      "-286.12836791766364 0.0 -286.12836791766364 0.08275124585750528 tensor(281.1231)\n",
      "-286.6117109738009 0.0 -286.6117109738009 0.1020774712366396 tensor(173.9787)\n",
      "-286.26260556901263 0.0 -286.26260556901263 0.07285659243361474 tensor(222.5802)\n",
      "-286.1339083678083 0.0 -286.1339083678083 0.10001094752430129 tensor(147.5014)\n",
      "-286.34406296334055 0.0 -286.34406296334055 0.2655427345020945 tensor(170.0566)\n",
      "-286.37017973203166 0.0 -286.37017973203166 0.13641923026956596 tensor(191.7171)\n",
      "-286.3499863641964 0.0 -286.3499863641964 0.3042817508929933 tensor(177.0218)\n",
      "-286.4322362824419 0.0 -286.4322362824419 0.14243097979171734 tensor(172.3253)\n",
      "-286.1202835592079 0.0 -286.1202835592079 0.2492646938617458 tensor(158.2405)\n",
      "-286.13506804534234 0.0 -286.13506804534234 0.18106336223396227 tensor(209.8453)\n",
      "-286.0834476451129 0.0 -286.0834476451129 0.1980709794692317 tensor(193.3350)\n",
      "-286.48084908178316 0.0 -286.48084908178316 0.20646733192527633 tensor(181.6948)\n",
      "-285.8432581551582 0.0 -285.8432581551582 0.22920793710131365 tensor(179.3602)\n",
      "-286.7683761952823 0.0 -286.7683761952823 0.0589997804557845 tensor(163.0777)\n",
      "-286.01088668452735 0.0 -286.01088668452735 0.16985894635786447 tensor(192.0954)\n",
      "-286.29662743242494 0.0 -286.29662743242494 0.1633249183537684 tensor(166.4388)\n",
      "-286.1772179661939 0.0 -286.1772179661939 0.17502150856508106 tensor(187.4265)\n",
      "-286.2757823964347 0.0 -286.2757823964347 0.16013411326161078 tensor(161.2580)\n",
      "-286.38526643810576 0.0 -286.38526643810576 0.2906802960423347 tensor(205.7795)\n",
      "-286.72890831508374 0.0 -286.72890831508374 0.241682386662042 tensor(217.0607)\n",
      "-286.2096723499476 0.0 -286.2096723499476 0.18256483201245008 tensor(163.0453)\n",
      "-286.34177108992986 0.0 -286.34177108992986 0.16533312798773384 tensor(151.1778)\n",
      "-286.3069250376759 0.0 -286.3069250376759 0.09986431235347717 tensor(143.4916)\n",
      "-286.16368281278324 0.0 -286.16368281278324 0.08215698995943373 tensor(176.1274)\n",
      "-286.4737532601969 0.0 -286.4737532601969 0.1810548921981758 tensor(159.3108)\n",
      "-286.7113654575383 0.0 -286.7113654575383 0.17136245122968244 tensor(228.7653)\n",
      "-286.491240677453 0.0 -286.491240677453 0.1305883210937324 tensor(180.9383)\n",
      "-286.3822576660283 0.0 -286.3822576660283 0.18351742045906902 tensor(189.0697)\n",
      "-286.41512519173375 0.0 -286.41512519173375 0.1512580500294449 tensor(163.2630)\n",
      "-286.7619151824822 0.0 -286.7619151824822 0.2739942248808521 tensor(158.4886)\n",
      "-286.52777884895266 0.0 -286.52777884895266 0.18331230839384804 tensor(174.5121)\n",
      "-285.9495677562031 0.0 -285.9495677562031 0.07384014198355439 tensor(181.3682)\n",
      "-286.3835881863752 0.0 -286.3835881863752 0.10113706071795417 tensor(158.0260)\n",
      "-286.1952465800113 0.0 -286.1952465800113 0.12883759989489954 tensor(176.9169)\n",
      "-286.6455236473365 0.0 -286.6455236473365 0.30285692706796186 tensor(184.4518)\n",
      "-286.72693652522236 0.0 -286.72693652522236 0.03574912889142957 tensor(157.6688)\n",
      "-286.76638036023456 0.0 -286.76638036023456 0.24600875136326375 tensor(155.9466)\n",
      "-286.07434900170915 0.0 -286.07434900170915 0.22247358436405676 tensor(185.2419)\n",
      "-286.7687555973031 0.0 -286.7687555973031 0.19707555164191023 tensor(163.4817)\n",
      "-286.4833996630453 0.0 -286.4833996630453 0.12648303909988678 tensor(153.5934)\n",
      "-286.04819027750347 0.0 -286.04819027750347 0.1457258704919931 tensor(171.2230)\n",
      "-286.5048777915194 0.0 -286.5048777915194 0.16909946494888867 tensor(155.9793)\n",
      "-286.409307676 0.0 -286.409307676 0.04236132174907411 tensor(184.4347)\n",
      "-286.4550086984305 0.0 -286.4550086984305 0.21585988580929039 tensor(164.3824)\n",
      "-286.24318174240875 0.0 -286.24318174240875 0.21595584844296145 tensor(190.5562)\n",
      "-286.5172184633291 0.0 -286.5172184633291 0.1323959799120019 tensor(172.8468)\n",
      "-285.9467410978417 0.0 -285.9467410978417 0.1485402934391213 tensor(169.3668)\n",
      "-286.7659284988707 0.0 -286.7659284988707 0.13750762183975646 tensor(166.9726)\n",
      "-286.25702644853527 0.0 -286.25702644853527 0.158551860524976 tensor(182.2817)\n",
      "-286.531314157436 0.0 -286.531314157436 0.14406973651429988 tensor(179.6070)\n",
      "-286.4876605485566 0.0 -286.4876605485566 0.20212125909306147 tensor(162.7002)\n",
      "-286.2840868469009 0.0 -286.2840868469009 0.24052119920208195 tensor(152.6455)\n",
      "-286.2800081722945 0.0 -286.2800081722945 0.18777512806378766 tensor(185.0891)\n",
      "-286.51070299089474 0.0 -286.51070299089474 0.1545593295446512 tensor(230.3852)\n",
      "-286.6711988197896 0.0 -286.6711988197896 0.1415901966556269 tensor(166.9662)\n",
      "-286.53649607713976 0.0 -286.53649607713976 0.13383364923159788 tensor(247.8737)\n",
      "-286.1618468575662 0.0 -286.1618468575662 0.16904615302556303 tensor(189.1767)\n",
      "-286.5643491303338 0.0 -286.5643491303338 0.21586749499821575 tensor(186.4219)\n",
      "-286.3273430443488 0.0 -286.3273430443488 0.272065170061095 tensor(197.2730)\n",
      "-286.369387169244 0.0 -286.369387169244 0.27864863898843667 tensor(194.3047)\n",
      "-286.14769982957205 0.0 -286.14769982957205 0.3046825256956573 tensor(173.2453)\n",
      "-286.49633240405007 0.0 -286.49633240405007 0.24442785278900736 tensor(135.9470)\n",
      "-286.67331473333684 0.0 -286.67331473333684 0.2715268282234635 tensor(184.6945)\n",
      "-286.4248044605591 0.0 -286.4248044605591 0.26856642173713435 tensor(176.1258)\n",
      "-286.6305212773797 0.0 -286.6305212773797 0.13970776220000963 tensor(162.9548)\n",
      "-286.2573226237998 0.0 -286.2573226237998 0.2086851270510408 tensor(165.9216)\n",
      "-286.6079943157053 0.0 -286.6079943157053 0.1130114257718107 tensor(154.4492)\n",
      "-285.6891103529476 0.0 -285.6891103529476 0.306908018664625 tensor(206.1992)\n",
      "-286.3423568771399 0.0 -286.3423568771399 0.09849810110332871 tensor(158.1010)\n",
      "-286.3373000227875 0.0 -286.3373000227875 0.3317959084104223 tensor(208.6203)\n",
      "-286.06390505558227 0.0 -286.06390505558227 0.05635830098460826 tensor(187.8571)\n",
      "-286.5184088559827 0.0 -286.5184088559827 0.1530772473127151 tensor(178.8530)\n",
      "-286.46229994265735 0.0 -286.46229994265735 0.2959265342131058 tensor(170.6431)\n",
      "-286.81288220857016 0.0 -286.81288220857016 0.262570419064978 tensor(177.0776)\n",
      "-286.4007356197782 0.0 -286.4007356197782 0.1843449347116461 tensor(163.2859)\n",
      "-286.5886567664134 0.0 -286.5886567664134 0.29876959236323963 tensor(172.5751)\n",
      "-286.749335265634 0.0 -286.749335265634 0.23340131117957755 tensor(171.6258)\n",
      "== Era 5 | Epoch 0 metrics ==\n",
      "\tloss -286.381\n",
      "\tforce 0\n",
      "\tdkl -286.381\n",
      "\tlogp 85.1085\n",
      "\tlogq -201.273\n",
      "\tess 0.176152\n",
      "-286.4694728347779 0.0 -286.4694728347779 0.06487870408447122 tensor(233.4533)\n",
      "-286.5149657154843 0.0 -286.5149657154843 0.29568363798565467 tensor(194.0043)\n",
      "-286.1856827793247 0.0 -286.1856827793247 0.12268512047419268 tensor(184.5077)\n",
      "-286.285702425188 0.0 -286.285702425188 0.10894453087348621 tensor(209.0626)\n",
      "-287.0014646467125 0.0 -287.0014646467125 0.1451389266842331 tensor(238.8169)\n",
      "-286.39774068923083 0.0 -286.39774068923083 0.15374232832706766 tensor(191.8957)\n",
      "-286.8200868343644 0.0 -286.8200868343644 0.11441787361346736 tensor(222.3287)\n",
      "-286.9299290827603 0.0 -286.9299290827603 0.06636762698947489 tensor(167.1097)\n",
      "-286.53812187217574 0.0 -286.53812187217574 0.21287292227234805 tensor(259.0772)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.1030487674826 0.0 -286.1030487674826 0.14757232188571232 tensor(177.4414)\n",
      "-286.79881426928785 0.0 -286.79881426928785 0.19925762210835296 tensor(175.7745)\n",
      "-286.8895556518922 0.0 -286.8895556518922 0.20663419603924182 tensor(212.8828)\n",
      "-286.73436938298016 0.0 -286.73436938298016 0.2396118599507676 tensor(198.8368)\n",
      "-286.3219745816881 0.0 -286.3219745816881 0.1414377985216021 tensor(222.2222)\n",
      "-286.2504391436577 0.0 -286.2504391436577 0.14604459521012364 tensor(155.7067)\n",
      "-286.80813812640076 0.0 -286.80813812640076 0.2522846437389289 tensor(207.6366)\n",
      "-286.7118538872095 0.0 -286.7118538872095 0.10008081763760789 tensor(187.6564)\n",
      "-286.7248982663979 0.0 -286.7248982663979 0.24895956847581732 tensor(175.3379)\n",
      "-286.25743717191267 0.0 -286.25743717191267 0.2283220077208355 tensor(182.6109)\n",
      "-286.70812550199093 0.0 -286.70812550199093 0.12354206327816915 tensor(155.5527)\n",
      "-287.02429185487904 0.0 -287.02429185487904 0.2074911297505572 tensor(183.9654)\n",
      "-286.7605332923435 0.0 -286.7605332923435 0.18745167359066117 tensor(158.8124)\n",
      "-286.1704688245992 0.0 -286.1704688245992 0.32089527814509516 tensor(226.3543)\n",
      "-286.5882127030257 0.0 -286.5882127030257 0.10410378392104733 tensor(154.4433)\n",
      "-286.71172426551567 0.0 -286.71172426551567 0.05887054200879128 tensor(197.1583)\n",
      "-287.0180851086522 0.0 -287.0180851086522 0.14303277386700203 tensor(183.5023)\n",
      "-286.6987684964759 0.0 -286.6987684964759 0.06128725822005252 tensor(166.1700)\n",
      "-286.48528612818455 0.0 -286.48528612818455 0.08404510499129332 tensor(206.7670)\n",
      "-286.8280337964889 0.0 -286.8280337964889 0.10290575137752386 tensor(221.7487)\n",
      "-286.68425852082265 0.0 -286.68425852082265 0.2102484987911565 tensor(149.8141)\n",
      "-286.27803629628227 0.0 -286.27803629628227 0.17444551160962468 tensor(174.8035)\n",
      "-286.5572779699839 0.0 -286.5572779699839 0.38354634124464143 tensor(179.0352)\n",
      "-286.64753322560676 0.0 -286.64753322560676 0.18789436337107973 tensor(170.0627)\n",
      "-287.26555914348137 0.0 -287.26555914348137 0.11634278278355245 tensor(181.3099)\n",
      "-286.5768805473442 0.0 -286.5768805473442 0.16344975453955896 tensor(153.4133)\n",
      "-286.5284512019856 0.0 -286.5284512019856 0.28615729453513966 tensor(169.6821)\n",
      "-286.41556123802116 0.0 -286.41556123802116 0.21209274176687243 tensor(176.9616)\n",
      "-286.78766257792313 0.0 -286.78766257792313 0.22988717525143265 tensor(200.1625)\n",
      "-286.5152617935726 0.0 -286.5152617935726 0.19018519514913398 tensor(176.7538)\n",
      "-286.35271339865426 0.0 -286.35271339865426 0.0817979253233195 tensor(184.3849)\n",
      "-286.9216427437409 0.0 -286.9216427437409 0.24815558192116563 tensor(177.7186)\n",
      "-286.5355535742622 0.0 -286.5355535742622 0.15126096575715642 tensor(220.5007)\n",
      "-286.04394912389034 0.0 -286.04394912389034 0.2917677887953626 tensor(209.4176)\n",
      "-286.3549369299805 0.0 -286.3549369299805 0.2086820494724529 tensor(243.2676)\n",
      "-286.33183494451794 0.0 -286.33183494451794 0.1515122991766604 tensor(192.7315)\n",
      "-286.5680532926746 0.0 -286.5680532926746 0.3048053057336233 tensor(156.6109)\n",
      "-286.578384581574 0.0 -286.578384581574 0.17918392298169425 tensor(165.7022)\n",
      "-286.66870635981786 0.0 -286.66870635981786 0.32558083962233736 tensor(267.9098)\n",
      "-286.65636602479185 0.0 -286.65636602479185 0.11479358468604112 tensor(187.0022)\n",
      "-286.61495328872877 0.0 -286.61495328872877 0.17581434090990938 tensor(239.4020)\n",
      "-286.37977282650644 0.0 -286.37977282650644 0.23107990452053456 tensor(167.0402)\n",
      "-286.72540306162443 0.0 -286.72540306162443 0.1548446294498089 tensor(230.5324)\n",
      "-286.2337514481939 0.0 -286.2337514481939 0.061624929556468706 tensor(200.0953)\n",
      "-286.2878414091989 0.0 -286.2878414091989 0.18309349412661494 tensor(176.0887)\n",
      "-286.32500640660214 0.0 -286.32500640660214 0.18163100083592837 tensor(195.9829)\n",
      "-286.3737273666752 0.0 -286.3737273666752 0.1657721049925896 tensor(252.9528)\n",
      "-286.67228729776434 0.0 -286.67228729776434 0.30953741703443166 tensor(243.4074)\n",
      "-286.1962004731173 0.0 -286.1962004731173 0.18296378041876787 tensor(172.7544)\n",
      "-286.7697668144303 0.0 -286.7697668144303 0.15133248620077658 tensor(189.4724)\n",
      "-286.5913254035218 0.0 -286.5913254035218 0.2391747288953078 tensor(200.3641)\n",
      "-286.81713171005947 0.0 -286.81713171005947 0.1789171764972667 tensor(197.6193)\n",
      "-286.1447654218726 0.0 -286.1447654218726 0.30480665454102296 tensor(180.5477)\n",
      "-286.2681119708737 0.0 -286.2681119708737 0.23083953029085344 tensor(220.7432)\n",
      "-286.6228704485784 0.0 -286.6228704485784 0.21782239208029053 tensor(324.4716)\n",
      "-286.51989652956746 0.0 -286.51989652956746 0.18534961265134994 tensor(182.7521)\n",
      "-286.8695591128534 0.0 -286.8695591128534 0.026826299319645804 tensor(153.9097)\n",
      "-286.8899283170166 0.0 -286.8899283170166 0.29757876155518065 tensor(183.4833)\n",
      "-286.4178567244479 0.0 -286.4178567244479 0.09804426502225087 tensor(154.8524)\n",
      "-286.82715687398564 0.0 -286.82715687398564 0.10369779241722621 tensor(155.6991)\n",
      "-286.6171198900208 0.0 -286.6171198900208 0.13167871057531805 tensor(186.0081)\n",
      "-286.8377967203575 0.0 -286.8377967203575 0.07999511130493361 tensor(177.0422)\n",
      "-286.4198936980587 0.0 -286.4198936980587 0.21035928646118238 tensor(195.7338)\n",
      "-286.7344751830692 0.0 -286.7344751830692 0.3072991516882668 tensor(166.8233)\n",
      "-286.3771939050033 0.0 -286.3771939050033 0.16197827070461385 tensor(215.9955)\n",
      "-286.3667771868529 0.0 -286.3667771868529 0.21811714821041694 tensor(202.8112)\n",
      "-286.4024540376181 0.0 -286.4024540376181 0.1496566850175567 tensor(241.9607)\n",
      "-286.88422699203704 0.0 -286.88422699203704 0.2247936617255283 tensor(160.6115)\n",
      "-286.58863292357 0.0 -286.58863292357 0.2001819845718079 tensor(235.6666)\n",
      "-286.5917077869422 0.0 -286.5917077869422 0.31267365103455386 tensor(202.0222)\n",
      "-286.5990810877936 0.0 -286.5990810877936 0.3934638707933332 tensor(164.6092)\n",
      "-286.1774537968739 0.0 -286.1774537968739 0.10444206166790394 tensor(174.6839)\n",
      "-286.8953880750404 0.0 -286.8953880750404 0.09175931027535772 tensor(148.5917)\n",
      "-286.44408454333495 0.0 -286.44408454333495 0.24286855815747188 tensor(221.4481)\n",
      "-286.7113982268275 0.0 -286.7113982268275 0.16081735830372548 tensor(198.2297)\n",
      "-286.7460597379145 0.0 -286.7460597379145 0.1755405277154289 tensor(431.1631)\n",
      "-286.27408595016 0.0 -286.27408595016 0.11974278897381994 tensor(172.8775)\n",
      "-286.59090433569276 0.0 -286.59090433569276 0.11723801090008996 tensor(210.9138)\n",
      "-286.40423469460467 0.0 -286.40423469460467 0.18805597331165366 tensor(209.6004)\n",
      "-286.4571477178417 0.0 -286.4571477178417 0.21405447031686328 tensor(169.5390)\n",
      "-286.90682964946916 0.0 -286.90682964946916 0.24842130393459386 tensor(153.4132)\n",
      "-286.5380180920626 0.0 -286.5380180920626 0.21356970152812277 tensor(196.4168)\n",
      "-286.64853996061834 0.0 -286.64853996061834 0.15280591574019817 tensor(200.8963)\n",
      "-286.90723113166865 0.0 -286.90723113166865 0.12424519960492902 tensor(225.1027)\n",
      "-286.57083248928825 0.0 -286.57083248928825 0.3359013228374444 tensor(168.0175)\n",
      "-286.45100638887834 0.0 -286.45100638887834 0.07789495270497877 tensor(142.6255)\n",
      "-286.57961599932213 0.0 -286.57961599932213 0.17513324995092072 tensor(161.6243)\n",
      "-286.7311068778676 0.0 -286.7311068778676 0.11339085364615958 tensor(193.6215)\n",
      "-286.73955385647605 0.0 -286.73955385647605 0.25724075328498935 tensor(199.9445)\n",
      "-286.91504978587875 0.0 -286.91504978587875 0.24097412958824715 tensor(165.8817)\n",
      "-286.9768045065158 0.0 -286.9768045065158 0.35185963469161835 tensor(175.6196)\n",
      "== Era 6 | Epoch 0 metrics ==\n",
      "\tloss -286.586\n",
      "\tforce 0\n",
      "\tdkl -286.586\n",
      "\tlogp 85.4844\n",
      "\tlogq -201.102\n",
      "\tess 0.185773\n",
      "-286.2125007419434 0.0 -286.2125007419434 0.11219675073055438 tensor(198.8597)\n",
      "-286.70537562262234 0.0 -286.70537562262234 0.27236067410036774 tensor(226.5222)\n",
      "-286.75614476510657 0.0 -286.75614476510657 0.05604808510122567 tensor(182.7325)\n",
      "-286.6510531998024 0.0 -286.6510531998024 0.0679669358901609 tensor(222.1213)\n",
      "-286.8060090754005 0.0 -286.8060090754005 0.2206663876448064 tensor(205.1283)\n",
      "-286.8396209238668 0.0 -286.8396209238668 0.04996853562188655 tensor(330.5075)\n",
      "-286.57823477089255 0.0 -286.57823477089255 0.25769687772749017 tensor(173.5427)\n",
      "-286.4376849136971 0.0 -286.4376849136971 0.24694264442102937 tensor(202.9833)\n",
      "-285.995699538887 0.0 -285.995699538887 0.09332177194680345 tensor(196.0997)\n",
      "-286.8836865099015 0.0 -286.8836865099015 0.32105523698328486 tensor(172.0458)\n",
      "-286.92523266465344 0.0 -286.92523266465344 0.11408531746493174 tensor(236.2626)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.37424945966757 0.0 -286.37424945966757 0.27631506150752144 tensor(187.5516)\n",
      "-286.87764579615214 0.0 -286.87764579615214 0.33560314012720543 tensor(196.6000)\n",
      "-286.6968894805847 0.0 -286.6968894805847 0.18031224362893333 tensor(187.4604)\n",
      "-286.5287983371965 0.0 -286.5287983371965 0.20499660178731807 tensor(196.2932)\n",
      "-286.2364603642544 0.0 -286.2364603642544 0.11616869579152166 tensor(197.9263)\n",
      "-286.1971812865029 0.0 -286.1971812865029 0.11281678713560853 tensor(253.9944)\n",
      "-287.031403529598 0.0 -287.031403529598 0.08090413860815306 tensor(195.7730)\n",
      "-286.32109874125695 0.0 -286.32109874125695 0.2542890375275158 tensor(172.7984)\n",
      "-286.5775680322279 0.0 -286.5775680322279 0.2325613031681031 tensor(189.0569)\n",
      "-286.5379995265555 0.0 -286.5379995265555 0.059621040490759954 tensor(224.7545)\n",
      "-286.85048280028434 0.0 -286.85048280028434 0.07016776209457493 tensor(265.4737)\n",
      "-286.8202146132348 0.0 -286.8202146132348 0.25341311544609363 tensor(286.0131)\n",
      "-286.6823554320838 0.0 -286.6823554320838 0.18071701163551032 tensor(179.6245)\n",
      "-286.3014864006335 0.0 -286.3014864006335 0.21088596677885774 tensor(207.0568)\n",
      "-286.6140921388846 0.0 -286.6140921388846 0.14040899191670625 tensor(298.1315)\n",
      "-286.80674065751805 0.0 -286.80674065751805 0.12410619749033924 tensor(165.5810)\n",
      "-286.7354118260868 0.0 -286.7354118260868 0.16866001353565216 tensor(194.4968)\n",
      "-286.6320644679821 0.0 -286.6320644679821 0.23531880381654224 tensor(219.1721)\n",
      "-286.6515847720654 0.0 -286.6515847720654 0.2267386798412177 tensor(183.4203)\n",
      "-286.42914925652394 0.0 -286.42914925652394 0.32839669153711093 tensor(170.0604)\n",
      "-286.4208695252359 0.0 -286.4208695252359 0.2797013523319527 tensor(170.6312)\n",
      "-286.9402371937241 0.0 -286.9402371937241 0.27481974451788543 tensor(197.9817)\n",
      "-286.8746118143166 0.0 -286.8746118143166 0.3208634743253499 tensor(188.4059)\n",
      "-286.7840684295199 0.0 -286.7840684295199 0.15092144843146962 tensor(159.1613)\n",
      "-286.7563154522459 0.0 -286.7563154522459 0.0921571474421957 tensor(210.1781)\n",
      "-286.65002642815756 0.0 -286.65002642815756 0.26324554439604186 tensor(152.1128)\n",
      "-286.83029865867655 0.0 -286.83029865867655 0.2677451618461274 tensor(225.3008)\n",
      "-286.38730414280286 0.0 -286.38730414280286 0.10095315591703363 tensor(259.9836)\n",
      "-286.6013779706301 0.0 -286.6013779706301 0.26570361254236474 tensor(185.9333)\n",
      "-286.45470885436305 0.0 -286.45470885436305 0.33869785682406517 tensor(186.2503)\n",
      "-286.51877664138294 0.0 -286.51877664138294 0.155182765565348 tensor(200.1457)\n",
      "-286.58725473603704 0.0 -286.58725473603704 0.14145280312159528 tensor(189.6405)\n",
      "-286.878146786651 0.0 -286.878146786651 0.41183923813268825 tensor(196.1775)\n",
      "-286.5791812887837 0.0 -286.5791812887837 0.2173755305208758 tensor(260.5414)\n",
      "-286.6278313889331 0.0 -286.6278313889331 0.1099070059709188 tensor(162.8724)\n",
      "-286.53153137641357 0.0 -286.53153137641357 0.26521449172933537 tensor(192.0945)\n",
      "-286.74448990536473 0.0 -286.74448990536473 0.18283687406369448 tensor(188.3584)\n",
      "-286.5597001707928 0.0 -286.5597001707928 0.19898343248334796 tensor(194.2814)\n",
      "-286.6115871794217 0.0 -286.6115871794217 0.3086283140079877 tensor(206.4952)\n",
      "-286.3609083398691 0.0 -286.3609083398691 0.16085722005713804 tensor(158.3240)\n",
      "-286.78813215318985 0.0 -286.78813215318985 0.13951516303045386 tensor(204.5644)\n",
      "-286.72173172634575 0.0 -286.72173172634575 0.19788614707783728 tensor(215.1962)\n",
      "-286.4439396966185 0.0 -286.4439396966185 0.061688805923587005 tensor(157.8478)\n",
      "-286.7503900512121 0.0 -286.7503900512121 0.32582665885806783 tensor(216.7415)\n",
      "-286.4095079437234 0.0 -286.4095079437234 0.24790745838319583 tensor(201.4469)\n",
      "-286.6494652515171 0.0 -286.6494652515171 0.28402049881208774 tensor(176.2446)\n",
      "-286.5465233792516 0.0 -286.5465233792516 0.34151115928469 tensor(189.6890)\n",
      "-286.6230182559887 0.0 -286.6230182559887 0.12009408093201573 tensor(255.1516)\n",
      "-286.7756122904022 0.0 -286.7756122904022 0.3117107973129052 tensor(187.5634)\n",
      "-286.53470649665115 0.0 -286.53470649665115 0.2762523093480124 tensor(236.8669)\n",
      "-286.71414155996695 0.0 -286.71414155996695 0.2845596369541384 tensor(175.9638)\n",
      "-286.543896486578 0.0 -286.543896486578 0.18547754226507798 tensor(206.2546)\n",
      "-286.9345347148501 0.0 -286.9345347148501 0.37416298114097457 tensor(183.7532)\n",
      "-286.6980996338018 0.0 -286.6980996338018 0.22474591839138616 tensor(209.6125)\n",
      "-286.8429684602472 0.0 -286.8429684602472 0.2714468811474898 tensor(217.8358)\n",
      "-286.5370925578208 0.0 -286.5370925578208 0.30852340007909157 tensor(189.3450)\n",
      "-286.68009629441315 0.0 -286.68009629441315 0.20119090842624845 tensor(234.4152)\n",
      "-286.4574006872682 0.0 -286.4574006872682 0.33738422365135273 tensor(227.4968)\n",
      "-286.2390885353174 0.0 -286.2390885353174 0.4097915401760189 tensor(213.2955)\n",
      "-286.56895830755326 0.0 -286.56895830755326 0.19375967093011898 tensor(231.6656)\n",
      "-286.5564435017346 0.0 -286.5564435017346 0.11978665950755335 tensor(273.8718)\n",
      "-286.84443992172675 0.0 -286.84443992172675 0.3513389278919385 tensor(197.0016)\n",
      "-286.6871366392286 0.0 -286.6871366392286 0.23436969462475354 tensor(176.7959)\n",
      "-286.7898769363979 0.0 -286.7898769363979 0.29474742907413776 tensor(186.0447)\n",
      "-286.83715455042056 0.0 -286.83715455042056 0.24042250917322727 tensor(219.5415)\n",
      "-287.18655932445665 0.0 -287.18655932445665 0.1775101681317729 tensor(172.1320)\n",
      "-286.9248735049554 0.0 -286.9248735049554 0.262922972427854 tensor(180.7272)\n",
      "-286.407175004327 0.0 -286.407175004327 0.05419561013204531 tensor(197.8685)\n",
      "-286.5194793904886 0.0 -286.5194793904886 0.2067639614423958 tensor(231.6638)\n",
      "-286.67415135918213 0.0 -286.67415135918213 0.2856551402099014 tensor(201.4698)\n",
      "-286.9066633155428 0.0 -286.9066633155428 0.24523569372091217 tensor(153.4754)\n",
      "-286.8271050208193 0.0 -286.8271050208193 0.08316803568039036 tensor(186.9275)\n",
      "-286.5733344076358 0.0 -286.5733344076358 0.17450004474690087 tensor(195.1049)\n",
      "-286.4163495165835 0.0 -286.4163495165835 0.22170545150733006 tensor(221.7619)\n",
      "-286.88252194132883 0.0 -286.88252194132883 0.11497841536752264 tensor(228.2378)\n",
      "-286.65238914905416 0.0 -286.65238914905416 0.1729764660791576 tensor(217.2579)\n",
      "-286.5899772672456 0.0 -286.5899772672456 0.15763493881799018 tensor(175.4587)\n",
      "-286.6480691781528 0.0 -286.6480691781528 0.37677911328432007 tensor(184.9854)\n",
      "-286.5997523468326 0.0 -286.5997523468326 0.18625338424976343 tensor(181.8500)\n",
      "-287.12120102376866 0.0 -287.12120102376866 0.19181453819846336 tensor(190.5651)\n",
      "-286.63601724066626 0.0 -286.63601724066626 0.2306637206127523 tensor(217.6746)\n",
      "-286.94842136431726 0.0 -286.94842136431726 0.25614675625706784 tensor(302.1189)\n",
      "-286.63965408333297 0.0 -286.63965408333297 0.32535272914149554 tensor(193.6534)\n",
      "-286.62371976393393 0.0 -286.62371976393393 0.1669683684034433 tensor(197.4220)\n",
      "-287.21057526035503 0.0 -287.21057526035503 0.2819309880897823 tensor(149.8241)\n",
      "-286.9549901406052 0.0 -286.9549901406052 0.2761039932259491 tensor(221.6007)\n",
      "-286.84838255180966 0.0 -286.84838255180966 0.30223789278410096 tensor(189.3126)\n",
      "-286.92719870007915 0.0 -286.92719870007915 0.24196204834294982 tensor(232.8892)\n",
      "-287.1145231496499 0.0 -287.1145231496499 0.17531856668890153 tensor(220.5251)\n",
      "== Era 7 | Epoch 0 metrics ==\n",
      "\tloss -286.664\n",
      "\tforce 0\n",
      "\tdkl -286.664\n",
      "\tlogp 85.9265\n",
      "\tlogq -200.738\n",
      "\tess 0.216187\n",
      "-286.7051275310812 0.0 -286.7051275310812 0.23521093743121987 tensor(237.1226)\n",
      "-286.4789489761972 0.0 -286.4789489761972 0.25368064171832877 tensor(270.8951)\n",
      "-286.73700187148575 0.0 -286.73700187148575 0.2730485905582012 tensor(174.3496)\n",
      "-286.8733668970583 0.0 -286.8733668970583 0.2246739976454993 tensor(175.4882)\n",
      "-287.2029130305981 0.0 -287.2029130305981 0.133693336942518 tensor(188.1369)\n",
      "-286.7711728949485 0.0 -286.7711728949485 0.20046513830369553 tensor(192.3513)\n",
      "-286.5221267999548 0.0 -286.5221267999548 0.08701893639566909 tensor(189.5513)\n",
      "-286.9623104472212 0.0 -286.9623104472212 0.3758329356359306 tensor(202.0069)\n",
      "-286.8410752864833 0.0 -286.8410752864833 0.24720005044621923 tensor(230.7300)\n",
      "-286.6040887073526 0.0 -286.6040887073526 0.13268491346132963 tensor(252.9725)\n",
      "-286.67027704272886 0.0 -286.67027704272886 0.2517863465869032 tensor(188.4664)\n",
      "-286.7004461969693 0.0 -286.7004461969693 0.3472014160021392 tensor(223.4673)\n",
      "-286.8712325010344 0.0 -286.8712325010344 0.40484456864789053 tensor(187.7260)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-286.829989939537 0.0 -286.829989939537 0.2897993723674256 tensor(158.2422)\n",
      "-286.55234764676885 0.0 -286.55234764676885 0.22451236753008907 tensor(182.5948)\n",
      "-286.9383151640146 0.0 -286.9383151640146 0.14170541945339032 tensor(295.1155)\n",
      "-286.9684003038342 0.0 -286.9684003038342 0.17478064276434813 tensor(227.1028)\n",
      "-287.10364243493234 0.0 -287.10364243493234 0.32195787571074136 tensor(218.9781)\n",
      "-286.3360277810375 0.0 -286.3360277810375 0.24192631806098625 tensor(239.8895)\n",
      "-286.43053024083815 0.0 -286.43053024083815 0.2601425447803293 tensor(164.5041)\n",
      "-287.04702616368945 0.0 -287.04702616368945 0.1019783067028248 tensor(257.8064)\n",
      "-286.6678767203065 0.0 -286.6678767203065 0.23081487720553823 tensor(212.7097)\n",
      "-286.8493991942497 0.0 -286.8493991942497 0.18800788629736237 tensor(256.1802)\n",
      "-286.67042896794663 0.0 -286.67042896794663 0.3784958505739057 tensor(181.2171)\n",
      "-287.34412030840406 0.0 -287.34412030840406 0.33425404121055735 tensor(247.9975)\n",
      "-286.9731702036803 0.0 -286.9731702036803 0.09727553248065356 tensor(219.4536)\n",
      "-287.11926945479144 0.0 -287.11926945479144 0.03789595523491416 tensor(163.9323)\n",
      "-286.50388230012965 0.0 -286.50388230012965 0.08365849376149521 tensor(182.5828)\n",
      "-286.79659621329864 0.0 -286.79659621329864 0.15166473072118564 tensor(239.9860)\n",
      "-286.60232380627724 0.0 -286.60232380627724 0.28332394686403733 tensor(182.3956)\n",
      "-286.7361686327091 0.0 -286.7361686327091 0.27036968341626844 tensor(310.6502)\n",
      "-287.1405314622138 0.0 -287.1405314622138 0.3226840395631224 tensor(169.4439)\n",
      "-286.85451934326267 0.0 -286.85451934326267 0.16197404922562772 tensor(204.6224)\n",
      "-286.78085398175534 0.0 -286.78085398175534 0.43312872424535387 tensor(231.7300)\n",
      "-286.82309143866723 0.0 -286.82309143866723 0.14440783470693558 tensor(211.2426)\n",
      "-286.86184820650857 0.0 -286.86184820650857 0.2092553769990502 tensor(211.6835)\n",
      "-286.8187046177027 0.0 -286.8187046177027 0.10530626080191872 tensor(223.6224)\n",
      "-286.69973440398473 0.0 -286.69973440398473 0.14414746639005788 tensor(205.2086)\n",
      "-286.6815582377518 0.0 -286.6815582377518 0.27648959933973943 tensor(226.7081)\n",
      "-286.3392240682424 0.0 -286.3392240682424 0.29672732967617405 tensor(205.6684)\n",
      "-286.30204539633974 0.0 -286.30204539633974 0.18327761174268417 tensor(205.9614)\n",
      "-286.9939974562972 0.0 -286.9939974562972 0.16335210742022147 tensor(174.3632)\n",
      "-286.92143237544406 0.0 -286.92143237544406 0.20836598709017926 tensor(193.8035)\n",
      "-286.83553569084586 0.0 -286.83553569084586 0.16270178744583494 tensor(163.5426)\n",
      "-286.76533464164413 0.0 -286.76533464164413 0.349297294682171 tensor(256.5579)\n",
      "-286.8371557935526 0.0 -286.8371557935526 0.2009465673800938 tensor(246.4215)\n",
      "-286.8160750557877 0.0 -286.8160750557877 0.07326271958274312 tensor(167.5685)\n",
      "-286.8823254805593 0.0 -286.8823254805593 0.2802455997410326 tensor(186.7026)\n",
      "-286.5249919619716 0.0 -286.5249919619716 0.27977493958574384 tensor(351.4857)\n",
      "-286.8525953141327 0.0 -286.8525953141327 0.2972025812477313 tensor(180.0275)\n",
      "-286.5526228027247 0.0 -286.5526228027247 0.2319731460627911 tensor(264.4828)\n",
      "-286.6803804746669 0.0 -286.6803804746669 0.33329824700774624 tensor(188.2193)\n",
      "-287.1241544874114 0.0 -287.1241544874114 0.25131762443283917 tensor(198.9577)\n",
      "-286.69643886536636 0.0 -286.69643886536636 0.17585011510727058 tensor(305.9039)\n",
      "-286.7999376790368 0.0 -286.7999376790368 0.25493270110423366 tensor(194.5100)\n",
      "-286.81990001103 0.0 -286.81990001103 0.26124647663823486 tensor(227.4921)\n",
      "-286.7252615411342 0.0 -286.7252615411342 0.3559737407866879 tensor(199.3722)\n",
      "-286.668472114255 0.0 -286.668472114255 0.38806893461051956 tensor(266.6796)\n",
      "-287.08705536544323 0.0 -287.08705536544323 0.23526844723327367 tensor(161.1350)\n",
      "-286.5734950413953 0.0 -286.5734950413953 0.23350456212360954 tensor(194.8294)\n",
      "-287.02281706202245 0.0 -287.02281706202245 0.24385945070026918 tensor(244.4919)\n",
      "-286.65567685780707 0.0 -286.65567685780707 0.33467606953253193 tensor(205.5000)\n",
      "-286.62552971256076 0.0 -286.62552971256076 0.375472529954504 tensor(188.2577)\n",
      "-286.62419239347906 0.0 -286.62419239347906 0.2531121444946471 tensor(164.9235)\n",
      "-286.8862616966591 0.0 -286.8862616966591 0.19749680309061446 tensor(182.0879)\n",
      "-286.828004232362 0.0 -286.828004232362 0.1571163210196853 tensor(166.2971)\n",
      "-286.4875714086131 0.0 -286.4875714086131 0.09542657349574127 tensor(180.3933)\n",
      "-286.7631790754149 0.0 -286.7631790754149 0.1707731429707731 tensor(388.9104)\n",
      "-286.5729198058461 0.0 -286.5729198058461 0.20524689241827568 tensor(165.9179)\n",
      "-286.7893705857656 0.0 -286.7893705857656 0.15571472197884226 tensor(176.3740)\n",
      "-286.5369366490487 0.0 -286.5369366490487 0.18381749024673452 tensor(183.0267)\n",
      "-287.2869500832735 0.0 -287.2869500832735 0.2438440344455565 tensor(165.7797)\n",
      "-286.9395892720976 0.0 -286.9395892720976 0.3042210572662086 tensor(234.0159)\n",
      "-286.8777433847594 0.0 -286.8777433847594 0.20318956022759507 tensor(219.1007)\n",
      "-287.172572382234 0.0 -287.172572382234 0.3209183409753716 tensor(210.3555)\n",
      "-286.67537557706726 0.0 -286.67537557706726 0.1882523374770518 tensor(222.4595)\n",
      "-287.00396665925587 0.0 -287.00396665925587 0.22784389637778882 tensor(214.0296)\n",
      "-287.0540370911741 0.0 -287.0540370911741 0.2025384388551266 tensor(221.9659)\n",
      "-286.9959872172617 0.0 -286.9959872172617 0.19037665133941947 tensor(217.1162)\n",
      "-286.8507982428964 0.0 -286.8507982428964 0.24140555709944148 tensor(233.3649)\n",
      "-286.91618674649243 0.0 -286.91618674649243 0.16276034639725612 tensor(168.2343)\n",
      "-286.7223852905753 0.0 -286.7223852905753 0.11841868585465037 tensor(290.6506)\n",
      "-286.78958582678155 0.0 -286.78958582678155 0.3113394205986113 tensor(220.5411)\n",
      "-287.23952896367194 0.0 -287.23952896367194 0.18648999756863552 tensor(218.7879)\n",
      "-286.46053875500513 0.0 -286.46053875500513 0.2639306032651387 tensor(170.6819)\n",
      "-287.0358928732355 0.0 -287.0358928732355 0.2750317388306136 tensor(186.7544)\n",
      "-286.5139665474485 0.0 -286.5139665474485 0.11966907777515523 tensor(168.3616)\n",
      "-286.8756348685355 0.0 -286.8756348685355 0.36239266169828327 tensor(252.4029)\n",
      "-286.6896558955814 0.0 -286.6896558955814 0.16310555906344626 tensor(217.6168)\n",
      "-286.8846427293271 0.0 -286.8846427293271 0.1316139222283643 tensor(193.5947)\n",
      "-287.0339653759665 0.0 -287.0339653759665 0.328200614497528 tensor(252.7412)\n",
      "-286.9745109900882 0.0 -286.9745109900882 0.2290590307205144 tensor(163.0329)\n",
      "-286.64153419721146 0.0 -286.64153419721146 0.17591093750915482 tensor(192.3362)\n",
      "-286.91103966554374 0.0 -286.91103966554374 0.2266582048569968 tensor(186.1385)\n",
      "-287.10092141648636 0.0 -287.10092141648636 0.1787607417506162 tensor(178.5401)\n",
      "-287.023800693941 0.0 -287.023800693941 0.17161015716964037 tensor(167.3873)\n",
      "-287.21921562433704 0.0 -287.21921562433704 0.38969352140606994 tensor(395.6988)\n",
      "-286.72200532936574 0.0 -286.72200532936574 0.17475446093881786 tensor(177.9382)\n",
      "-286.734477953108 0.0 -286.734477953108 0.12622466538935778 tensor(179.7895)\n",
      "-286.52781119954864 0.0 -286.52781119954864 0.28954763137038897 tensor(202.7514)\n",
      "== Era 8 | Epoch 0 metrics ==\n",
      "\tloss -286.805\n",
      "\tforce 0\n",
      "\tdkl -286.805\n",
      "\tlogp 86.2185\n",
      "\tlogq -200.587\n",
      "\tess 0.228744\n",
      "-287.0572097223503 0.0 -287.0572097223503 0.09219903835337732 tensor(150.2419)\n",
      "-286.8396247598117 0.0 -286.8396247598117 0.2715266509283483 tensor(242.3999)\n",
      "-286.6500325210735 0.0 -286.6500325210735 0.148682043991058 tensor(211.1493)\n",
      "-286.7455392815797 0.0 -286.7455392815797 0.26405341318492964 tensor(288.1773)\n",
      "-286.7753948464727 0.0 -286.7753948464727 0.18785839304383659 tensor(232.9664)\n",
      "-286.90660708824066 0.0 -286.90660708824066 0.19582865936291274 tensor(218.5251)\n",
      "-286.7456809034296 0.0 -286.7456809034296 0.2226689997473517 tensor(185.1576)\n",
      "-287.11649722145773 0.0 -287.11649722145773 0.37260065192671654 tensor(249.5088)\n",
      "-286.6882674515507 0.0 -286.6882674515507 0.16319375676672807 tensor(160.5853)\n",
      "-286.76369506391296 0.0 -286.76369506391296 0.17696114803565802 tensor(210.0390)\n",
      "-287.1061806671522 0.0 -287.1061806671522 0.40541727531881816 tensor(418.5511)\n",
      "-286.6437601475707 0.0 -286.6437601475707 0.31931223530514125 tensor(204.0581)\n",
      "-287.0766641976234 0.0 -287.0766641976234 0.373615830639935 tensor(228.4409)\n",
      "-286.99850184012706 0.0 -286.99850184012706 0.17435497314662085 tensor(199.1859)\n",
      "-286.8309884675019 0.0 -286.8309884675019 0.2892985352363049 tensor(217.3904)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-287.1013126907318 0.0 -287.1013126907318 0.21329969181483255 tensor(193.4902)\n",
      "-286.75604462110755 0.0 -286.75604462110755 0.2194159025806836 tensor(201.2356)\n",
      "-287.0479481087499 0.0 -287.0479481087499 0.23300483909460953 tensor(223.6758)\n",
      "-286.7347007892731 0.0 -286.7347007892731 0.37456966217586135 tensor(215.4751)\n",
      "-286.59911385224126 0.0 -286.59911385224126 0.08058265640562165 tensor(185.7359)\n",
      "-286.9823837360541 0.0 -286.9823837360541 0.35942636246267073 tensor(177.2091)\n",
      "-287.4609491473624 0.0 -287.4609491473624 0.3299042787363679 tensor(167.8750)\n",
      "-286.5736689158928 0.0 -286.5736689158928 0.34961766350318535 tensor(295.1423)\n",
      "-286.8294812932953 0.0 -286.8294812932953 0.30271053184380814 tensor(179.9932)\n",
      "-287.1030216724084 0.0 -287.1030216724084 0.3387798203593732 tensor(164.6811)\n",
      "-286.815283436428 0.0 -286.815283436428 0.31125346821440225 tensor(171.8524)\n",
      "-286.8567798727716 0.0 -286.8567798727716 0.053799360710257295 tensor(168.7261)\n",
      "-286.7602947419879 0.0 -286.7602947419879 0.19893814320469982 tensor(189.8935)\n",
      "-287.11587032809757 0.0 -287.11587032809757 0.23372962587989038 tensor(175.2198)\n",
      "-286.7686554808277 0.0 -286.7686554808277 0.1459213148215139 tensor(208.1281)\n",
      "-287.01397388170574 0.0 -287.01397388170574 0.4335579421880758 tensor(178.3702)\n",
      "-286.65587457077487 0.0 -286.65587457077487 0.3481683008780554 tensor(244.8638)\n",
      "-287.18280979725915 0.0 -287.18280979725915 0.284815531716226 tensor(430.6361)\n",
      "-286.628265494825 0.0 -286.628265494825 0.2039574286916263 tensor(172.5725)\n",
      "-287.32148674242785 0.0 -287.32148674242785 0.3157235012477981 tensor(209.7782)\n",
      "-287.01108407852155 0.0 -287.01108407852155 0.12345189792072504 tensor(236.2919)\n",
      "-286.9331339850877 0.0 -286.9331339850877 0.10018542482101961 tensor(204.6678)\n",
      "-286.8302862146942 0.0 -286.8302862146942 0.3958995949603521 tensor(301.7319)\n",
      "-286.79835182797933 0.0 -286.79835182797933 0.2484478268942929 tensor(313.1901)\n",
      "-287.35360481134387 0.0 -287.35360481134387 0.2548474307822244 tensor(231.0449)\n",
      "-286.9057792144371 0.0 -286.9057792144371 0.23113913158488372 tensor(188.8429)\n",
      "-287.1720686442452 0.0 -287.1720686442452 0.12389622751131617 tensor(223.8701)\n",
      "-286.976529703684 0.0 -286.976529703684 0.2794480424419423 tensor(185.1243)\n",
      "-286.83850218843804 0.0 -286.83850218843804 0.35665030109056145 tensor(159.2189)\n",
      "-286.99874220942877 0.0 -286.99874220942877 0.1370353464899884 tensor(200.1767)\n",
      "-286.73374796147147 0.0 -286.73374796147147 0.33670804452559866 tensor(211.3503)\n",
      "-287.12605320956715 0.0 -287.12605320956715 0.3331321003317199 tensor(221.0128)\n",
      "-286.84457270627763 0.0 -286.84457270627763 0.2175156745192243 tensor(152.7801)\n",
      "-287.0420493351394 0.0 -287.0420493351394 0.19986228063923378 tensor(161.1046)\n",
      "-286.8024524871856 0.0 -286.8024524871856 0.2386064663598265 tensor(237.8255)\n",
      "-287.25728406036075 0.0 -287.25728406036075 0.3573951509876631 tensor(183.1455)\n",
      "-286.9298836527163 0.0 -286.9298836527163 0.39263220944215166 tensor(156.7927)\n",
      "-286.9723397804778 0.0 -286.9723397804778 0.22179540004383516 tensor(209.6372)\n",
      "-287.0273655629951 0.0 -287.0273655629951 0.22079273816363054 tensor(213.5401)\n",
      "-287.10869975363954 0.0 -287.10869975363954 0.4043994710422643 tensor(192.7422)\n",
      "-287.0812901103969 0.0 -287.0812901103969 0.3091673360624857 tensor(184.0265)\n",
      "-286.6897112519755 0.0 -286.6897112519755 0.2932173199049387 tensor(192.7628)\n",
      "-286.83354728929083 0.0 -286.83354728929083 0.37062246526215253 tensor(145.6975)\n",
      "-286.6152203846659 0.0 -286.6152203846659 0.17142484843982506 tensor(263.4224)\n",
      "-286.86005295342466 0.0 -286.86005295342466 0.06311688955025735 tensor(348.9819)\n",
      "-286.7983469531714 0.0 -286.7983469531714 0.15872551459349898 tensor(206.6689)\n",
      "-287.11657738350993 0.0 -287.11657738350993 0.3129350312003799 tensor(212.3888)\n",
      "-286.8822393890953 0.0 -286.8822393890953 0.32251456277160245 tensor(168.3711)\n",
      "-286.92779297049753 0.0 -286.92779297049753 0.1722049071449616 tensor(854.1323)\n",
      "-286.9160829625381 0.0 -286.9160829625381 0.1204963766140808 tensor(245.3994)\n",
      "-286.6454043898075 0.0 -286.6454043898075 0.25070082365422347 tensor(273.3447)\n",
      "-286.73731003448813 0.0 -286.73731003448813 0.23844873867166655 tensor(187.5018)\n",
      "-287.1170404122082 0.0 -287.1170404122082 0.348318341324451 tensor(208.2174)\n",
      "-287.23081506216556 0.0 -287.23081506216556 0.40540848018026554 tensor(183.9227)\n",
      "-287.14708330678695 0.0 -287.14708330678695 0.1558532590015027 tensor(217.4556)\n",
      "-287.3688336923589 0.0 -287.3688336923589 0.10809067127360884 tensor(230.6950)\n",
      "-287.1286409416422 0.0 -287.1286409416422 0.3569406775678038 tensor(206.1297)\n",
      "-287.08923038561153 0.0 -287.08923038561153 0.446601911270488 tensor(196.2884)\n",
      "-286.70217628971074 0.0 -286.70217628971074 0.07453050750228045 tensor(233.4053)\n",
      "-286.9972692904772 0.0 -286.9972692904772 0.16166515445260662 tensor(193.0537)\n",
      "-287.40161069576305 0.0 -287.40161069576305 0.2583818978290163 tensor(196.3698)\n",
      "-286.92614597096167 0.0 -286.92614597096167 0.08212546223886992 tensor(225.3383)\n",
      "-286.95210136771004 0.0 -286.95210136771004 0.3252940447279499 tensor(188.1326)\n",
      "-286.85039100290135 0.0 -286.85039100290135 0.0865930032980426 tensor(316.6640)\n",
      "-286.77936470357366 0.0 -286.77936470357366 0.1664174639171548 tensor(216.1399)\n",
      "-286.57949818543705 0.0 -286.57949818543705 0.3216810124578684 tensor(179.0159)\n",
      "-286.798570321087 0.0 -286.798570321087 0.33735501710093074 tensor(196.3049)\n",
      "-286.8213216256216 0.0 -286.8213216256216 0.3990027003536776 tensor(178.3868)\n",
      "-286.8634433807997 0.0 -286.8634433807997 0.11589578870919809 tensor(210.8563)\n",
      "-286.8241608648955 0.0 -286.8241608648955 0.23343200133783712 tensor(171.8497)\n",
      "-287.26885622534354 0.0 -287.26885622534354 0.24441679207079473 tensor(245.7507)\n",
      "-286.7328446570507 0.0 -286.7328446570507 0.25976863185528826 tensor(261.4404)\n",
      "-286.8294041032581 0.0 -286.8294041032581 0.37135303333348496 tensor(201.1029)\n",
      "-287.02979669855097 0.0 -287.02979669855097 0.3722470720718126 tensor(161.1753)\n",
      "-286.9591794374054 0.0 -286.9591794374054 0.20891552783447476 tensor(228.5379)\n",
      "-287.1002818881386 0.0 -287.1002818881386 0.3634962738841701 tensor(171.2278)\n",
      "-286.9319985940395 0.0 -286.9319985940395 0.11431289717229473 tensor(258.2939)\n",
      "-287.00833216701625 0.0 -287.00833216701625 0.3477680657169491 tensor(162.8787)\n",
      "-286.8032309727109 0.0 -286.8032309727109 0.2869899406196061 tensor(213.5754)\n",
      "-286.99761081362834 0.0 -286.99761081362834 0.08159759097328648 tensor(227.0463)\n",
      "-286.5804867373523 0.0 -286.5804867373523 0.238330373385917 tensor(374.0519)\n",
      "-286.8182463194971 0.0 -286.8182463194971 0.17541761153773733 tensor(195.7953)\n",
      "-286.6866169891823 0.0 -286.6866169891823 0.3685682587301666 tensor(177.7081)\n",
      "-286.9694243990781 0.0 -286.9694243990781 0.032386307228225095 tensor(232.5192)\n",
      "-286.9840757805905 0.0 -286.9840757805905 0.07785792030365721 tensor(372.8531)\n",
      "== Era 9 | Epoch 0 metrics ==\n",
      "\tloss -286.923\n",
      "\tforce 0\n",
      "\tdkl -286.923\n",
      "\tlogp 86.4303\n",
      "\tlogq -200.493\n",
      "\tess 0.248732\n",
      "-286.9088254611303 0.0 -286.9088254611303 0.22091537295538782 tensor(403.2410)\n",
      "-286.6678026587167 0.0 -286.6678026587167 0.2637949928378175 tensor(233.7457)\n",
      "-286.89510897739757 0.0 -286.89510897739757 0.07879267406213827 tensor(274.5981)\n",
      "-286.96320867115867 0.0 -286.96320867115867 0.4959548678226077 tensor(194.4616)\n",
      "-287.20048246768135 0.0 -287.20048246768135 0.1295711367529888 tensor(184.8986)\n",
      "-286.5054114431544 0.0 -286.5054114431544 0.264409009914005 tensor(227.9596)\n",
      "-287.11659594982837 0.0 -287.11659594982837 0.2644162607056213 tensor(218.2153)\n",
      "-286.88388738658585 0.0 -286.88388738658585 0.38042039403361394 tensor(163.3939)\n",
      "-286.84323967262935 0.0 -286.84323967262935 0.1481158913005492 tensor(219.1616)\n",
      "-286.89789675228974 0.0 -286.89789675228974 0.2084425836849604 tensor(185.6280)\n",
      "-287.0559113528724 0.0 -287.0559113528724 0.21564474712668671 tensor(609.4056)\n",
      "-287.1500732405041 0.0 -287.1500732405041 0.4177687558790624 tensor(573.3710)\n",
      "-286.97649157415213 0.0 -286.97649157415213 0.22526902606315466 tensor(212.5000)\n",
      "-286.9387449412193 0.0 -286.9387449412193 0.24879046796011117 tensor(219.4715)\n",
      "-286.6935316905517 0.0 -286.6935316905517 0.23922391577109137 tensor(193.5324)\n",
      "-287.18537956918345 0.0 -287.18537956918345 0.30448810260053993 tensor(283.6353)\n",
      "-286.8731598295965 0.0 -286.8731598295965 0.24367160488620793 tensor(279.3231)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-287.1974354358955 0.0 -287.1974354358955 0.24741621583893092 tensor(414.6770)\n",
      "-286.9126605344217 0.0 -286.9126605344217 0.19326177991835528 tensor(197.4445)\n",
      "-287.1525202454744 0.0 -287.1525202454744 0.15849238446212519 tensor(180.5447)\n",
      "-286.99595224483573 0.0 -286.99595224483573 0.13534719086020094 tensor(221.3385)\n",
      "-287.0839793721429 0.0 -287.0839793721429 0.2945747959313512 tensor(205.0207)\n",
      "-286.92071639395357 0.0 -286.92071639395357 0.24238989951667803 tensor(198.5116)\n",
      "-286.9655107787159 0.0 -286.9655107787159 0.3045876064012117 tensor(302.7847)\n",
      "-287.01217130365694 0.0 -287.01217130365694 0.21326813705089678 tensor(141.8040)\n",
      "-286.72580910681984 0.0 -286.72580910681984 0.25328739579993514 tensor(171.2045)\n",
      "-286.8274692717415 0.0 -286.8274692717415 0.3325019587180933 tensor(198.5363)\n",
      "-286.9714275810692 0.0 -286.9714275810692 0.22347510584246394 tensor(280.2572)\n",
      "-286.91179021224457 0.0 -286.91179021224457 0.22762529192807113 tensor(431.2482)\n",
      "-286.9297917601647 0.0 -286.9297917601647 0.11752767758159971 tensor(208.6117)\n",
      "-287.1825254831671 0.0 -287.1825254831671 0.3102090269954769 tensor(219.4586)\n",
      "-286.79671585500324 0.0 -286.79671585500324 0.3063550117796918 tensor(163.9399)\n",
      "-286.9226515678334 0.0 -286.9226515678334 0.32329624119951134 tensor(176.7857)\n",
      "-286.7464644841252 0.0 -286.7464644841252 0.2072839512276673 tensor(190.0484)\n",
      "-287.024375865575 0.0 -287.024375865575 0.29839891176855715 tensor(192.5707)\n",
      "-286.9181323637867 0.0 -286.9181323637867 0.14168384998239938 tensor(195.0276)\n",
      "-286.58518287257374 0.0 -286.58518287257374 0.31482691967576176 tensor(230.6602)\n",
      "-287.15015203384837 0.0 -287.15015203384837 0.17093635393185627 tensor(249.2677)\n",
      "-286.7375693599705 0.0 -286.7375693599705 0.23287715834627146 tensor(234.2610)\n",
      "-287.17512714136683 0.0 -287.17512714136683 0.21974723732054702 tensor(172.7054)\n",
      "-287.1795634042732 0.0 -287.1795634042732 0.43988695359923835 tensor(162.3461)\n",
      "-286.780941272013 0.0 -286.780941272013 0.2954367758375671 tensor(307.8568)\n",
      "-287.14376232277226 0.0 -287.14376232277226 0.32093072326788474 tensor(169.2187)\n",
      "-287.109930493893 0.0 -287.109930493893 0.28696782599020776 tensor(175.1206)\n",
      "-287.07920246296374 0.0 -287.07920246296374 0.06462544071604283 tensor(167.4148)\n",
      "-286.7464361898765 0.0 -286.7464361898765 0.28234757219739276 tensor(181.8779)\n",
      "-286.96641703813737 0.0 -286.96641703813737 0.2301934171976399 tensor(198.5641)\n",
      "-286.7106423137802 0.0 -286.7106423137802 0.24357316742221743 tensor(248.5241)\n",
      "-287.0257763380099 0.0 -287.0257763380099 0.2194671820684843 tensor(273.5811)\n",
      "-286.9578080700712 0.0 -286.9578080700712 0.17740050729720333 tensor(178.4288)\n",
      "-286.7725326130907 0.0 -286.7725326130907 0.2447343209075607 tensor(229.2414)\n",
      "-287.0783519792292 0.0 -287.0783519792292 0.16768137812053216 tensor(234.6600)\n",
      "-286.8539578740083 0.0 -286.8539578740083 0.27954648962353484 tensor(299.8999)\n",
      "-286.73310477833775 0.0 -286.73310477833775 0.30657796879025057 tensor(212.2045)\n",
      "-286.88368759143015 0.0 -286.88368759143015 0.23924178239729363 tensor(242.7648)\n",
      "-287.0843382304865 0.0 -287.0843382304865 0.15026530252378853 tensor(241.0356)\n",
      "-286.9910644636909 0.0 -286.9910644636909 0.1797825590738058 tensor(185.6133)\n",
      "-286.8803827729191 0.0 -286.8803827729191 0.28156776415676676 tensor(164.9419)\n",
      "-286.8229351443449 0.0 -286.8229351443449 0.2878102313542664 tensor(298.6600)\n",
      "-287.01464397333234 0.0 -287.01464397333234 0.3062647058815183 tensor(212.2709)\n",
      "-286.87251736990163 0.0 -286.87251736990163 0.08289947795113742 tensor(216.1884)\n",
      "-287.0353830611585 0.0 -287.0353830611585 0.22793171468258294 tensor(166.4705)\n",
      "-286.810929019201 0.0 -286.810929019201 0.2228506821984637 tensor(248.8539)\n",
      "-287.05076590170694 0.0 -287.05076590170694 0.1809839679950288 tensor(189.7300)\n",
      "-287.29384643387164 0.0 -287.29384643387164 0.2375866423387684 tensor(196.8143)\n",
      "-286.5030546625494 0.0 -286.5030546625494 0.40853458607835424 tensor(201.9808)\n",
      "-287.0188240470325 0.0 -287.0188240470325 0.1640092149614358 tensor(189.3215)\n",
      "-287.1084152631114 0.0 -287.1084152631114 0.06111284248921995 tensor(220.5330)\n",
      "-287.1081958376235 0.0 -287.1081958376235 0.20556306546215064 tensor(709.1591)\n",
      "-286.99335100413623 0.0 -286.99335100413623 0.38760681897030164 tensor(270.9484)\n",
      "-287.1265169369964 0.0 -287.1265169369964 0.3205928994943988 tensor(217.1891)\n",
      "-287.02876009813144 0.0 -287.02876009813144 0.2549887430358216 tensor(240.2741)\n",
      "-287.3432511621457 0.0 -287.3432511621457 0.30989051296217474 tensor(183.9918)\n",
      "-287.2500611388548 0.0 -287.2500611388548 0.21843110681037403 tensor(178.5705)\n",
      "-286.8731990714913 0.0 -286.8731990714913 0.40175192218203437 tensor(201.8497)\n",
      "-287.2456492990226 0.0 -287.2456492990226 0.28628434132909786 tensor(357.3750)\n",
      "-287.22927055509706 0.0 -287.22927055509706 0.381503234359129 tensor(209.8026)\n",
      "-287.0302326108395 0.0 -287.0302326108395 0.3455290310634339 tensor(185.2588)\n",
      "-287.53285485253946 0.0 -287.53285485253946 0.3068722534672124 tensor(154.5724)\n",
      "-287.1626457145828 0.0 -287.1626457145828 0.38425111649535604 tensor(195.6214)\n",
      "-287.14725564266087 0.0 -287.14725564266087 0.33643377955018433 tensor(197.8291)\n",
      "-286.8365223776451 0.0 -286.8365223776451 0.29399276956898596 tensor(213.2208)\n",
      "-286.9513138548691 0.0 -286.9513138548691 0.23611329599270925 tensor(199.9678)\n",
      "-287.3930548927986 0.0 -287.3930548927986 0.07936101275984374 tensor(238.8882)\n",
      "-286.9311216443224 0.0 -286.9311216443224 0.21478708786459738 tensor(166.4290)\n",
      "-287.25153206630324 0.0 -287.25153206630324 0.25775104719796893 tensor(170.0066)\n",
      "-286.80515042631754 0.0 -286.80515042631754 0.3263855810062905 tensor(220.0596)\n",
      "-287.01897232273456 0.0 -287.01897232273456 0.2937647702514415 tensor(246.7591)\n",
      "-286.8838448251279 0.0 -286.8838448251279 0.23630433934850228 tensor(197.9111)\n",
      "-286.97655688604016 0.0 -286.97655688604016 0.2652759305304557 tensor(223.7853)\n",
      "-287.0246663354871 0.0 -287.0246663354871 0.27500160458150574 tensor(208.2706)\n",
      "-287.0862524302613 0.0 -287.0862524302613 0.17028767682748355 tensor(219.1334)\n",
      "-287.18804262172426 0.0 -287.18804262172426 0.2782841256503332 tensor(182.2585)\n",
      "-287.22517117932773 0.0 -287.22517117932773 0.3191974962675373 tensor(181.0779)\n",
      "-286.5780759659523 0.0 -286.5780759659523 0.30242889582127175 tensor(359.3135)\n",
      "-287.19691163124367 0.0 -287.19691163124367 0.14818239624378515 tensor(161.1250)\n",
      "-286.8319959205642 0.0 -286.8319959205642 0.24182559034814888 tensor(307.3519)\n",
      "-287.31098839243333 0.0 -287.31098839243333 0.35197685821614666 tensor(174.6270)\n",
      "-286.67855561447425 0.0 -286.67855561447425 0.2320940132651029 tensor(261.9719)\n",
      "Accept rate: 0.4169921875\n",
      "Topological susceptibility = 1.27 +/- 0.10\n",
      "... vs HMC estimate = 1.23 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "pre_flow_model, flow_act = flow_train(param)\n",
    "flow_eval(pre_flow_model,flow_act)\n",
    "pre_flow = pre_flow_model['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept rate: 0.3291015625\n",
      "Topological susceptibility = 1.15 +/- 0.10\n",
      "... vs HMC estimate = 1.23 +/- 0.02\n"
     ]
    }
   ],
   "source": [
    "train_force = False\n",
    "flow_model = None\n",
    "if train_force:\n",
    "    flow_model, flow_act = flow_train(param, with_force=True, pre_model=pre_flow_model)\n",
    "else:\n",
    "    flow_model = pre_flow_model\n",
    "flow_eval(flow_model,flow_act)\n",
    "flow = flow_model['layers']\n",
    "# flow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.7122)\n",
      "tensor(11.7961)\n"
     ]
    }
   ],
   "source": [
    "def test_force(x = None):\n",
    "    model = flow_model\n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    if x == None:\n",
    "        pre_model = pre_flow_model\n",
    "        pre_layers, pre_prior = pre_model['layers'], pre_model['prior']\n",
    "        pre_xi = pre_prior.sample_n(1)\n",
    "        x = ft_flow(pre_layers, pre_xi)\n",
    "    xi = ft_flow_inv(layers, x)\n",
    "    f = ft_force(param, layers, xi)\n",
    "    f_s = torch.linalg.norm(f)\n",
    "    print(f_s)\n",
    "\n",
    "test_force()\n",
    "test_force(torch.reshape(field,(1,)+field.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.6436154345146917  topo: -1.0\n",
      "plaq(x) 0.6436154345146917  force.norm 28.595181015284275\n",
      "Traj:    1  ACCEPT:  dH:  0.00048705497  exp(-dH):  0.99951306   plaq:  0.66561836   topo: -2.0\n",
      "plaq(x) 0.6656183553517631  force.norm 28.11460650971445\n",
      "Traj:    2  ACCEPT:  dH: -0.0012323184  exp(-dH):  1.0012331    plaq:  0.62357274   topo: -1.0\n",
      "plaq(x) 0.6235727353424116  force.norm 30.429344370433586\n",
      "Traj:    3  ACCEPT:  dH:  0.00029376435  exp(-dH):  0.99970628   plaq:  0.59412058   topo:  0.0\n",
      "plaq(x) 0.5941205758120947  force.norm 28.538774002763446\n",
      "Traj:    4  ACCEPT:  dH: -0.00022747014  exp(-dH):  1.0002275    plaq:  0.62219674   topo:  1.0\n",
      "plaq(x) 0.6221967424532626  force.norm 29.916468389422278\n",
      "Traj:    5  ACCEPT:  dH:  0.0015577067  exp(-dH):  0.99844351   plaq:  0.69874304   topo:  0.0\n",
      "plaq(x) 0.6987430410613997  force.norm 27.98159411963013\n",
      "Traj:    6  ACCEPT:  dH: -0.00032087838  exp(-dH):  1.0003209    plaq:  0.67307872   topo:  1.0\n",
      "plaq(x) 0.6730787163797621  force.norm 28.518424796315625\n",
      "Traj:    7  ACCEPT:  dH:  0.00077215078  exp(-dH):  0.99922815   plaq:  0.70049534   topo: -1.0\n",
      "plaq(x) 0.700495339510128  force.norm 26.89234293290679\n",
      "Traj:    8  ACCEPT:  dH: -0.0012721773  exp(-dH):  1.001273     plaq:  0.66540883   topo:  0.0\n",
      "plaq(x) 0.6654088264275213  force.norm 29.405163029196643\n",
      "Traj:    9  ACCEPT:  dH:  0.00016879286  exp(-dH):  0.99983122   plaq:  0.69503567   topo:  1.0\n",
      "plaq(x) 0.6950356687547556  force.norm 29.921528990639054\n",
      "Traj:   10  ACCEPT:  dH:  0.00047373024  exp(-dH):  0.99952638   plaq:  0.70418054   topo:  0.0\n",
      "plaq(x) 0.7041805422347297  force.norm 28.493194243088904\n",
      "Traj:   11  ACCEPT:  dH: -0.001727274  exp(-dH):  1.0017288    plaq:  0.6201183    topo:  0.0\n",
      "plaq(x) 0.6201183043001965  force.norm 29.863361836419333\n",
      "Traj:   12  ACCEPT:  dH:  0.0017614606  exp(-dH):  0.99824009   plaq:  0.68144781   topo:  0.0\n",
      "plaq(x) 0.6814478082971551  force.norm 26.620524871368225\n",
      "Traj:   13  ACCEPT:  dH: -0.0025228051  exp(-dH):  1.002526     plaq:  0.59561599   topo:  2.0\n",
      "plaq(x) 0.5956159937569036  force.norm 30.83293196184776\n",
      "Traj:   14  ACCEPT:  dH:  0.0023831829  exp(-dH):  0.99761965   plaq:  0.69107713   topo:  3.0\n",
      "plaq(x) 0.691077129381011  force.norm 27.657695997486584\n",
      "Traj:   15  ACCEPT:  dH:  0.00067120825  exp(-dH):  0.99932902   plaq:  0.70214813   topo:  1.0\n",
      "plaq(x) 0.7021481323054406  force.norm 26.9647054557474\n",
      "Traj:   16  ACCEPT:  dH:  9.9697448e-05  exp(-dH):  0.99990031   plaq:  0.72899772   topo:  1.0\n",
      "Run times:  [0.18726088001858443, 0.15491189900785685, 0.15535295603331178, 0.14232373295817524]\n",
      "Per trajectory:  [0.04681522000464611, 0.03872797475196421, 0.038838239008327946, 0.03558093323954381]\n"
     ]
    }
   ],
   "source": [
    "field = run(param, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plaq(field_run[0]) 0.7302893102810509\n",
      "tensor([40.8650], grad_fn=<AddBackward0>) tensor([-40.8650], grad_fn=<AddBackward0>)\n",
      "original_action tensor(-2.4770, grad_fn=<AddBackward0>)\n",
      "eff_action tensor([3.3880], grad_fn=<AddBackward0>)\n",
      "plaq(x) 0.03995800269130618  logJ tensor([40.8650], grad_fn=<AddBackward0>)  force.norm 7.234809490104591\n",
      "plaq(y) 0.7302892218853114\n",
      "plaq(x) 0.7302893102810509  force.norm 19.320145488334514\n"
     ]
    }
   ],
   "source": [
    "flows = flow\n",
    "\n",
    "print(f'plaq(field_run[0]) {action(param, field_run[0]) / (-param.beta*param.volume)}')\n",
    "# field.requires_grad_(True)\n",
    "x = field_run\n",
    "logJ = 0.0\n",
    "for layer in reversed(flows):\n",
    "    x, lJ = layer.reverse(x)\n",
    "    logJ += lJ\n",
    "\n",
    "# x is the prior distribution now\n",
    "    \n",
    "x.requires_grad_(True)\n",
    "    \n",
    "y = x\n",
    "logJy = 0.0\n",
    "for layer in flows:\n",
    "    y, lJ = layer.forward(y)\n",
    "    logJy += lJ\n",
    "    \n",
    "s = action(param, y[0]) - logJy\n",
    "\n",
    "print(logJ,logJy)\n",
    "\n",
    "\n",
    "# print(\"eff_action\", s + 136.3786)\n",
    "\n",
    "print(\"original_action\", action(param, y[0]) + 91)\n",
    "\n",
    "print(\"eff_action\", s + 56)\n",
    "\n",
    "s.backward()\n",
    "\n",
    "f = x.grad\n",
    "\n",
    "x.requires_grad_(False)\n",
    "\n",
    "print(f'plaq(x) {action(param, x[0]) / (-param.beta*param.volume)}  logJ {logJ}  force.norm {torch.linalg.norm(f)}')\n",
    "\n",
    "print(f'plaq(y) {action(param, y[0]) / (-param.beta*param.volume)}')\n",
    "\n",
    "print(f'plaq(x) {action(param, field_run[0]) / (-param.beta*param.volume)}  force.norm {torch.linalg.norm(force(param, field_run[0]))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 8, 8])\n",
      "tensor(7.2348)\n",
      "tensor(7.2348)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x = ft_flow_inv(flow, field_run)\n",
    "# x = field_run\n",
    "#for layer in reversed(flows):\n",
    "#    x, lJ = layer.reverse(x)\n",
    "ff = ft_force(param, flow, x)\n",
    "print(torch.linalg.norm(ff))\n",
    "fff = ft_force(param, flow, x)\n",
    "print(torch.linalg.norm(fff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-52.6120], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ft_flow_inv(flow, field_run)\n",
    "ft_action(param, flow, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattern(l):\n",
    "    return [x for y in l for x in y]\n",
    "\n",
    "def average(l):\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def sub_avg(l):\n",
    "    avg = average(l)\n",
    "    return np.array([x - avg for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_hmc_info_list = []\n",
    "def ft_leapfrog(param, flow, x, p):\n",
    "    mom_norm = torch.sum(p*p)\n",
    "    info_list = []\n",
    "    dt = param.dt\n",
    "    x_ = x + 0.5*dt*p\n",
    "    f = ft_force(param, flow, x_)\n",
    "    p_ = p + (-dt)*f\n",
    "    info = np.array((float(torch.linalg.norm(f)),\n",
    "                     float(ft_action(param, flow, x_).detach()),\n",
    "                     float(torch.sum(p*p_)/np.sqrt(mom_norm*torch.sum(p_*p_)))))\n",
    "    info_list.append(info)\n",
    "    for i in range(param.nstep-1):\n",
    "        x_ = x_ + dt*p_\n",
    "        f = ft_force(param, flow, x_)\n",
    "        info = np.array((float(torch.linalg.norm(f)),\n",
    "                        float(ft_action(param, flow, x_).detach()),\n",
    "                        float(torch.sum(p*p_)/np.sqrt(mom_norm*torch.sum(p_*p_)))))\n",
    "        info_list.append(info)\n",
    "        p_ = p_ + (-dt)*f\n",
    "    x_ = x_ + 0.5*dt*p_\n",
    "    print(np.sqrt(average([l[0]**2 for l in info_list])),\n",
    "          (info_list[0][1], info_list[-1][1]),\n",
    "          info_list[-1][2])\n",
    "    ft_hmc_info_list.append(info_list)\n",
    "    return (x_, p_)\n",
    "\n",
    "def ft_hmc(param, flow, field):\n",
    "    x = ft_flow_inv(flow, field)\n",
    "    p = torch.randn_like(x)\n",
    "    act0 = ft_action(param, flow, x).detach() + 0.5*torch.sum(p*p)\n",
    "    x_, p_ = ft_leapfrog(param, flow, x, p)\n",
    "    xr = regularize(x_)\n",
    "    act = ft_action(param, flow, xr).detach() + 0.5*torch.sum(p_*p_)\n",
    "    prob = torch.rand([], dtype=torch.float64)\n",
    "    dH = act-act0\n",
    "    exp_mdH = torch.exp(-dH)\n",
    "    acc = prob < exp_mdH\n",
    "    # ADJUST ME\n",
    "    newx = xr if acc else x\n",
    "    # newx = xr\n",
    "    newfield = ft_flow(flow, newx)\n",
    "    return (float(dH), float(exp_mdH), acc, newfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_run(param, flow, field = None):\n",
    "    if field == None:\n",
    "        field = param.initializer()\n",
    "    ft_hmc_info_list = []\n",
    "    with open(param.uniquestr(), \"w\") as O:\n",
    "        params = param.summary()\n",
    "        O.write(params)\n",
    "        put(params)\n",
    "        plaq, topo = (action(param, field) / (-param.beta*param.volume), topocharge(field))\n",
    "        status = f\"Initial configuration:  plaq: {plaq}  topo: {topo}\\n\"\n",
    "        O.write(status)\n",
    "        put(status)\n",
    "        ts = []\n",
    "        for n in range(param.nrun):\n",
    "            t = -timer()\n",
    "            for i in range(param.ntraj):\n",
    "                field_run = torch.reshape(field,(1,)+field.shape)\n",
    "                dH, exp_mdH, acc, field_run = ft_hmc(param, flow, field_run)\n",
    "                field = field_run[0]\n",
    "                plaq = action(param, field) / (-param.beta*param.volume)\n",
    "                topo = topocharge(field)\n",
    "                ifacc = \"ACCEPT\" if acc else \"REJECT\"\n",
    "                status = f\"Traj: {n*param.ntraj+i+1:4}  {ifacc}:  dH: {dH:< 12.8}  exp(-dH): {exp_mdH:< 12.8}  plaq: {plaq:< 12.8}  topo: {topo:< 3.3}\\n\"\n",
    "                O.write(status)\n",
    "                if (i+1) % (param.ntraj//param.nprint) == 0:\n",
    "                    put(status)\n",
    "            t += timer()\n",
    "            ts.append(t)\n",
    "        print(\"Run times: \", ts)\n",
    "        print(\"Per trajectory: \", [t/param.ntraj for t in ts])\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.6627686419055754  topo: 0.0\n",
      "13.378684424948641 (-53.73971180677989, -53.659984669959464) 0.9732045603479264\n",
      "Traj:    1  ACCEPT:  dH: -0.0028627377  exp(-dH):  1.0028668    plaq:  0.72662017   topo:  0.0\n",
      "20.26347831375218 (-53.562232506413864, -52.51380989435269) 0.9547163537573339\n",
      "Traj:    2  REJECT:  dH:  0.34848501   exp(-dH):  0.7057565    plaq:  0.72661999   topo:  0.0\n",
      "16.331966844133525 (-53.60969968082007, -53.5585371652662) 0.9871465395562814\n",
      "Traj:    3  ACCEPT:  dH: -0.01379607   exp(-dH):  1.0138917    plaq:  0.6983424    topo:  0.0\n",
      "15.456432820850809 (-53.48774725216437, -51.383267278283995) 0.9653108529207802\n",
      "Traj:    4  ACCEPT:  dH:  0.0012047187  exp(-dH):  0.99879601   plaq:  0.65318636   topo:  0.0\n",
      "17.38078591052218 (-51.4535395027086, -54.30936491400867) 0.9909948866061801\n",
      "Traj:    5  REJECT:  dH:  0.37634973   exp(-dH):  0.68636225   plaq:  0.6531864    topo:  0.0\n",
      "35.43744672181124 (-51.327466510777974, -53.465637988215036) 0.9281868109773596\n",
      "Traj:    6  ACCEPT:  dH: -0.15434702   exp(-dH):  1.1668957    plaq:  0.71093385   topo:  0.0\n",
      "15.535000928208063 (-53.45320493750932, -51.42441087822177) 0.9552687369118391\n",
      "Traj:    7  ACCEPT:  dH: -0.0011914536  exp(-dH):  1.0011922    plaq:  0.6461284    topo: -1.0\n",
      "23.06389833045077 (-51.46880347093773, -53.67123148790015) 0.9434053398399815\n",
      "Traj:    8  ACCEPT:  dH:  0.0015404392  exp(-dH):  0.99846075   plaq:  0.67910896   topo:  1.0\n",
      "16.92993178443698 (-53.5179269058146, -54.06083237338593) 0.9489774273851215\n",
      "Traj:    9  ACCEPT:  dH: -0.00015412726  exp(-dH):  1.0001541    plaq:  0.68273308   topo:  1.0\n",
      "12.969595993539926 (-54.058912449528094, -54.306914875239755) 0.955107973650511\n",
      "Traj:   10  ACCEPT:  dH:  0.0019660485  exp(-dH):  0.99803588   plaq:  0.73416219   topo:  0.0\n",
      "30.85691425218947 (-54.26930810452904, -53.79230798895892) 0.9370152712818821\n",
      "Traj:   11  ACCEPT:  dH: -0.012914589  exp(-dH):  1.0129983    plaq:  0.75832971   topo: -1.0\n",
      "41.08221376840277 (-53.81776460416696, -54.1330761326166) 0.9468752261952766\n",
      "Traj:   12  REJECT:  dH:  0.91905518   exp(-dH):  0.39889575   plaq:  0.75832963   topo: -1.0\n",
      "16.28873829269258 (-53.759261303504694, -51.664386930728135) 0.9762294481356144\n",
      "Traj:   13  REJECT:  dH:  1.1226503    exp(-dH):  0.32541621   plaq:  0.7583295    topo: -1.0\n",
      "21.008676834328952 (-53.79906595189755, -54.62731103899593) 0.943361124106045\n",
      "Traj:   14  ACCEPT:  dH: -1.3135548e-05  exp(-dH):  1.0000131    plaq:  0.75526707   topo:  0.0\n",
      "21.737955605619803 (-54.66207999838345, -53.060096067981476) 0.9821507032045335\n",
      "Traj:   15  ACCEPT:  dH: -0.0042543995  exp(-dH):  1.0042635    plaq:  0.6992068    topo: -2.0\n",
      "17.44147096910208 (-53.05921917675779, -53.53543340909203) 0.9379909186224787\n",
      "Traj:   16  ACCEPT:  dH: -0.004506941  exp(-dH):  1.0045171    plaq:  0.71858599   topo: -1.0\n",
      "Run times:  [49.210321658989415, 48.43188964197179, 51.30715547100408, 48.83916216599755]\n",
      "Per trajectory:  [12.302580414747354, 12.107972410492948, 12.82678886775102, 12.209790541499387]\n"
     ]
    }
   ],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 0.5, # 0.3\n",
    "    nstep = 64, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    nprint = 4,\n",
    "    #\n",
    "    seed = 1331)\n",
    "\n",
    "# field = ft_run(param, pre_flow)\n",
    "field = ft_run(param, pre_flow, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (8, 8)\n",
      "volume = 64\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.7185859884287391  topo: -1.0\n",
      "35.58825429661411 (-53.64953457231129, -54.05251106745193) 0.957981301570924\n",
      "Traj:    1  ACCEPT:  dH: -0.2646876    exp(-dH):  1.3030238    plaq:  0.68012964   topo:  0.0\n",
      "26.996907862728886 (-54.19039581798784, -55.528032313186635) 0.9705934656072241\n",
      "Traj:    2  ACCEPT:  dH: -0.53375325   exp(-dH):  1.7053208    plaq:  0.72417941   topo:  1.0\n",
      "27.88171560805188 (-55.551799406136276, -54.5756659006458) 0.9700067180225045\n",
      "Traj:    3  ACCEPT:  dH: -0.056386845  exp(-dH):  1.0580069    plaq:  0.69634589   topo: -1.0\n",
      "30.950330945550775 (-54.69092790549618, -52.0511193358186) 0.8944220460873022\n",
      "Traj:    4  ACCEPT:  dH:  0.49485686   exp(-dH):  0.60965816   plaq:  0.60480921   topo:  1.0\n",
      "42.92329937977941 (-52.085588556007835, -51.74406041828097) 0.9394773190295843\n",
      "Traj:    5  ACCEPT:  dH:  1.4368181    exp(-dH):  0.23768285   plaq:  0.60804535   topo:  0.0\n",
      "38.00301832838383 (-51.873624249150694, -52.757717810494306) 0.9258193876900157\n",
      "Traj:    6  ACCEPT:  dH:  0.038387657  exp(-dH):  0.96233981   plaq:  0.58123425   topo:  1.0\n",
      "35.22892280252326 (-53.22091797957671, -55.23524470750701) 0.8767224631523443\n",
      "Traj:    7  ACCEPT:  dH:  0.34903601   exp(-dH):  0.70536773   plaq:  0.68684068   topo: -1.0\n",
      "23.941304169941006 (-54.728030273380675, -56.780222288185385) 0.9276780282679817\n",
      "Traj:    8  ACCEPT:  dH:  0.0077801568  exp(-dH):  0.99225003   plaq:  0.62917951   topo:  0.0\n",
      "24.943339009956937 (-56.844686842587116, -55.13331555219384) 0.9319964750639239\n",
      "Traj:    9  ACCEPT:  dH:  0.055358955  exp(-dH):  0.94614546   plaq:  0.68972046   topo:  1.0\n",
      "22.440244160062182 (-55.16759522951719, -52.85900902171925) 0.9338277040068887\n",
      "Traj:   10  ACCEPT:  dH:  0.21426302   exp(-dH):  0.80713606   plaq:  0.73328651   topo:  1.0\n",
      "19.096260487544203 (-52.8224772115673, -52.82784154313915) 0.9696469934213552\n",
      "Traj:   11  ACCEPT:  dH:  0.0012020647  exp(-dH):  0.99879866   plaq:  0.68277718   topo:  1.0\n",
      "19.313080461755142 (-52.730448221377394, -54.52637175245269) 0.9717668257942134\n",
      "Traj:   12  ACCEPT:  dH: -0.007852273  exp(-dH):  1.0078832    plaq:  0.74413809   topo: -1.0\n",
      "23.666000519373462 (-54.6552295601915, -56.432465789498856) 0.9452418333187083\n",
      "Traj:   13  ACCEPT:  dH:  0.0011154809  exp(-dH):  0.99888514   plaq:  0.76299576   topo:  0.0\n",
      "27.957394648453104 (-56.43501353949768, -54.54487284518599) 0.9203630598367989\n",
      "Traj:   14  ACCEPT:  dH:  0.012011765  exp(-dH):  0.98806009   plaq:  0.7899924    topo:  0.0\n",
      "18.30451756394263 (-54.5405986658175, -55.3224239910282) 0.9652732000527815\n",
      "Traj:   15  REJECT:  dH:  2.257752     exp(-dH):  0.10458533   plaq:  0.78999232   topo:  0.0\n",
      "37.77276954140078 (-54.510944840602754, -55.3435972040131) 0.9190150780740204\n",
      "Traj:   16  REJECT:  dH:  1.3453336    exp(-dH):  0.2604528    plaq:  0.78999235   topo:  0.0\n",
      "Run times:  [49.852381792035885, 51.01098068500869, 50.13728597201407, 49.76928914000746]\n",
      "Per trajectory:  [12.463095448008971, 12.752745171252172, 12.534321493003517, 12.442322285001865]\n"
     ]
    }
   ],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (8, 8),\n",
    "    tau = 0.5, # 0.3\n",
    "    nstep = 64, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    nprint = 4,\n",
    "    #\n",
    "    seed = 1331)\n",
    "\n",
    "# field = ft_run(param, pre_flow)\n",
    "field = ft_run(param, pre_flow, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.389480087367922"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_list = np.array([l[1] for l in flattern(ft_hmc_info_list)])\n",
    "action_list = sub_avg(action_list)\n",
    "np.sqrt(average(action_list**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.10260986658055"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_list = np.array([l[0] for l in flattern(ft_hmc_info_list)])\n",
    "np.sqrt(average(force_list**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18.27711285  34.3038331   38.47971804  22.92079991  14.89793795\n",
      "  18.08681988  20.31581029  22.14914023  23.87103188  23.34911165\n",
      "  19.31782271  15.31122775  14.29842581  16.53974364  20.60968096\n",
      "  22.45969878  19.81038763  14.50076622   9.74271184   7.16144983\n",
      "   6.43791031   6.51055981   6.79499594   7.1529161    7.57889242\n",
      "   8.06840814   8.57790798   9.01304499   9.2346246    9.09240327\n",
      "   8.49076822   7.45884179   6.1723308    4.90264743   3.92519736\n",
      "   3.41259392   3.33438466   3.51030509   3.78384734   4.08238111\n",
      "   4.38564792   4.69295284   5.00689449   5.32765208   5.65228937\n",
      "   5.97601477   6.29380474   6.60160793   6.89683138   7.17811448\n",
      "   7.44458231   7.69486476   7.92618572   8.13380598   8.31116147\n",
      "   8.45151134   8.55369476   8.63963128   8.80064034   9.28327141\n",
      "  10.49678124  12.55137939  14.37275549  13.96333735  11.64322749\n",
      "   9.55646993   7.99003083   7.13012197   6.80463101   6.76225949\n",
      "   6.8546393    6.99590927   7.07011514   6.90727963   6.40603871\n",
      "   5.93640038   6.47391934   7.69889543   7.89758357   6.70325643\n",
      "   5.20463712   4.44842423   5.30479877   7.56515663   9.77525269\n",
      "  10.57607148   9.53100557   7.31967971   5.20875115   4.29273971\n",
      "   4.3742423    4.46751304   4.21554814   3.78150345   3.52223581\n",
      "   3.83686459   4.9196566    6.54362955   8.07562341   8.50956515\n",
      "   6.75802594   5.46946832  15.16676447  52.63846448  53.75123761\n",
      "  10.76499507  21.17631202  22.17027148  21.65428042  22.20897088\n",
      "  22.5027739   17.10938879  22.36276513 112.45737971  24.24721718\n",
      "  12.82536261  10.74940208  13.4223691   14.94937677  14.8863887\n",
      "  13.87030096  12.69411774  11.82549238  11.3354702   11.11209938\n",
      "  11.03881207  11.04113864  11.07196111  12.53376468  12.3060228\n",
      "  12.25005477  12.31318565  12.40584877  12.42137898  12.26525881\n",
      "  11.8863148   11.29696737  10.56770783   9.78901244   9.01104511\n",
      "   8.19052656   7.18386837   5.82400271   4.17571925   3.44157014\n",
      "   5.51578172   9.01158621  12.2908102   14.13082018  14.00215363\n",
      "  12.45083196  11.55145378  13.04877967  12.30251725   9.26142454\n",
      "  10.20182935   4.71069178  39.806007     8.54289387  12.96557415\n",
      "  12.31170189  10.58170718   8.89237539   7.42296669   6.32695753\n",
      "   6.48021403   9.16126647  13.98593929  19.39015     23.60745482\n",
      "  25.3537191   24.25685428  20.15101198  11.99206729  17.8339489\n",
      "  34.15586388  22.2067692    9.16113481   9.48572018  14.73601359\n",
      "  18.87368278  20.23322987  18.88882713  16.65910862  15.73477901\n",
      "  16.76084547  19.06231326  22.03833311  25.28545504  28.25679333\n",
      "  30.35068527  31.26408771  31.52909574  30.53847527  27.55708838\n",
      "  23.30175189  18.78671725  14.81868485  11.94118339  10.66304271\n",
      "  11.4080512   13.90032432  17.08613051  19.62920397  20.4115009\n",
      "  19.0052518   15.77025864  11.54830528   7.46565212   5.51624122\n",
      "   7.24205006  10.30687736  13.11362608  15.16921534  16.29804853\n",
      "  16.49562447  15.91246121  14.82268452  13.60814088  12.82100567\n",
      "  12.94453659  12.94795112  10.34316779   7.91085669   8.03737885\n",
      "   7.43350627   7.49465382   8.0506828    8.28472413   8.14721848\n",
      "   7.8258923    7.48296564   7.24211048   7.22153149   7.53260275\n",
      "   8.21625275   9.16553574  10.12461332  10.78521602  10.93646292\n",
      "  10.6175518   10.26042648  10.80548654  13.31178937  17.65978412\n",
      "  21.83717234  23.08474023  20.91874729  18.14838151  17.4321697\n",
      "  18.52473654  20.03092819  21.30400111  21.97642684  21.39743813\n",
      "  19.27460213  21.3844432   25.36525837  25.32362167  22.66396696\n",
      "  17.53584835  13.08516668  12.62256888  13.72108643  13.83830736\n",
      "  13.68117154  15.25547398  18.38937366  20.58543356  20.17317996\n",
      "  17.7482817   15.62152952  15.98397499  26.57353757  47.74757206\n",
      "  53.22287684   6.55857521   5.47372054   5.84429518   5.84891277\n",
      "   5.79153266   5.6695087    5.49243757   5.28635604   5.07962427\n",
      "   4.8982269    4.7657495    4.70433896   4.73504908   4.87721425\n",
      "   5.14755334   5.56065877   6.13313349   6.89432078   7.90829989\n",
      "   9.31383946  11.38097737  14.50350049  18.66326167  21.52005461]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(force_list[0:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.7016228019160151  topo: -2.0\n",
      "plaq(x) 0.7016228019160151  force.norm 27.473372391179087\n",
      "Traj:    1  ACCEPT:  dH: -0.00080053206  exp(-dH):  1.0008009    plaq:  0.678064     topo:  0.0\n",
      "plaq(x) 0.6780639990092816  force.norm 28.79359239911523\n",
      "Traj:    2  ACCEPT:  dH:  0.0011502039  exp(-dH):  0.99885046   plaq:  0.74059871   topo:  0.0\n",
      "plaq(x) 0.7405987100486567  force.norm 27.80254480347343\n",
      "Traj:    3  ACCEPT:  dH: -0.0012113997  exp(-dH):  1.0012121    plaq:  0.70697632   topo: -1.0\n",
      "plaq(x) 0.7069763193143671  force.norm 30.515196608105374\n",
      "Traj:    4  ACCEPT:  dH:  0.0015065459  exp(-dH):  0.99849459   plaq:  0.72056544   topo:  0.0\n",
      "plaq(x) 0.720565440451751  force.norm 27.413912752174674\n",
      "Traj:    5  ACCEPT:  dH: -0.00072322231  exp(-dH):  1.0007235    plaq:  0.69602558   topo:  0.0\n",
      "plaq(x) 0.696025580670386  force.norm 28.34971124354062\n",
      "Traj:    6  ACCEPT:  dH:  0.0012864396  exp(-dH):  0.99871439   plaq:  0.74450964   topo: -1.0\n",
      "plaq(x) 0.7445096390193563  force.norm 25.8031928155474\n",
      "Traj:    7  ACCEPT:  dH: -0.0019431629  exp(-dH):  1.0019451    plaq:  0.65638725   topo: -1.0\n",
      "plaq(x) 0.6563872487614806  force.norm 29.265801813579625\n",
      "Traj:    8  ACCEPT:  dH:  0.0011946523  exp(-dH):  0.99880606   plaq:  0.68807034   topo:  0.0\n",
      "plaq(x) 0.6880703431834577  force.norm 26.882931584631223\n",
      "Traj:    9  ACCEPT:  dH: -0.00090115329  exp(-dH):  1.0009016    plaq:  0.64260286   topo: -1.0\n",
      "plaq(x) 0.642602856585121  force.norm 27.879005513123886\n",
      "Traj:   10  ACCEPT:  dH: -0.00090838013  exp(-dH):  1.0009088    plaq:  0.63991168   topo:  0.0\n",
      "plaq(x) 0.639911683252812  force.norm 29.734828907488563\n",
      "Traj:   11  ACCEPT:  dH:  0.0020228642  exp(-dH):  0.99797918   plaq:  0.72748726   topo: -2.0\n",
      "plaq(x) 0.7274872574663527  force.norm 27.367001902538032\n",
      "Traj:   12  ACCEPT:  dH:  6.6708202e-05  exp(-dH):  0.99993329   plaq:  0.70469542   topo: -1.0\n",
      "plaq(x) 0.7046954211429337  force.norm 26.416435564297192\n",
      "Traj:   13  ACCEPT:  dH: -0.0016355412  exp(-dH):  1.0016369    plaq:  0.64813809   topo: -1.0\n",
      "plaq(x) 0.6481380934588363  force.norm 29.035849165047235\n",
      "Traj:   14  ACCEPT:  dH: -0.00021445427  exp(-dH):  1.0002145    plaq:  0.63324501   topo: -2.0\n",
      "plaq(x) 0.6332450114154201  force.norm 29.05985232371214\n",
      "Traj:   15  ACCEPT:  dH:  0.00081225964  exp(-dH):  0.99918807   plaq:  0.6797113    topo: -2.0\n",
      "plaq(x) 0.6797112967972444  force.norm 28.163712809266272\n",
      "Traj:   16  ACCEPT:  dH: -0.001066602  exp(-dH):  1.0010672    plaq:  0.65649121   topo: -2.0\n",
      "Run times:  [0.1703325430280529, 0.19043595297262073, 0.18909807497402653, 0.1899968829820864]\n",
      "Per trajectory:  [0.042583135757013224, 0.04760898824315518, 0.04727451874350663, 0.0474992207455216]\n"
     ]
    }
   ],
   "source": [
    "param = Param(\n",
    "    beta = 2.0,\n",
    "    lat = (12, 12),\n",
    "    tau = 0.5, # 0.3\n",
    "    nstep = 64, # 3\n",
    "    # ADJUST ME\n",
    "    ntraj = 4, # 2**16 # 2**10 # 2**15\n",
    "    nprint = 4,\n",
    "    #\n",
    "    seed = 1331)\n",
    "\n",
    "# field = param.initializer()\n",
    "\n",
    "field = run(param, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 12, 12])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_run.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7676,  0.1112, -1.1515,  0.0148, -1.6742,  2.7194, -3.3007,\n",
       "           0.3404,  0.8066,  2.7574, -0.4600, -1.9311],\n",
       "         [ 1.2636, -0.3440, -1.2371,  1.0828,  2.4481, -1.8026,  1.8030,\n",
       "          -2.8691, -0.6625,  2.9842,  0.5663, -3.2327],\n",
       "         [ 1.0764,  0.5406, -0.9713,  0.1759, -1.2286, -0.8030,  0.1867,\n",
       "           0.1817,  1.1472,  0.1158, -0.0745, -0.3469],\n",
       "         [ 1.7824, -2.2857, -1.2961,  1.7109,  2.1828, -1.3668,  0.4849,\n",
       "           0.7049, -1.7105,  1.9339, -1.3661, -0.7745],\n",
       "         [-0.7513, -2.1414,  2.1709,  0.8740, -0.0108, -3.3313,  0.9532,\n",
       "          -1.1550,  1.2115, -0.5403,  1.7388,  0.9816],\n",
       "         [ 0.7189, -1.3359,  0.4805,  1.3458, -1.1233, -0.9647,  2.4161,\n",
       "          -0.0155, -1.5422,  0.0277,  0.2828, -0.2902],\n",
       "         [-2.2164,  2.0370, -0.7510,  1.3763, -0.2672, -1.2977, -1.6096,\n",
       "           3.0478, -2.5262,  0.8780,  1.9587, -0.6297],\n",
       "         [ 0.8151, -0.4133, -0.5078,  1.6036, -2.1299,  1.0449, -1.5394,\n",
       "           0.0590,  1.0788,  1.4732, -1.7132,  0.2289],\n",
       "         [-1.2565,  0.1842,  3.4680, -1.4354, -1.9104, -0.1097,  1.1371,\n",
       "           1.9769, -1.2118, -2.0801,  3.9294, -2.6918],\n",
       "         [ 1.6861, -2.3711, -0.3452, -0.8227,  0.3123,  1.6862, -0.8730,\n",
       "          -0.7200, -0.3577,  1.1445,  1.9726, -1.3122],\n",
       "         [-0.5653, -0.4111,  0.8711, -1.6333, -0.5028, -0.4497,  0.6192,\n",
       "           1.8687,  0.9242,  0.0135, -2.0557,  1.3213],\n",
       "         [-0.8777,  0.4205,  1.9085, -3.2414,  1.2773, -1.9449,  3.0699,\n",
       "          -1.1889, -1.0777,  0.3014,  0.6732,  0.6797]],\n",
       "\n",
       "        [[-1.7462, -1.4369,  1.6231, -1.6332,  1.3182, -3.3461,  3.0246,\n",
       "           1.4953, -0.3890, -2.8450, -1.7117,  0.8991],\n",
       "         [ 1.3504,  1.8056,  1.8913,  0.8234, -3.2989,  1.2231, -3.8806,\n",
       "          -0.6711,  0.7980,  0.5712, -0.4552,  0.8465],\n",
       "         [-0.4219, -1.3064, -1.5722, -0.6653,  3.0114,  2.0118,  3.6281,\n",
       "           0.5772, -1.2325,  1.6359,  2.2767, -0.6091],\n",
       "         [-1.1410,  1.6852,  2.0100,  0.4750, -2.9364, -2.3725, -2.6708,\n",
       "          -3.1940, -0.3362, -2.1543, -0.8627, -0.4351],\n",
       "         [ 0.7005,  0.5562, -2.9109, -2.0740,  0.1196,  2.0840,  1.6157,\n",
       "           3.4756,  0.5536,  3.0279, -0.0771, -1.8332],\n",
       "         [-0.2890, -1.0945,  0.5959,  0.1241,  1.2367, -1.1300, -2.5928,\n",
       "          -3.7323, -0.9787, -1.5467, -0.0906,  1.1812],\n",
       "         [ 2.0702, -1.3027, -0.0713, -0.1018, -0.9580, -0.6249,  3.4008,\n",
       "           0.3375,  1.3215,  0.4713, -1.2046, -0.8651],\n",
       "         [-2.0032,  0.4471,  0.2039, -0.0234,  1.8393, -0.5032, -0.5735,\n",
       "           2.4153, -1.1897, -1.7850,  1.8868,  1.0283],\n",
       "         [ 3.0760,  2.4785, -1.4972,  1.5419,  1.3224,  2.4769, -0.1996,\n",
       "          -2.1175,  0.1731,  3.7264, -1.9162,  1.0044],\n",
       "         [-3.7691, -1.2138,  2.5994,  1.9866, -0.2361, -2.0320, -0.0218,\n",
       "           2.6751,  1.8209, -1.4037,  0.5531, -0.8265],\n",
       "         [ 1.5274, -0.4326, -1.6490, -0.8384, -0.0233,  2.1126,  0.6204,\n",
       "          -1.9682, -3.2501, -2.1190,  1.9094, -0.7241],\n",
       "         [ 0.6460, -0.1857, -1.2230,  0.3851, -1.3949,  0.1003, -2.3504,\n",
       "           0.7071,  2.7090,  2.4210, -0.3079,  0.3336]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force(param, field_run[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nets(layers):\n",
    "    nets = []\n",
    "    for l in layers:\n",
    "        nets.append(l.plaq_coupling.net)\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_u1_equiv_layers_net(*, lattice_shape, nets):\n",
    "    n_layers = len(nets)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        # periodically loop through all arrangements of maskings\n",
    "        mu = i % 2\n",
    "        off = (i//2) % 4\n",
    "        net = nets[i]\n",
    "        plaq_coupling = NCPPlaqCouplingLayer(\n",
    "            net, mask_shape=lattice_shape, mask_mu=mu, mask_off=off)\n",
    "        link_coupling = GaugeEquivCouplingLayer(\n",
    "            lattice_shape=lattice_shape, mask_mu=mu, mask_off=off, \n",
    "            plaq_coupling=plaq_coupling)\n",
    "        layers.append(link_coupling)\n",
    "    return torch.nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_nets(flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flow = make_u1_equiv_layers_net(lattice_shape = param.lat, nets = get_nets(flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latsize = (12, 12)\n",
      "volume = 144\n",
      "beta = 2.0\n",
      "trajs = 4\n",
      "tau = 0.5\n",
      "steps = 64\n",
      "seed = 1331\n",
      "nth = 2\n",
      "nth_interop = 2\n",
      "Initial configuration:  plaq: 0.6481015786072608  topo: 2.0\n",
      "23.769701649797888 (-116.53445539389065, -120.53610026514147) 0.965123015498082\n",
      "Traj:    1  ACCEPT:  dH:  0.12061243   exp(-dH):  0.88637742   plaq:  0.65311072   topo: -2.0\n",
      "32.16286069243797 (-120.1481012358675, -120.23093118538995) 0.9419354022939522\n",
      "Traj:    2  ACCEPT:  dH:  0.0054811496  exp(-dH):  0.99453384   plaq:  0.65460432   topo:  2.0\n",
      "39.85399081188922 (-120.16897302727948, -122.11547603755164) 0.9641809328701209\n",
      "Traj:    3  ACCEPT:  dH: -0.031140951  exp(-dH):  1.0316309    plaq:  0.69446443   topo: -2.0\n",
      "43.151113410659725 (-122.34084850976468, -119.03432826546131) 0.912262122722401\n",
      "Traj:    4  ACCEPT:  dH: -0.11953146   exp(-dH):  1.1269687    plaq:  0.63606857   topo: -1.0\n",
      "37.32056471850772 (-118.79559013169148, -120.57127954351374) 0.8971280972684773\n",
      "Traj:    5  ACCEPT:  dH:  0.063506931  exp(-dH):  0.93846761   plaq:  0.71716421   topo:  0.0\n",
      "31.525947622452254 (-120.34978187661164, -120.18800371310238) 0.9490511396364038\n",
      "Traj:    6  REJECT:  dH:  1.8051277    exp(-dH):  0.16445345   plaq:  0.71716418   topo:  0.0\n",
      "21.981104549539186 (-120.51793823681439, -118.79110843013098) 0.9795888157904371\n",
      "Traj:    7  ACCEPT:  dH:  0.35809325   exp(-dH):  0.69900789   plaq:  0.77278793   topo: -3.0\n",
      "16.790002889093287 (-118.78770229540413, -119.99529589323888) 0.9750767753607218\n",
      "Traj:    8  ACCEPT:  dH: -0.32977936   exp(-dH):  1.3906613    plaq:  0.72906726   topo: -3.0\n",
      "20.10290763753857 (-120.109189646012, -122.19614494750377) 0.9795244550017411\n",
      "Traj:    9  ACCEPT:  dH: -0.0029667294  exp(-dH):  1.0029711    plaq:  0.71634143   topo:  0.0\n",
      "38.533243741249315 (-121.8502596514434, -121.45443726754044) 0.9508370950959553\n",
      "Traj:   10  REJECT:  dH:  1.412203     exp(-dH):  0.24360602   plaq:  0.71634154   topo:  0.0\n",
      "31.78762169409526 (-121.91985384664135, -123.85569853569626) 0.943599435724918\n",
      "Traj:   11  ACCEPT:  dH: -0.0086900357  exp(-dH):  1.0087279    plaq:  0.69031481   topo: -2.0\n",
      "38.29414732108992 (-123.85366912392122, -120.88848900964177) 0.8790109670482266\n",
      "Traj:   12  ACCEPT:  dH:  0.05919811   exp(-dH):  0.94252003   plaq:  0.74857854   topo: -1.0\n",
      "32.59696295322438 (-121.18576349027039, -117.86382115058458) 0.96585616166117\n",
      "Traj:   13  ACCEPT:  dH: -0.11422347   exp(-dH):  1.1210026    plaq:  0.64079263   topo: -1.0\n",
      "51.75975960463486 (-117.50482031051895, -121.17442890497792) 0.9515584564833551\n",
      "Traj:   14  REJECT:  dH:  0.95387557   exp(-dH):  0.38524508   plaq:  0.64079274   topo: -1.0\n",
      "41.411872376188214 (-117.64635057237332, -114.73908419080271) 0.9537618812179821\n",
      "Traj:   15  REJECT:  dH:  3.0160706    exp(-dH):  0.048993356  plaq:  0.64079265   topo: -1.0\n",
      "45.65353916760584 (-117.63842531769595, -121.91787436820296) 0.9519859238084336\n",
      "Traj:   16  ACCEPT:  dH:  0.69612624   exp(-dH):  0.49851269   plaq:  0.64626207   topo: -1.0\n",
      "Run times:  [54.869982750038616, 54.67483307002112, 53.498376384028234, 53.64896607701667]\n",
      "Per trajectory:  [13.717495687509654, 13.66870826750528, 13.374594096007058, 13.412241519254167]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[2.3752e+00, 3.7908e+00, 4.3528e+00, 2.5210e+00, 4.7764e+00,\n",
       "          5.4460e+00, 4.3141e+00, 5.1648e+00, 3.5046e+00, 4.6180e+00,\n",
       "          1.0509e+00, 1.6771e+00],\n",
       "         [4.8399e+00, 1.5831e+00, 1.4803e+00, 4.7336e+00, 5.1794e+00,\n",
       "          1.3116e+00, 1.0175e+00, 3.4732e+00, 2.8924e+00, 1.1776e+00,\n",
       "          6.2588e+00, 1.9701e+00],\n",
       "         [4.8401e-01, 6.6562e-01, 4.7643e+00, 3.3379e+00, 5.0044e+00,\n",
       "          3.4488e+00, 3.2837e+00, 5.6005e+00, 5.1433e+00, 5.3443e+00,\n",
       "          1.8785e+00, 4.6364e+00],\n",
       "         [7.2549e-01, 5.4411e+00, 4.0904e+00, 1.7220e+00, 5.6641e+00,\n",
       "          5.6044e+00, 6.8245e-01, 5.5383e+00, 4.4251e-01, 1.0098e+00,\n",
       "          2.8486e+00, 5.2598e+00],\n",
       "         [2.0633e+00, 1.9559e+00, 3.9353e+00, 4.3933e+00, 2.8059e+00,\n",
       "          3.3580e+00, 2.2833e+00, 3.0077e+00, 5.1626e+00, 3.5316e+00,\n",
       "          5.1970e+00, 1.5598e+00],\n",
       "         [5.3014e+00, 8.9498e-01, 1.8614e-01, 4.6153e+00, 5.6853e+00,\n",
       "          4.7606e+00, 4.4472e-01, 4.4610e+00, 6.1098e+00, 5.8906e-01,\n",
       "          6.2272e+00, 3.6547e+00],\n",
       "         [4.0483e-01, 2.4834e+00, 2.9601e+00, 3.5253e+00, 1.0280e+00,\n",
       "          5.0649e+00, 1.4886e+00, 3.7110e+00, 1.7836e+00, 3.6921e+00,\n",
       "          3.0671e+00, 4.8418e-01],\n",
       "         [3.5442e+00, 5.8296e+00, 3.2885e+00, 2.1253e+00, 3.2401e+00,\n",
       "          2.2498e+00, 5.7244e+00, 3.6926e+00, 3.8110e+00, 6.0906e+00,\n",
       "          3.3621e+00, 3.8998e+00],\n",
       "         [2.3834e+00, 4.5601e+00, 2.1211e-01, 7.5147e-01, 1.7712e+00,\n",
       "          8.4991e-01, 3.0266e+00, 1.8792e+00, 1.5614e-01, 2.6998e-01,\n",
       "          3.5944e+00, 5.7552e+00],\n",
       "         [2.3368e+00, 6.1368e-01, 1.0627e+00, 2.0983e+00, 1.9888e+00,\n",
       "          2.4709e+00, 5.7717e+00, 3.8461e+00, 5.8450e+00, 2.3565e+00,\n",
       "          1.7216e+00, 3.6943e-02],\n",
       "         [2.5093e+00, 5.0610e+00, 1.2345e+00, 3.6613e+00, 5.3584e+00,\n",
       "          5.7617e+00, 5.0036e+00, 4.0383e+00, 3.2778e+00, 3.8422e+00,\n",
       "          4.1944e+00, 4.9556e+00],\n",
       "         [1.1944e+00, 2.7347e+00, 5.1228e+00, 8.3419e-01, 4.0988e+00,\n",
       "          2.0052e+00, 2.5201e+00, 3.7536e+00, 2.7829e+00, 5.5470e+00,\n",
       "          4.0232e+00, 2.1085e+00]],\n",
       "\n",
       "        [[3.4897e+00, 6.7719e-01, 1.0602e+00, 4.5322e+00, 5.0214e+00,\n",
       "          1.4155e+00, 1.3356e+00, 5.6174e+00, 7.6092e-01, 4.7401e+00,\n",
       "          3.9088e+00, 1.3021e+00],\n",
       "         [4.8293e+00, 4.5170e-01, 5.7933e+00, 9.6413e-01, 5.5493e+00,\n",
       "          1.8270e-01, 2.2931e+00, 4.8783e+00, 2.0487e+00, 7.0367e-02,\n",
       "          5.6412e+00, 5.3029e+00],\n",
       "         [6.4852e-05, 5.9484e+00, 1.1759e+00, 5.9061e+00, 8.6990e-01,\n",
       "          6.2182e+00, 1.2657e+00, 3.7284e+00, 5.2697e+00, 4.5366e+00,\n",
       "          5.5989e-01, 7.4671e-01],\n",
       "         [4.1626e-01, 3.8581e+00, 5.8078e+00, 1.9207e+00, 5.6666e+00,\n",
       "          6.1874e+00, 4.0426e+00, 2.6775e+00, 6.0947e+00, 1.0709e+00,\n",
       "          2.9434e+00, 3.4470e+00],\n",
       "         [5.8513e+00, 2.2803e+00, 4.7969e+00, 7.4585e-01, 5.8034e+00,\n",
       "          6.2785e+00, 2.4391e+00, 3.1790e+00, 8.9014e-01, 3.1808e+00,\n",
       "          4.9133e+00, 5.7901e+00],\n",
       "         [5.3683e+00, 3.8416e+00, 4.6231e+00, 3.7602e+00, 3.9175e-01,\n",
       "          6.1311e+00, 2.6214e+00, 5.6846e+00, 4.3117e-01, 2.4402e-01,\n",
       "          7.9803e-01, 5.9186e+00],\n",
       "         [5.4029e+00, 1.8871e+00, 4.5210e+00, 5.1058e+00, 4.9340e+00,\n",
       "          2.6211e+00, 6.1963e-01, 1.2710e+00, 5.6992e+00, 2.6970e-01,\n",
       "          3.5921e+00, 1.6039e+00],\n",
       "         [1.3750e+00, 1.5011e+00, 5.6442e+00, 8.6734e-01, 2.0996e+00,\n",
       "          6.2423e+00, 3.9199e+00, 6.0103e+00, 1.4955e-01, 4.6582e+00,\n",
       "          9.2766e-01, 1.7330e+00],\n",
       "         [4.6355e+00, 5.8884e+00, 5.5757e+00, 2.7443e+00, 2.0890e+00,\n",
       "          2.1976e+00, 1.3933e+00, 6.8537e-01, 2.9627e+00, 1.8305e+00,\n",
       "          2.4693e+00, 1.3580e+00],\n",
       "         [3.3740e+00, 2.8066e+00, 4.6352e+00, 4.7352e+00, 5.7433e+00,\n",
       "          5.3400e+00, 1.6434e+00, 6.1849e+00, 2.1989e+00, 5.0589e+00,\n",
       "          5.3720e+00, 4.2556e+00],\n",
       "         [1.4190e+00, 2.4536e+00, 4.3957e+00, 4.8569e+00, 3.4079e-01,\n",
       "          1.2534e+00, 5.0692e+00, 8.5662e-01, 4.7274e+00, 4.8739e+00,\n",
       "          4.4446e+00, 6.0901e+00],\n",
       "         [3.8371e+00, 4.5341e+00, 6.0533e+00, 2.1718e+00, 4.9343e-01,\n",
       "          1.1757e+00, 4.7199e+00, 4.3454e-01, 4.7134e+00, 4.8956e+00,\n",
       "          5.8343e+00, 2.6523e+00]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_run(param, new_flow, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-209.9513)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action(param, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
